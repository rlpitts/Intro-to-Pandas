{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce116b0-1ba0-4355-95c9-3480dcca960e",
   "metadata": {},
   "source": [
    "# Introduction to Pandas\n",
    "Pandas is an extensive, powerful library for manipulating and analyzing serial and/or tabular data structures with column and row labels. It comes with a suite of built-in statistical functions, time series construction and analysis tools, grouping and sorting functions, hierarchical indexing functions, and even some built-in plotting methods. The data structures that Pandas is built around are highly mutable, adaptable to many data types, and can be loaded from or saved to a wide variety of text, binary, or database files. Several of Python's machine learning libraries and large language modeling packages are built around pandas data structures, especially SciKitLearn and TensorFlow. [Some accelerated operations are supported using the `bottleneck` and `numexpr` libraries.](https://pandas.pydata.org/docs/user_guide/basics.html#accelerated-operations)\n",
    "\n",
    "In the interest of ensuring you know about the best tools for your data, I should mention that Pandas has a couple of potentially deal-breaking limitations: support for data structures with >2 dimensions is limited and hard to use, and there is little native support for parallelization. If you have N-dimensional data where N>2, [`Xarray` is probably the better choice of software](https://xarray.dev/). Similarly, if speed is enough of an issue that you need to distribute your array processing over multiple CPUs or nodes, including `bottleneck` and `numexpr` may help, but [you might consider `Polars` instead](https://docs.pola.rs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaed61b3-0344-4714-9b62-e0abb0110aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sb \n",
    "###^standard import abbr is sns, after a West Wing character's initials\n",
    "###I use sb so I don't have to remember that. I never saw West Wing.\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib as mpl\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf54ef-79ea-43f4-96c2-e6d018725ed6",
   "metadata": {},
   "source": [
    "### Pandas Object Classes\n",
    "Most data structures you will use in Pandas will be one of the following two object classes:\n",
    "\n",
    "1. `Series`: a 1D array that can optionally have an \"index\" assigned to every entry and a \"name\" for the whole distinct from the variable name in your code. The indexes (and name) need only to be hashable; they don't have to be numeric or even unique. Data can also be of any type.\n",
    "2. `DataFrame`: a 2D array or table of values where every row and column can be assigned a label and/or index; like a `dict` of `Series` stacked column-wise, but with different data selection syntax. Arithmetic operations can be performed along either axis, where axis=0 typically refers to row-wise operations and axis=1 refers to column-wise operations. The data type of each column is determined separately. DataFrames can also be grouped by column or row values with the `groupby()` method, or rearranged with the `pivot()` or `pivot_table()` methods, such that the result is hierarchical, like a 2D projection of higher-dimensional data structures.\n",
    "\n",
    "These are the workhorses of data analysis with Pandas. Other Pandas data structures you may encounter include:\n",
    "\n",
    "3. `Index`: the Pandas datatype of attributes that store the row and column labels of a DataFrame or Series. You could convert a 1D array to an `Index` type object, but more often you will extract `Index` objects from existing Series or DataFrames and, if needed, convert them to NumPy arrays or lists.\n",
    "4. `DataFrameGroupBy` (abbr. GroupBy): a structure returned by the `df.groupby()` method. It may look like the input DataFrame sorted by the values of the column label(s) entered as arguments, but for any basic function or statistical method called on this object, the number of return values will equal the number of unique values or value combinations from the column(s) used to group the DataFrame contents. If two or more labels are used for grouping, the result will be hierarchical, and will have MultiIndexes instead of just Indexes.\n",
    "5. `MultiIndex`: the hierarchical analogue of `Index` for GroupBy objects and other hierarchical DataFrames. Each list item is a tuple instead of a string or scalar value. We will talk about MultiIndexing and hierarchical DataFrames rather late in this course because, in my view, their complexity makes their usage awkward and error-prone. If you need such functionality and don't like the constraints of working strictly in 2D, Xarray might be a better alternative.\n",
    "6. `NumpyExtensionArray`, `DatetimeArray`, and `TimedeltaArray`: to extend NumPy functionality, especially to time series, Pandas includes an `array(data, dtype=dtype)` function that can be used to declare simple arrays or store timestamps or time increments depending on what you enter for the `dtype` kwarg. Most familiar datatypes will produce an array of the `NumpyExtensionArray`, but if you set `dtype` equal to `'datetime64[ns]'` or `'timedelta64[ns]'`, the resulting array will be of `DatetimeArray` or `TimedeltaArray` type, respectively. \n",
    "\n",
    "DataFrames are the primary structure that Pandas is designed to work with, so we'll focus on those. **Functions that work on a DataFrame will also work on a Series unless they explicitly require multiple columns.** For the sake of demonstration, however, I will mock up a couple of example Series and DataFrames.\n",
    "\n",
    "Series construction is simple: just call `pandas.Series()`. It is best if you can provide at least a 1D data array/list, if not also a corresponding list of indexes—for the same reason that allocating and then replacing values in a NumPy array is faster than appending to a list—but it is possible to create an empty Series and fill it later. If you provide the data but no index list, indexes will be assigned automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "fc63b0f4-cede-4c4e-852c-2e25ade27a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    a\n",
      "2    b\n",
      "3    c\n",
      "Name: abc, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ser1 = pd.Series(['a','b','c'], index=[1,2,3], name='abc')\n",
    "print(ser1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ffd101-b814-425d-a864-257cd66ca24d",
   "metadata": {},
   "source": [
    "Note that the left column changes depending on whether we include the index kwarg (default index is 0-based), and that you can optionally assign the series a name.\n",
    "\n",
    "The indexes also do not have to be numeric. They can be letters or even datatime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "526cb1fd-ea19-4aee-84cc-e150d0f34dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    1\n",
      "b    2\n",
      "c    3\n",
      "Name: abc123, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ser2 = pd.Series([1,2,3], index=['a','b','c'], name='abc123')\n",
    "print(ser2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d54e38b5-3e8f-4205-a1d8-e837fc81e792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2023-12-21 08:34:00', '2023-12-31 08:36:00',\n",
      "               '2024-01-11 08:31:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "2023-12-21 08:34:00    7.01583\n",
      "2023-12-31 08:36:00    7.11306\n",
      "2024-01-11 08:31:00    7.46278\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#from datetime import datetime as dttm\n",
    "sunrises= ['2023-12-21 08:34:00',\n",
    "           '2023-12-31 08:36:00',\n",
    "           '2024-01-11 08:31:00']\n",
    "daylens = [7.01583, 7.11306, 7.46278]\n",
    "srt = pd.to_datetime(sunrises,\n",
    "                     format=\"%Y-%m-%d %H:%M:%S\")\n",
    "#much easier to use pandas to convert to datetime objects than\n",
    "# to go through the datetime module - more on this later\n",
    "print(srt)\n",
    "print(pd.Series(daylens,index=srt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d436b4-ca2e-4e5c-95f8-aa11507ae752",
   "metadata": {},
   "source": [
    "Construction of a DataFrame is similar to that of a Series, but now you have the option to specify both the column names and the row indexes. If you leave these kwargs empty, pandas will assign 0-based integer indexes to both. Technically, even the first positional argument, the data, is optional. You can leave it blank and fill it later, although this is not recommended (see sub-section on initializing DataFrames with NumPy vs. pure Pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "e15dba54-9c47-4978-8ba3-31d1915703b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    b    c     d\n",
      "1  0.5  1.0  1.5   2.0\n",
      "2  2.5  3.0  3.5   4.0\n",
      "3  4.5  5.0  5.5   6.0\n",
      "4  6.5  7.0  7.5   8.0\n",
      "5  8.5  9.0  9.5  10.0\n"
     ]
    }
   ],
   "source": [
    "dummy_df = pd.DataFrame(np.linspace(0.5,10,20).reshape(5,4),\n",
    "                        columns=['a','b','c','d'], index=list(range(1,6)))\n",
    "print(dummy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbacbb-54f7-4333-9333-4d7c72c8a319",
   "metadata": {},
   "source": [
    "#### Basic Attributes\n",
    "DataFrames and Series have more than a dozen attributes besides those used to select subsets of the data, and several hundred object methods to transform, aggregate, and broadcast data and functions thereof. We cannot possibly cover all the object methods, but I will do my best to cover the ones I have experience with and/or can demonstrate within the limits of the presentation format.\n",
    "\n",
    "There are a small enough number of attributes that they can be listed here and in the upcoming section on data selection:\n",
    "| Attribute Syntax | Value |\n",
    "| --- | --- |\n",
    "| `df.axes` | nested list of row & column indexes (labels) |\n",
    "| `df.columns` | Index-type, list of column labels (add `.values` to get an array) |\n",
    "| `df.index` | Index-type, list of row indexes (labels; add `.values` to get an array) |\n",
    "| `df.dtypes` | list of datatypes by column |\n",
    "| `df.empty` | boolean, True if df is empty |\n",
    "| `df.ndim` | number of axes (1 for a Series, 2 for a DataFrame) |\n",
    "| `df.shape` | tuple, length of df along each axis |\n",
    "| `df.size` | integer, total number of data entries |\n",
    "| `df.values` | returns df converted to a NumPy array (can also be applied to `.columns` & `.index`) |\n",
    "\n",
    "Here are a few of the most basic attributes in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "372c66aa-a4a6-46ca-a017-3ae64fc1b6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Index([1, 2, 3, 4, 5], dtype='int64'),\n",
       " Index(['a', 'b', 'c', 'd'], dtype='object')]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.axes #returns both row and column labels/indexes in a nested list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "00bbbcab-7ae4-48ff-b5c2-c8ff943de34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (5, 4) 20\n"
     ]
    }
   ],
   "source": [
    "print(dummy_df.ndim, dummy_df.shape, dummy_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7369f990-0567-44ee-81bb-ff6b99e9b73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/F - Series abc contains NaNs: False\n"
     ]
    }
   ],
   "source": [
    "print('T/F - Series', ser1.name, 'contains NaNs:', ser1.hasnans) #Series only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275817f6-cfae-4136-89a0-a56b54d2d750",
   "metadata": {},
   "source": [
    "#### Initializing DataFrames with NumPy vs. Pure Pandas\n",
    "If you have to build a DataFrame from scratch within a Python program (e.g. as the output of a bunch of simulations), allocating and filling a NumPy array and then converting to a DataFrame is usually faster than creating an empty DataFrame and filling that, particularly if the NumPy array is purely comprised of floats. It's up to you to determine the best balance of speed and fool-proofing depending on the method you use to fill your (future) DataFrame, and you may need to test subsets of your data to find the best construction method depending on how you assign entries, rows, or columns to the DataFrame.\n",
    "\n",
    "Let's say you're simulating 100 dust grains in an interstellar UV radiation field over 10000 hours, and the following DataFrame is to keep track of the instantaneous surface temperature of each grain in Kelvin (something vaguely like this could show up in an astrochemistry paper). Let's compare a NumPy-first instantiation to pure Pandas. To make this demo a little more realistically inefficient to construct, I assume that each particle's data comes from a separate data structure and so must be added to the final array 1 column at a time. Note that I'm using one of several time series constructor functions. Don't worry about them too much right now; we'll cover time series later. I'm just using them to increase the computational burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "18167724-a21a-492d-94b5-e2232ea0b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimedeltaIndex(['1 days 00:00:00', '1 days 01:00:00', '1 days 02:00:00',\n",
      "                '1 days 03:00:00', '1 days 04:00:00', '1 days 05:00:00',\n",
      "                '1 days 06:00:00', '1 days 07:00:00', '1 days 08:00:00',\n",
      "                '1 days 09:00:00', '1 days 10:00:00', '1 days 11:00:00',\n",
      "                '1 days 12:00:00', '1 days 13:00:00', '1 days 14:00:00',\n",
      "                '1 days 15:00:00', '1 days 16:00:00', '1 days 17:00:00',\n",
      "                '1 days 18:00:00', '1 days 19:00:00', '1 days 20:00:00',\n",
      "                '1 days 21:00:00', '1 days 22:00:00', '1 days 23:00:00',\n",
      "                '2 days 00:00:00', '2 days 01:00:00', '2 days 02:00:00',\n",
      "                '2 days 03:00:00', '2 days 04:00:00', '2 days 05:00:00'],\n",
      "               dtype='timedelta64[ns]', freq='H')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "partid = np.arange(1,101)\n",
    "ts = pd.timedelta_range(start='1 day',periods=10000, freq='1H')\n",
    "print(ts[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3155fb7c-ab2c-43b4-b219-19d4a8fe65d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        1          2          3          4          5    \\\n",
      "1 days 00:00:00   45.132719  18.706452  23.681400  77.297622  29.968109   \n",
      "1 days 01:00:00   42.414091  15.198667   8.644502  26.829536   4.930699   \n",
      "1 days 02:00:00   87.395789  22.426079  85.479914  17.824291  13.695420   \n",
      "1 days 03:00:00  150.882766  39.727747   4.926684  35.183574   7.326537   \n",
      "1 days 04:00:00   11.864747  40.819665   8.266819  31.063394  20.859097   \n",
      "\n",
      "                       6          7          8          9          10   ...  \\\n",
      "1 days 00:00:00  13.487936  10.816268  27.426948  11.739450  22.612319  ...   \n",
      "1 days 01:00:00  57.313018  86.826926  21.660628   5.241199  19.713096  ...   \n",
      "1 days 02:00:00  43.080175  10.783574  67.239200  64.652637  16.542422  ...   \n",
      "1 days 03:00:00  15.930251   7.082038  76.137979  15.832700  59.018916  ...   \n",
      "1 days 04:00:00  10.197294  24.008260  15.544460   6.295166  13.939077  ...   \n",
      "\n",
      "                        91         92         93         94         95   \\\n",
      "1 days 00:00:00   10.939851   3.291486   9.435333  11.810341   3.315489   \n",
      "1 days 01:00:00  108.537795  55.559438  30.980446  27.835846   8.280404   \n",
      "1 days 02:00:00   59.607540  15.339047  25.697686  22.142811   3.005843   \n",
      "1 days 03:00:00   12.973617  67.507364  20.214948  10.163416  37.390247   \n",
      "1 days 04:00:00    4.470187  12.616930   4.513192  11.804861  14.863749   \n",
      "\n",
      "                       96         97         98         99          100  \n",
      "1 days 00:00:00  16.063714  35.353971  14.638000  67.405534   17.235578  \n",
      "1 days 01:00:00  65.949796  41.274419  33.294003  28.657865   11.447813  \n",
      "1 days 02:00:00  13.625674  14.613779  23.204835  43.946803   14.651313  \n",
      "1 days 03:00:00  12.163849   7.085532  12.231080  48.379254  121.697870  \n",
      "1 days 04:00:00  17.964688  39.006968   5.784223  24.185061   36.215125  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "NumPy-first approach took 28.268 ms\n",
      "[33.277151 31.981383 28.88302  28.679584 28.562468 27.947251 27.84787\n",
      " 28.113462 28.455603 29.401997]\n"
     ]
    }
   ],
   "source": [
    "#Start with NumPy implementation\n",
    "runtimes = np.zeros(100)\n",
    "for i in range(100):\n",
    "    t0 = time.time_ns()\n",
    "    data = np.zeros((len(ts), len(partid)))\n",
    "    for j,v in enumerate(partid):\n",
    "        data[:,j] = 10*np.random.chisquare(3,len(ts))\n",
    "        #this gives fairly believably values for such a quick and dirty demo\n",
    "    dummy_df = pd.DataFrame(data,columns=partid,index=ts)\n",
    "    if i==1:\n",
    "        print(dummy_df.head())\n",
    "    runtimes[i] = (time.time_ns()-t0)\n",
    "    del data #trying to stop NumPy from skewing the results with cached data\n",
    "    del dummy_df\n",
    "    #doesn't seem to help\n",
    "print('NumPy-first approach took {:.3f} ms'.format(np.mean(runtimes)*10**-6))\n",
    "print(runtimes[:10]*10**-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0ad31970-4e23-425e-bf4e-f5c933e32dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       1          2          3          4          5    \\\n",
      "1 days 00:00:00  35.313166  11.996560  62.050499  23.940570  34.210093   \n",
      "1 days 01:00:00  33.586879  23.129506  21.573475   6.287035  26.682972   \n",
      "1 days 02:00:00  24.858662   3.823288  94.689335  40.610323  36.974038   \n",
      "1 days 03:00:00  12.265710  22.941618  21.732078  35.709924  53.274873   \n",
      "1 days 04:00:00   1.740359  27.318770  55.228072  45.982285  28.161902   \n",
      "\n",
      "                       6          7          8          9          10   ...  \\\n",
      "1 days 00:00:00   9.948517   3.366698   2.798905  40.368338  16.360232  ...   \n",
      "1 days 01:00:00  24.520840  35.693240  16.916476  66.463949  55.664016  ...   \n",
      "1 days 02:00:00  69.898572  12.921330  26.019181  30.696951  42.910638  ...   \n",
      "1 days 03:00:00  25.931199  43.234775  11.747854  79.388570  66.596115  ...   \n",
      "1 days 04:00:00  30.965781  20.444539  48.600102  15.508818  27.792224  ...   \n",
      "\n",
      "                       91         92         93         94          95   \\\n",
      "1 days 00:00:00  58.689731  42.315043  47.237271  48.676583   95.429496   \n",
      "1 days 01:00:00  23.903999  26.485876  17.799468  15.116500   73.350708   \n",
      "1 days 02:00:00  11.790501  13.564586  17.529895  29.710533  136.428804   \n",
      "1 days 03:00:00  28.046393  42.743310  16.103425  48.058843   51.853363   \n",
      "1 days 04:00:00  85.993026  48.157024  20.143773  10.863239   48.796472   \n",
      "\n",
      "                       96          97         98         99          100  \n",
      "1 days 00:00:00  17.153895  125.002056  19.904787  61.643117   28.489922  \n",
      "1 days 01:00:00  80.354984    5.244301  45.035488  29.738625   20.350957  \n",
      "1 days 02:00:00   6.821208   60.462630  13.538492  72.474699  109.363960  \n",
      "1 days 03:00:00  59.056258    2.505843  25.548857  39.263672   14.384252  \n",
      "1 days 04:00:00   8.185418   24.642326  11.370654  20.015998   19.529786  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "Pure Pandas approach took 37.713 ms\n",
      "[42.652386 42.760195 38.047364 37.792336 37.9014   38.177575 37.870374\n",
      " 38.344151 39.544501 38.350958]\n"
     ]
    }
   ],
   "source": [
    "#now the pure Pandas approach\n",
    "runtimes = np.zeros(100)\n",
    "for i in range(len(runtimes)):\n",
    "    t0 = time.time_ns()\n",
    "    dummy_df2=pd.DataFrame(columns=partid,index=ts)\n",
    "    for pid in partid:\n",
    "        dummy_df2[pid]=10.*np.random.chisquare(3,len(ts))\n",
    "    if i==1:\n",
    "        print(dummy_df2.head())\n",
    "    runtimes[i] = (time.time_ns()-t0)\n",
    "    del dummy_df2  #try to avoid skewing the results with cached data\n",
    "    #doesn't seem to help\n",
    "print('Pure Pandas approach took {:.3f} ms'.format(np.mean(runtimes)*10**-6))\n",
    "print(runtimes[:10]*10**-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586b49b-a41a-4ca1-a4db-3058c7047cd7",
   "metadata": {},
   "source": [
    "As you can see, the NumPy-first approach can do about 4 iterations of the program in the time it takes the pure-Pandas version to do about 3 iterations. You can also see that the first attempt (or 2) to run either set of calculations usually takes longer than subsequent calculations despite my best efforts to eliminate cached data, though this is more consistently true for the Pandas approach.\n",
    "\n",
    "**Note on converting DataFrames to Numpy.** If you need to convert a DataFrame to a numpy array, you can use `df.to_numpy()`. However, both column and index labels will then be lost. If you used a data column as indexes and want to keep it, you can append the indexes as a column before converting to an array, but remember that NumPy arrays have limited support for mixed data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bef4ee-e52c-4d64-8618-1d20375c4519",
   "metadata": {},
   "source": [
    "### Basic I/O\n",
    "The most used read/write combos are `df = pd.read_csv()`/`df.to_csv()` and `df = pd.read_excel()`/`df.to_excel()`, but Pandas can read and write to a wide variety of text and binary formats, including HDF5 and Python pickle files. Even SQL tables can be loaded or output. [I'll refer you to the documentation for the full list of readers and writers](https://pandas.pydata.org/docs/user_guide/io.html#). Most of these readers also accept a URL as a filepath if you want to pull data from a public online repository like Kaggle.\n",
    "\n",
    "**Most pandas data readers default to assuming the top row is a row of column names**. If you have data where you know this not to be the case, you can either choose a different row by setting the `header` kwarg to a different number or override the column names by setting `columns` equal to a list of your choosing. For row indexes/labels, **the default behavior of most pandas data readers is to assume there is no index column and assign 0-based indexes to every row below the header row.** Often there is a column in the dataset that makes more sense to index rows by, so you can change the default behavior by setting `index_col` equal to index of the column you want to use as row indexes. Remember that columns are indexed left to right starting from 0.\n",
    "\n",
    "The table in the documentation does not include it, but for standard tab-delimited text files, you can use either `pd.read_table(filepath_or_buffer,sep='\\t')` or `pd.read_csv(filepath_or_buffer,sep='\\t')`. Many of the keyword arguments (kwargs) in these and other reader functions are similar to those of `np.gen_from_txt()`, but are much less fussy about text encoding, missing values, and mixed data types. \n",
    "\n",
    "I'll load one of my favorite datasets, the 5250 exoplanet dataset. Those of you who attended my Matplotlib for Publication workshop will remember this from the exercises. (I like it because it's very clean for real data but it's big enough and just realistically flawed/\"ugly\" enough to provide some practice at data cleanup.) Notice that I've kept the default behavior for the column names, but have changed the row indexes to the `name` column, which was the leftmost column in the csv file. It didn't make sense to assign a number to every planet when each planet comes with a more meaningful and unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9050d85b-6805-46ab-92e1-5d2e460cc0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>star_mag</th>\n",
       "      <th>planet_type</th>\n",
       "      <th>discovery_yr</th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>detection_method</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11 Comae Berenices b</th>\n",
       "      <td>304.0</td>\n",
       "      <td>4.72307</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2007</td>\n",
       "      <td>6169.20</td>\n",
       "      <td>12.096</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>0.892539</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11 Ursae Minoris b</th>\n",
       "      <td>409.0</td>\n",
       "      <td>5.01300</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2009</td>\n",
       "      <td>4687.32</td>\n",
       "      <td>12.208</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14 Andromedae b</th>\n",
       "      <td>246.0</td>\n",
       "      <td>5.23133</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2008</td>\n",
       "      <td>1526.40</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.508693</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14 Herculis b</th>\n",
       "      <td>58.0</td>\n",
       "      <td>6.61935</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2002</td>\n",
       "      <td>2588.14</td>\n",
       "      <td>12.544</td>\n",
       "      <td>2.773069</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.37</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16 Cygni B b</th>\n",
       "      <td>69.0</td>\n",
       "      <td>6.21500</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>1996</td>\n",
       "      <td>566.04</td>\n",
       "      <td>13.44</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      distance  star_mag planet_type  discovery_yr  mass_ME  \\\n",
       "#name                                                                         \n",
       "11 Comae Berenices b     304.0   4.72307   Gas Giant          2007  6169.20   \n",
       "11 Ursae Minoris b       409.0   5.01300   Gas Giant          2009  4687.32   \n",
       "14 Andromedae b          246.0   5.23133   Gas Giant          2008  1526.40   \n",
       "14 Herculis b             58.0   6.61935   Gas Giant          2002  2588.14   \n",
       "16 Cygni B b              69.0   6.21500   Gas Giant          1996   566.04   \n",
       "\n",
       "                     radius_RE  orbital_radius_AU  orbital_period_yr  \\\n",
       "#name                                                                  \n",
       "11 Comae Berenices b    12.096           1.290000           0.892539   \n",
       "11 Ursae Minoris b      12.208           1.530000           1.400000   \n",
       "14 Andromedae b          12.88           0.830000           0.508693   \n",
       "14 Herculis b           12.544           2.773069           4.800000   \n",
       "16 Cygni B b             13.44           1.660000           2.200000   \n",
       "\n",
       "                      eccentricity detection_method  \n",
       "#name                                                \n",
       "11 Comae Berenices b          0.23  Radial Velocity  \n",
       "11 Ursae Minoris b            0.08  Radial Velocity  \n",
       "14 Andromedae b               0.00  Radial Velocity  \n",
       "14 Herculis b                 0.37  Radial Velocity  \n",
       "16 Cygni B b                  0.68  Radial Velocity  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('exoplanets_5250_EarthUnits.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736eaae8-7fbc-4ab1-948b-c50bd0f7fb65",
   "metadata": {},
   "source": [
    "The columns of this data table are, from left to right:\n",
    "- **name**: exoplanet name\n",
    "- **distance**: distance to the planetary system in light years\n",
    "- **star_mag**: apparent brightness of the star as seen from Earth in the astronomical magnitude system (which is admittedly awful)\n",
    "- **planet_type**: values include \"Gas Giant\" (like Jupiter or Saturn), \"Neptune-like\" (sort of a mini gas giant or ice giant), \"Super Earth\" (thought to be rocky but up to a few times larger than Earth), and \"Terrestrial\" (comparable to or smaller than Earth)\n",
    "- **discovery_yr**: year that the discovery of the planet was published\n",
    "- **mass_ME**: mass of the planet in units of Earth masses (1 Earth mass = $5.972 \\times 10^{24}$ kg)\n",
    "- **radius_RE**: radius of the planet in units of Earth radii (1 Earth radius = 6371 km)\n",
    "- **orbital_radius_AU**: the exoplanet's orbital semi-major axis in units of the average distance between the Earth and Sun (1 Astronomical Unit, or AU)\n",
    "- **orbital_period_yr**: the time taken for the exoplanet to orbit its star in units of Earth's orbital period (1 year)\n",
    "- **eccentricity**: measure of the deviation of the exoplanet's orbit from a perfect circle. Values range from 0 to 1 where 0 = perfect circle and 1 indicates a parabolic (just barely unbound) orbit.\n",
    "- **detection_method**: principle method used to detect the planet. See [this educational slideshow by NASA](https://exoplanets.nasa.gov/alien-worlds/ways-to-find-a-planet/) or [this Wikipedia page](https://en.wikipedia.org/wiki/Methods_of_detecting_exoplanets) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9e05b-c9d3-4dd3-908f-ab323044000b",
   "metadata": {},
   "source": [
    "When I loaded the DataFrame above, note that I set `index_col=0`, that is, I told `read_csv()` to use the leftmost column as the indexes of the DataFrame. That means I can now grab the entries of any planet by name, if I know the name of the planet as it is rendered in this database. Without that command, `'#name'` is just another column, and every row is assigned a numerical index in the order in which it appears, starting at 0. The default can be useful for regularly sampled time series, but these data  The default behavior for setting column names is to assume the first row is a row of column labels, so I did not need to do anything else. If your data have column labels on another row, `read_csv()` also has the `header` and `skiprows` kwargs to specify the row with the column names and rows to ignore, respectively. The list of other kwargs for parsing and formatting data, and managing memory while you do it, is very long, so [I will refer you to the onliner documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas.read_csv) and move on.\n",
    "\n",
    "If for some reason I wanted to, for example, save this dataframe as a plain text file with pipes (|) for separators and commas in place of decimal points (don't do this with regular CSV files!), I would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82364b80-817f-49fd-b3e3-5efba81a0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('exoplanets_5250_EarthUnits.txt', sep='|',\n",
    "          decimal=',', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419c7d5-9d0f-4843-8a76-061dd635ab2c",
   "metadata": {},
   "source": [
    "Note: the index kwarg is `True` by default, but I draw it to your attention because if you allow default indexing and don't want those indexes to be saved, you may want to set that kwarg to `False`.\n",
    "\n",
    "If I wanted to append the table as a new sheet in an existing Excel file, the command would be something like:\n",
    "`df.to_csv('other_file.xlsx', mode='a', sheet_name='Sheet2')`\n",
    "\n",
    "We will go more in depth into I/O with more complex and hierarchical data later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193d500-60b4-4499-b548-5c4e32c13642",
   "metadata": {},
   "source": [
    "### Inspecting Data\n",
    "Recall that the first thing I did after importing was call this function `df.head()`. That's a good way to get an overview of your data without loading the whole thing: it displays the first 5 rows of the table with all column names and row indexes. For tables larger than 2 GB, your first view of the data is likely to be with `df.head()` simply because Excel and other text editors/viewers may refuse to load a file of that size. There is also a `df.tail()` function that outputs the last 5 rows of your data instead. Both `head()` and `tail()` accept an integer argument for the number of rows to return if you want a different number than the default 5. E.g. if you only wanted the last row, you would call `df.tail(1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17df5d54-44b9-4851-9c54-d6b38df571d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>star_mag</th>\n",
       "      <th>planet_type</th>\n",
       "      <th>discovery_yr</th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>detection_method</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>YZ Ceti b</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.074</td>\n",
       "      <td>Terrestrial</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.01634</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YZ Ceti c</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.074</td>\n",
       "      <td>Super Earth</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.02156</td>\n",
       "      <td>0.008487</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YZ Ceti d</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.074</td>\n",
       "      <td>Super Earth</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.02851</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.07</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           distance  star_mag  planet_type  discovery_yr mass_ME radius_RE  \\\n",
       "#name                                                                        \n",
       "YZ Ceti b      12.0    12.074  Terrestrial          2017    0.70     0.913   \n",
       "YZ Ceti c      12.0    12.074  Super Earth          2017    1.14      1.05   \n",
       "YZ Ceti d      12.0    12.074  Super Earth          2017    1.09      1.03   \n",
       "\n",
       "           orbital_radius_AU  orbital_period_yr  eccentricity detection_method  \n",
       "#name                                                                           \n",
       "YZ Ceti b            0.01634           0.005476          0.06  Radial Velocity  \n",
       "YZ Ceti c            0.02156           0.008487          0.00  Radial Velocity  \n",
       "YZ Ceti d            0.02851           0.012868          0.07  Radial Velocity  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fb9f5-6d14-4af3-bfed-9fdef322f7bd",
   "metadata": {},
   "source": [
    "If you have a smaller or more symmetrical table that might be more intuitively reorganized if the row and column orders were switched, you can transpose the table with `df.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ce4c9f8d-7b63-4a4f-a589-c9235b81fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    b    c     d\n",
      "0  0.5  1.0  1.5   2.0\n",
      "1  2.5  3.0  3.5   4.0\n",
      "2  4.5  5.0  5.5   6.0\n",
      "3  6.5  7.0  7.5   8.0\n",
      "4  8.5  9.0  9.5  10.0 \n",
      "\n",
      "      0    1    2    3     4\n",
      "a  0.5  2.5  4.5  6.5   8.5\n",
      "b  1.0  3.0  5.0  7.0   9.0\n",
      "c  1.5  3.5  5.5  7.5   9.5\n",
      "d  2.0  4.0  6.0  8.0  10.0\n"
     ]
    }
   ],
   "source": [
    "dummy = pd.DataFrame(np.linspace(0.5,10,20).reshape(5,4),columns=['a','b','c','d'])\n",
    "print(dummy,'\\n\\n',dummy.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923888c6-4c16-4d66-a735-23e585145637",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Other great tools for getting an overview (and sanity check) of your data are `df.info()` and `df.describe()`. `df.info()` prints the zero-based index, name, count of non-Null data, and the data type of each colum for all columns, and also briefly describes the row-indexing system and the size of the DataFrame in memory. `df.describe()` immediately outputs the count, mean, standard deviation, minimum, maximum, and quartiles of all numeric columns, automatically excluding NaNs. **Note:** integer columns are treated as floats and object-type columns are ignored even if most of the data are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "1b5523e6-5c1e-4232-983c-3604e3868c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>star_mag</th>\n",
       "      <th>discovery_yr</th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "      <th>eccentricity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5233.000000</td>\n",
       "      <td>5089.000000</td>\n",
       "      <td>5250.000000</td>\n",
       "      <td>5227.000000</td>\n",
       "      <td>5233.000000</td>\n",
       "      <td>4961.000000</td>\n",
       "      <td>5.250000e+03</td>\n",
       "      <td>5250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2167.168737</td>\n",
       "      <td>12.683738</td>\n",
       "      <td>2015.732190</td>\n",
       "      <td>460.035267</td>\n",
       "      <td>5.627083</td>\n",
       "      <td>6.962942</td>\n",
       "      <td>4.791509e+02</td>\n",
       "      <td>0.063924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3245.522087</td>\n",
       "      <td>3.107571</td>\n",
       "      <td>4.307336</td>\n",
       "      <td>3761.458727</td>\n",
       "      <td>5.315522</td>\n",
       "      <td>138.673600</td>\n",
       "      <td>1.680445e+04</td>\n",
       "      <td>0.141402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>1992.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>2.737850e-04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>389.000000</td>\n",
       "      <td>10.939000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>3.970000</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>1.259411e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1371.000000</td>\n",
       "      <td>13.543000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>8.470000</td>\n",
       "      <td>2.732800</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>3.449692e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2779.000000</td>\n",
       "      <td>15.021000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>11.715200</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>1.442163e-01</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27727.000000</td>\n",
       "      <td>44.610000</td>\n",
       "      <td>2023.000000</td>\n",
       "      <td>239136.000000</td>\n",
       "      <td>77.280000</td>\n",
       "      <td>7506.000000</td>\n",
       "      <td>1.101370e+06</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           distance     star_mag  discovery_yr        mass_ME    radius_RE  \\\n",
       "count   5233.000000  5089.000000   5250.000000    5227.000000  5233.000000   \n",
       "mean    2167.168737    12.683738   2015.732190     460.035267     5.627083   \n",
       "std     3245.522087     3.107571      4.307336    3761.458727     5.315522   \n",
       "min        4.000000     0.872000   1992.000000       0.020000     0.296000   \n",
       "25%      389.000000    10.939000   2014.000000       3.970000     1.760000   \n",
       "50%     1371.000000    13.543000   2016.000000       8.470000     2.732800   \n",
       "75%     2779.000000    15.021000   2018.000000     159.000000    11.715200   \n",
       "max    27727.000000    44.610000   2023.000000  239136.000000    77.280000   \n",
       "\n",
       "       orbital_radius_AU  orbital_period_yr  eccentricity  \n",
       "count        4961.000000       5.250000e+03   5250.000000  \n",
       "mean            6.962942       4.791509e+02      0.063924  \n",
       "std           138.673600       1.680445e+04      0.141402  \n",
       "min             0.004400       2.737850e-04      0.000000  \n",
       "25%             0.053000       1.259411e-02      0.000000  \n",
       "50%             0.102800       3.449692e-02      0.000000  \n",
       "75%             0.286000       1.442163e-01      0.060000  \n",
       "max          7506.000000       1.101370e+06      0.950000  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9a06e907-f497-4c08-87c5-7fb50afa3bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5250 entries, 11 Comae Berenices b to YZ Ceti d\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   distance           5233 non-null   float64\n",
      " 1   star_mag           5089 non-null   float64\n",
      " 2   planet_type        5250 non-null   object \n",
      " 3   discovery_yr       5250 non-null   int64  \n",
      " 4   mass_ME            5250 non-null   object \n",
      " 5   radius_RE          5250 non-null   object \n",
      " 6   orbital_radius_AU  4961 non-null   float64\n",
      " 7   orbital_period_yr  5250 non-null   float64\n",
      " 8   eccentricity       5250 non-null   float64\n",
      " 9   detection_method   5250 non-null   object \n",
      "dtypes: float64(5), int64(1), object(4)\n",
      "memory usage: 451.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b963da0-784d-42e1-8155-879276e552f6",
   "metadata": {},
   "source": [
    "The count lists of both descriptive functions immediately tell me that most columns have some missing data, and the data types for `mass_ME` and `radius_RE` are a red flag. The metrics returned by `.describe()` also tell me, given some subject-matter expertise, that at least half of the eccentricity values should be taken as assumed filler values (i.e. with a mountain of salt). In a previous iteration I also saw negative values in that column, which are physically impossible and indicated a need to update or nullify those values.\n",
    "\n",
    "**Memory Usage.** Another stat that you should take with a mountain of salt: the memory usage stat. Since most lecture attendees are HPC users, let's take a closer look with a function that many of you will need if you plan to use Pandas on NAISS resources: `.memory_usage()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "60b2065c-d534-4d74-8128-1fae00d8de9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                174136\n",
       "distance              42000\n",
       "star_mag              42000\n",
       "planet_type           42000\n",
       "discovery_yr          42000\n",
       "mass_ME               42000\n",
       "radius_RE             42000\n",
       "orbital_radius_AU     42000\n",
       "orbital_period_yr     42000\n",
       "eccentricity          42000\n",
       "detection_method      42000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage()#deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928cd81-54d8-4234-82a2-0c3f5de3ecc2",
   "metadata": {},
   "source": [
    "This function returns the size in memory of every column, plus the size of the indexes by default. You can hide the contribution of the row and column labels by setting `index=False`, and you can get an estimate of how much system-level memory the object-type data columns consume by setting `deep=True`. The latter is important to check because object-type columns can be much larger in memory than initially reported. Watch what happens when the command is rerun with `deep=True` uncommented.\n",
    "\n",
    "What's going on? [This article](https://pythonspeed.com/articles/pandas-dataframe-series-memory-usage/) provides a fuller and perhaps better explanation, but the short answer is that numerical and Boolean datatypes have fixed size in memory (e.g. an int64 or float64 number will always be 8 bytes, whether it's 1 or Avogadro's number), whereas object-type data (strings) are variable in size and usually must be stored somewhere else because they're almost always bigger. When memory is allocated for the DataFrame itself, that parcel of memory also contains all of the numerical or Boolean data, but only pointers to the object-type data. The actual values of the object-type data are stored wherever space can be found in memory, which requires significantly more overhead. When `deep=False`, as is the default, *the memory usage reported for object-type data is only what is used by the pointers*.\n",
    "\n",
    "That said, even when `deep=True`, you only get a *worst-case estimate* of the memory usage of object-type data. The actual usage as reported by a dedicated memory profile will typically be somewhat smaller because Python has some built-in string optimization routines that cache frequently used strings, and because the estimate often includes temporary structures that get deallocated. Still, if `.memory_usage()` is your only tool to check the size of your data in memory, the results when `deep=True` will generally be closer to the real value than the default output (and it's better to design around the worst-case scenario)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003b1bf-1a30-4a9c-92e8-29a3c807601e",
   "metadata": {},
   "source": [
    "Back to basics: some of the other returns of `.info()` can also be called individually for the whole DataFrame or individual columns or rows, specifically `.count()` and, as we mentioned in the attributes section, `.dtypes`. Whether you need the count(s) for the whole DataFrame or just one row or column, with the correct data selection syntax, the method is just `.count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2929e524-723a-478a-ae94-7210a8be1112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['55 Cancri e'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c1691-7e01-434e-b917-c4fdf3740b8b",
   "metadata": {},
   "source": [
    "That basically means the row labelled '55 Cancri e' has 10 data entries. If you suspect there are duplicates, you can replace `.count()` with `.nunique()`, which will return a single value for a Series or Index list, and a Series when given a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "e2256151-cc04-443b-9ab0-65097ac02cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 5250 eccentricity values, only 175 are unique.\n"
     ]
    }
   ],
   "source": [
    "print('Of', df['eccentricity'].count(), 'eccentricity values, only', \n",
    "      df['eccentricity'].nunique(), 'are unique.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b92b9-151f-4dad-8c1c-4ad7888ae230",
   "metadata": {},
   "source": [
    "There is also a `.value_counts()` method that counts every unique row-wise combination of values for however many columns you give it. It is mainly used for GroupBy objects, which we will discuss later. For now, just observe the behavior below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "265b47d6-db6b-4322-a05d-8b653a1689ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection_method\n",
      "Transit                          3945\n",
      "Radial Velocity                  1027\n",
      "Gravitational Microlensing        154\n",
      "Direct Imaging                     62\n",
      "Transit Timing Variations          24\n",
      "Eclipse Timing Variations          17\n",
      "Orbital Brightness Modulation       9\n",
      "Pulsar Timing                       7\n",
      "Astrometry                          2\n",
      "Pulsation Timing Variations         2\n",
      "Disk Kinematics                     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Compare:\n",
      "\n",
      "discovery_yr  detection_method           \n",
      "2016          Transit                        1453\n",
      "2014          Transit                         802\n",
      "2021          Transit                         415\n",
      "2018          Transit                         253\n",
      "2022          Transit                         188\n",
      "                                             ... \n",
      "2013          Astrometry                        1\n",
      "2007          Pulsation Timing Variations       1\n",
      "              Direct Imaging                    1\n",
      "2018          Transit Timing Variations         1\n",
      "              Eclipse Timing Variations         1\n",
      "Name: count, Length: 118, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['detection_method'].value_counts())\n",
    "print('\\nCompare:\\n')\n",
    "print(df[['discovery_yr', 'detection_method']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818457d-462d-4cc4-83ea-52ff53a69d0f",
   "metadata": {},
   "source": [
    "To get the datatypes of a single column, a singular version of the datatypes attribute is used, `df['col_name'].dtype` because column data are expected to all be of the same datatype. For a row, which is expected to have varying data types, the syntax is more like that used for the whole DataFrame: `df.loc['row_name'].dtypes` or `df.iloc['row_index'].dtypes` depending on whether you select the row by a custom label or the 0-based numerical index, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c79199-c6ec-4a3b-9df8-8d009c51a7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance             float64\n",
       "star_mag             float64\n",
       "planet_type           object\n",
       "discovery_yr           int64\n",
       "mass_ME               object\n",
       "radius_RE             object\n",
       "orbital_radius_AU    float64\n",
       "orbital_period_yr    float64\n",
       "eccentricity         float64\n",
       "detection_method      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96b479-89c0-4123-ab35-fb3099197892",
   "metadata": {},
   "source": [
    "As for why the mass and radius columns have type `object` instead of `float64`, let's see what happens if we try to coerce the data of those columns to the expected type (don't worry about the syntax just yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a883f9cf-d2ce-48f9-b744-3cf332e26f95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[245], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmass_ME\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:6534\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6530\u001b[0m     results \u001b[38;5;241m=\u001b[39m [ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   6532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6533\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6534\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   6535\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:414\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    412\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    416\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    417\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    418\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    419\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[1;32m    420\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py:616\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    614\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 616\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m    618\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    620\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:238\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:183\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:134\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ' '"
     ]
    }
   ],
   "source": [
    "df['mass_ME'].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae5aba-eeb2-489e-884e-112096513138",
   "metadata": {},
   "source": [
    "Aha. There's at least one whitespace in that column. Pandas was initially designed for economic data where cells might contain dates, currency symbols, addresses, and other types of data where numbers might appear with spaces, so Pandas assumes all whitespace is intentional and marks the column as `object` even if the whitespace is in an otherwise totally numerical column. Pandas will only coerce columns of numbers and missing values to `float64` if there are no non-numeric characters.\n",
    "\n",
    "Prior inspection of the data revealed that the mass and radius columns both had a mix of floats, integers (as a result of rounding to significant digits), missing values, and *cells that look empty but actually contain a space character*. Let's take a quick look at an entry where I know this has happened from looking at the original Excel file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a8d14dfb-97a4-49fa-904e-551b27a8679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance                      1308.0\n",
      "star_mag                      12.994\n",
      "planet_type                Gas Giant\n",
      "discovery_yr                    2014\n",
      "mass_ME                       343.44\n",
      "radius_RE                           \n",
      "orbital_radius_AU                NaN\n",
      "orbital_period_yr                2.2\n",
      "eccentricity                     0.0\n",
      "detection_method     Radial Velocity\n",
      "Name: Kepler-97 c, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.loc['Kepler-97 c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4bee3cee-ffc7-4caa-af8d-39fbba391e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['Kepler-97 c','radius_RE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3664e8c3-6ecf-485d-b424-6159cd6a3c96",
   "metadata": {},
   "source": [
    "If I select the whole row or a subset of the columns, the value in the radius column just looks empty, but if I look at just the *'radius_RE'* column for this planet, it shows the single space in quotes.\n",
    "\n",
    "Pandas devotes a substantial fraction of its functional library to making it easy to deal with malformed data. An issue like this could be fixed at import with the `converters` kwarg of `read_csv()` if you know about it a priori. I held off so I can show you a little later how to find and handle different types of missing data within Python, as you might if your typical choice of file viewer chokes on the size of the data.\n",
    "\n",
    "To get there, we need to dive into how subsets of Pandas DataFrames are selected, so you can understand the use of `df.loc[...]` and `df.iloc[...]` among other selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca36a0-fc5a-475a-a5b0-a1dd5ff0e479",
   "metadata": {},
   "source": [
    "### Accessing Data\n",
    "Access by column(s) and/or row(s) is simple in principle, though it can be error prone if you're used to NumPy arrays and Python dictionaries. The official documentation can be a bit verbose if you're just trying to remember when to use `.loc`, `.iloc`, or neither. The following summary table should help:\n",
    "\n",
    "| To Access... | Syntax |\n",
    "| :--- | :--- |\n",
    "| 1 column | `df['col_name']` |\n",
    "| 1 named row | `df.loc['row_name']` |\n",
    "| 1 row by index | `df.iloc[index]` |\n",
    "| 1 column by index (rarely used) | `df.iloc[:,index]` |\n",
    "| subset of columns | `df[['col0', 'col1', 'col2']]` |\n",
    "| subset of named rows | `df.loc[['rowA','rowB','rowC']]` |\n",
    "| subset of rows by index | `df.iloc[i_m:i_n]`  where *i_m* & *i_n* are the m<sup>th</sup> & n<sup>th</sup> integer indexes |\n",
    "| rows & columns by name | `df.loc['row','col']` or `df.loc[['rowA','rowB', ...],['col0', 'col1', ...]]` |\n",
    "| rows & columns by index | `df.iloc[i_m:i_n, j_p:j_q]` where *i* & *j* are row & column indexes, respectively |\n",
    "| columns by name & rows by index | `df[['col0', 'col1', 'col2']].iloc[i_m:i_n]` |\n",
    "\n",
    "**Columns** alone can be selected by name in square brackets (`[]`) like ordinary dictionary entries. **Rows**, with or without columns, must be accessed by adding either `.loc` if selection is by name, or `.iloc` if selection is by index, between the name of the DataFrame and the `[]`. If you need to select both rows and columns, the row and column names must be given in row-major order, as with most other Python array functions: `[row(s), col(s)]`. Also note that `.iloc[]` is endpoint-exclusive like regular Python array slicing operations, while `.loc[]` is endpoint-inclusive. \n",
    "\n",
    "As a reminder, if you just want to view the column labels or row labels/indexes, the the commands are `df.columns` and `df.index`, respectively. Both return Pandas Series of type `Index`, which can be used with any Pandas method that works on Series but *cannot* be directly input into NumPy functions or list comprehension. If you need the output of either command to be a list or an array, add `.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "eedac922-ab2b-4d83-92f5-20f235c0d0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance                       409.0\n",
      "star_mag                       5.013\n",
      "planet_type                Gas Giant\n",
      "discovery_yr                    2009\n",
      "mass_ME                      4687.32\n",
      "radius_RE                     12.208\n",
      "orbital_radius_AU               1.53\n",
      "orbital_period_yr                1.4\n",
      "eccentricity                    0.08\n",
      "detection_method     Radial Velocity\n",
      "Name: 11 Ursae Minoris b, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e1c6cddf-1732-4686-b2e5-6d878818be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                planet_type  mass_ME\n",
      "#name                               \n",
      "51 Eridani b      Gas Giant   636.00\n",
      "51 Pegasi b       Gas Giant   146.28\n",
      "55 Cancri b       Gas Giant   264.13\n",
      "55 Cancri c       Gas Giant    54.51\n",
      "55 Cancri d       Gas Giant  1233.20\n",
      "55 Cancri e     Super Earth     7.99\n",
      "55 Cancri f       Gas Giant    44.84\n",
      "61 Virginis b  Neptune-like     5.10\n",
      "61 Virginis c  Neptune-like    18.20\n",
      "61 Virginis d  Neptune-like    22.90\n"
     ]
    }
   ],
   "source": [
    "print(df[['planet_type','mass_ME']].iloc[25:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d98a8-2d27-45e3-a0e8-1e6680d7c9d5",
   "metadata": {},
   "source": [
    "#### Conditional Data Selection\n",
    "In practice, you will very often need to filter rows (and columns) by conditions rather than names or indexes. Conditional operators (`==`, `>`, `<`, `=>`, `=<`, and `!=`) return boole-type Pandas Series that are used to select and return subsets of the input data for which the condition is `True`. For a single condition, the syntax is fairly intuitive once you've memorized the previous syntax table. To filter by multiple conditions, however, there are a few extra things to remember: \n",
    "1. To combine multiple conditions, you must use the \"bitwise or\" pipe operator `|` and the \"bitwise and\" ampersand operator `&`, instead of the usual `or` or `and`, respectively. The \"bitwise exclusive or\" operator `^` and \"bitwise not\" operator `~` are also available.\n",
    "2. Each condition must be enclosed in parentheses `()` so that all conditions will be evaluated to boole-type Pandas Series that the bitwise operators can safely combine. Typically, forgetting the `()` will show up as a TypeError since conditions are often combined across differently-typed columns, but if you want to understand why errors result from missing `()`, it is helpful to know that the filtering expression `df['A']>2 & df['B']<=5` would be evaluated as `df['A']>(2 & df['B']<=5)`. (Believe me, if I knew why, I would submit a push request to change this behavior.)\n",
    "3. If you want to filter by a list of values, it is better to use the `.isin()` attribute than a bitwise chain of conditions. Syntax: `df['col'].isin([value1, value2, ...])`. Note that this does *not* work the other way around, i.e. you cannot use `.isin()` to check if column, row, or cell entries contain a substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b5146eff-1a08-45a0-bf5b-e396b676d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        distance  star_mag  planet_type  discovery_yr  mass_ME\n",
      "#name                                                                         \n",
      "16 Cygni B b                69.0   6.21500    Gas Giant          1996   566.04\n",
      "47 Ursae Majoris b          45.0   5.03352    Gas Giant          1996   804.54\n",
      "51 Pegasi b                 50.0   5.45309    Gas Giant          1995   146.28\n",
      "55 Cancri b                 41.0   5.95084    Gas Giant          1996   264.13\n",
      "70 Virginis b               58.0   4.96808    Gas Giant          1996  2381.82\n",
      "GJ 876 b                    15.0  10.16000    Gas Giant          1998   723.64\n",
      "HD 168443 b                129.0   6.92122    Gas Giant          1998  2424.15\n",
      "HD 187123 b                150.0   7.83000    Gas Giant          1998   166.31\n",
      "HD 195019 b                123.0   6.87591    Gas Giant          1998  1265.64\n",
      "HD 210277 b                 69.0   6.54348    Gas Giant          1998   410.22\n",
      "HD 217107 b                 65.0   6.15500    Gas Giant          1998   413.40\n",
      "PSR B1257+12 b            1957.0       NaN  Terrestrial          1994     0.02\n",
      "PSR B1257+12 c            1957.0       NaN  Super Earth          1992     4.30\n",
      "PSR B1257+12 d            1957.0       NaN  Super Earth          1992     3.90\n",
      "Rho Coronae Borealis b      57.0   5.40816    Gas Giant          1997   332.28\n",
      "Tau Bootis b                51.0   4.48635    Gas Giant          1996  1892.10\n",
      "Upsilon Andromedae b        44.0   4.09565    Gas Giant          1996   218.66 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[df['discovery_yr'] < 1999].iloc[:, :5],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "576e7b97-6826-4285-b310-d8ebffe21882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#name\n",
      "55 Cancri e              Super Earth\n",
      "GJ 436 b                Neptune-like\n",
      "GJ 581 b                Neptune-like\n",
      "GJ 876 d                Neptune-like\n",
      "HD 160691 d             Neptune-like\n",
      "HD 190360 c             Neptune-like\n",
      "HD 4308 b               Neptune-like\n",
      "HD 49674 b              Neptune-like\n",
      "HD 69830 b              Neptune-like\n",
      "HD 69830 c              Neptune-like\n",
      "HD 69830 d              Neptune-like\n",
      "HD 99492 b              Neptune-like\n",
      "OGLE-2005-BLG-169L b    Neptune-like\n",
      "OGLE-2005-BLG-390L b    Neptune-like\n",
      "PSR B1257+12 b           Terrestrial\n",
      "PSR B1257+12 c           Super Earth\n",
      "PSR B1257+12 d           Super Earth\n",
      "Name: planet_type, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[ (df['discovery_yr'] < 2007) &\n",
    "              (df['planet_type'] != 'Gas Giant'),\n",
    "      'planet_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "dd3c084a-5ab8-475a-a29f-c98458ce49b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#name\n",
      "PSR B1257+12 b    Terrestrial\n",
      "PSR B1257+12 c    Super Earth\n",
      "PSR B1257+12 d    Super Earth\n",
      "Name: planet_type, dtype: object\n",
      "\n",
      "...looks the same as...\n",
      "\n",
      "#name\n",
      "PSR B1257+12 b    Terrestrial\n",
      "PSR B1257+12 c    Super Earth\n",
      "PSR B1257+12 d    Super Earth\n",
      "Name: planet_type, dtype: object\n",
      "\n",
      "...but only use the first version!\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[(df.index.str.contains('PSR')) &\n",
    "             (df['discovery_yr'] < 2000), 'planet_type'])\n",
    "print('\\n...looks the same as...\\n')\n",
    "print(df[(df.index.str.contains('PSR')) &\n",
    "         (df['discovery_yr'] < 2000)]['planet_type'])\n",
    "print(\"\\n...but only use the first version!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec1bdc5-8acb-4b68-a151-5f193e39be1c",
   "metadata": {},
   "source": [
    "Here I'm touching on a rather complex topic: [chain indexing](https://pandas.pydata.org/docs/user_guide/indexing.html#why-does-assignment-fail-when-using-chained-indexing). We don't have time to get into this in depth, but the gist is that if it looks like you can access data either with `df[y][x]` or `df.loc[y,x]`, where *x* is a column or subset of columns and *y* is a row label or row-filtering condition, **you should always prefer the .loc[] format**. The syntax that looks like standard nested list or nested dict selection is called chain indexing, and with DataFrames, it's hard to know whether it will return a copy of the selected data or a view of the original data. It's also not always this obvious when you've used chained indexing, so Pandas is programmed to raise a `SettingWithCopy` warning to help you avoid the frustration.\n",
    "\n",
    "I also want to call your attention to this snippet:\n",
    "`df.index.str.contains('PSR')`\n",
    "Here we took advantage of one of the more brilliant features of Pandas: **string vectorization**. Many string functions can be called on and broadcast to any Pandas Series of type `object` (`string`) or `Index` by adding the attribute `.str` and then a string method of your choice. You can also append `.str.method()`, where `method` is any string method, directly after another method that returns a Series with string-like contents, like the example above. The official documentation contains a helpful [summary table of allowed string operations](https://pandas.pydata.org/docs/user_guide/text.html#method-summary) and detailed discussions of how to use methods with multiple input/output options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39fa88-3611-4099-9d8a-7fdd1f5367aa",
   "metadata": {},
   "source": [
    "#### The .query() method\n",
    "Pandas has a `.query()` method that can make conditional selection simpler and more readable. It works a bit like the built-in `eval()` and `exec()` functions (in fact, it uses these functions under-the-hood) in that it takes your filter conditions as a string, and allows you to use the plain-English versions\\* (`and`, `or` etc.) of the bitwise binary operators discussed in the previous section. It lets you use the word `index` if you don't know the name of the index label you're looking for, and lets you filter either index or column\\*\\* values with the same syntax *as long as all column and index labels are unique.* You can even reference variables within the query statement by prefixing variable names with the `@` symbol.\n",
    "\n",
    "\\*Binary operations are not implemented for some data types. If the array is coerced to `ExtensionArray` type, the plain-English forms of certain binary operators (namely `is` and `is not`) may raise a `NotImplemented` error.\n",
    "\n",
    "\\*\\*Note that you can both select and filter by rows, but can only filter by columns. Column selection must be made via chain indexing, which means the `.query()` method is not suitable for assigning values.\n",
    "\n",
    "Let's rewrite the previous data selection command with the `.query()` function, and let the year vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "70275e56-fb13-49d1-8e50-e9caea2a5bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#name\n",
       "PSR B1257+12 b    Terrestrial\n",
       "PSR B1257+12 c    Super Earth\n",
       "PSR B1257+12 d    Super Earth\n",
       "PSR B1620-26 b      Gas Giant\n",
       "Name: planet_type, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2009\n",
    "df.query(\"index.str.contains('PSR') and \\\n",
    "        `discovery_yr` < @y\")['planet_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea6dc-2f9b-4348-9138-0f77e106b11d",
   "metadata": {},
   "source": [
    "\n",
    "**Very important:** to distinguish column and row names from other strings within the query statement, you must bracket column and row labels with *grave accents* (\\`\\`) instead of single or double quotes.\n",
    "\n",
    "It does not matter whether you use single quotes for strings within the query and double quotes for the whole statement, or vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3e82e-39af-4ecf-b676-1d9703bc0c9e",
   "metadata": {},
   "source": [
    "#### Finding and handling invalid data\n",
    "Pandas provides a couple of convenience functions for selecting only invalid or only valid data from your DataFrame or any subset of it that is at least a Series: `.isna()` and `.notna()`. You may also see `.isnull()` and `.notnull()`, but these are aliases for `.isna()` and `.notna()`, respectively, and the use of the `na` versions is generally preferred over `null`. The `.isna()` method selects both NaNs and None values, but not $\\pm$infinity and, as I mentioned earlier, not whitespaces in otherwise numerical columns. Infinite values and whitespaces-as-placeholders need to be replaced with NaN or None in order to take advantage of Pandas' built-in filters for invalid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6d0e49-9942-4565-b603-b402e9e7934b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>planet_type</th>\n",
       "      <th>discovery_yr</th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CI Tauri b</th>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2019</td>\n",
       "      <td>3688.80</td>\n",
       "      <td>12.432</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoRoT-7 d</th>\n",
       "      <td>Neptune-like</td>\n",
       "      <td>2022</td>\n",
       "      <td>17.14</td>\n",
       "      <td>4.3008</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DS Tucanae A b</th>\n",
       "      <td>Neptune-like</td>\n",
       "      <td>2019</td>\n",
       "      <td>413.40</td>\n",
       "      <td>5.7008</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPIC 201238110 b</th>\n",
       "      <td>Super Earth</td>\n",
       "      <td>2019</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPIC 201427007 b</th>\n",
       "      <td>Super Earth</td>\n",
       "      <td>2021</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   planet_type  discovery_yr  mass_ME radius_RE  \\\n",
       "#name                                                             \n",
       "CI Tauri b           Gas Giant          2019  3688.80    12.432   \n",
       "CoRoT-7 d         Neptune-like          2022    17.14    4.3008   \n",
       "DS Tucanae A b    Neptune-like          2019   413.40    5.7008   \n",
       "EPIC 201238110 b   Super Earth          2019     4.16      1.87   \n",
       "EPIC 201427007 b   Super Earth          2021     2.86       1.5   \n",
       "\n",
       "                  orbital_radius_AU  \n",
       "#name                                \n",
       "CI Tauri b                      NaN  \n",
       "CoRoT-7 d                       NaN  \n",
       "DS Tucanae A b                  NaN  \n",
       "EPIC 201238110 b                NaN  \n",
       "EPIC 201427007 b                NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['orbital_radius_AU'].isna()].iloc[:5,2:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d632d8-11b7-4568-88f9-063f424aa547",
   "metadata": {},
   "source": [
    "Now, let's return to the example of Kepler-97 c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9cb0e269-739c-4521-8875-2a0185c88cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance                      1308.0\n",
      "star_mag                      12.994\n",
      "planet_type                Gas Giant\n",
      "discovery_yr                    2014\n",
      "mass_ME                       343.44\n",
      "radius_RE                           \n",
      "orbital_radius_AU                NaN\n",
      "orbital_period_yr                2.2\n",
      "eccentricity                     0.0\n",
      "detection_method     Radial Velocity\n",
      "Name: Kepler-97 c, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.loc['Kepler-97 c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259ce46-564b-4ac2-adda-0eb4f50a9c7a",
   "metadata": {},
   "source": [
    "As you can see, there's a NaN in the orbital radius entry and a blank in the planet radius entry. Let's also double-check which columns have the whitespace filler problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f2bb5425-40ce-4fe1-8250-07814651d0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance             float64\n",
      "star_mag             float64\n",
      "planet_type           object\n",
      "discovery_yr           int64\n",
      "mass_ME               object\n",
      "radius_RE             object\n",
      "orbital_radius_AU    float64\n",
      "orbital_period_yr    float64\n",
      "eccentricity         float64\n",
      "detection_method      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96468ddb-f06a-4393-a10b-6cd3e84ee98e",
   "metadata": {},
   "source": [
    "The only unexpected types are for mass and radius, which should be floats, so let's go ahead and fix those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a04092-d4e1-4d00-b483-c45e4a4d180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mass_ME'] = df['mass_ME'].replace(' ', np.nan).astype('float64')\n",
    "df['radius_RE'] = df['radius_RE'].replace(' ', np.nan).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1f1a9-a1e9-4610-b675-dc584548db2e",
   "metadata": {},
   "source": [
    "Note that the `.replace()` method I used here is **not** the vector string method, but a Series/DataFrame method of the same name. We'll get to more Series and DataFrame methods later when we talk about operations.\n",
    "\n",
    "Once all the white-spaces have been converted to NaN and the type is converted to float, it becomes possible to use `.isna()` and `.notna()`, and a number of other methods and function kwargs for handling NaNs become available:\n",
    "- `df.fillna(fill_value, inplace=False)` systematically replaces NaNs or Nones with the specified fill value. If the data are object- or string-type, *fill_value* can be a string.\n",
    "- `df.dropna(axis=axis, inplace=False)` drops rows (axis=0) or columns (axis=1) with missing values.\n",
    "- If you need to interpolate over missing data, you can use `ser.interpolate(method=method)` where ser is a Series of numerical or time-like data and method can be 'linear', 'time', 'index', or a SciPy interpolation method. Technically you can use `interpolate()` on a DataFrame as well, but a DataFrame may not be the best format to store image-like data. If you need access-by-label capabilities for images and image cubes, the module you probably want is [Xarray](https://xarray.dev/).\n",
    "- Many numerical methods like `.mean()` and `.cumsum()` have a `skipna` kwarg to control whether or not NaNs are included in the calculation (default is to skip them).\n",
    "\n",
    "**Real-valued bad data.** If you have numerical data that you know to be bad (e.g. all those perfect 0's in the `eccentricity` column of the example DataFrame), or if there are infinities that you would prefer to be masked, there is the `df.mask(condition, other=None)` function, where you can pass conditional selection criteria and use the `other` kwarg to provide substitute values in the form of a scalar, Series, DataFrame, or callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c30baab8-8464-460c-a966-a2c7228408dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#name\n",
      "11 Comae Berenices b    0.23\n",
      "11 Ursae Minoris b      0.08\n",
      "14 Andromedae b         0.00\n",
      "14 Herculis b           0.37\n",
      "16 Cygni B b            0.68\n",
      "Name: eccentricity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['eccentricity'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29049e8d-1427-4b1c-b904-20be8c7f39e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#name\n",
      "11 Comae Berenices b    0.23\n",
      "11 Ursae Minoris b      0.08\n",
      "14 Andromedae b          NaN\n",
      "14 Herculis b           0.37\n",
      "16 Cygni B b            0.68\n",
      "Name: eccentricity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['eccentricity'].mask(df['eccentricity']==0.0).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b1bba-f6f8-4d88-86d7-e9ccbc187692",
   "metadata": {},
   "source": [
    "### Reindexing, Sorting, Comparing, and Combining\n",
    "Have more than one DataFrame? Do they have partially overlapping column or index labels? Do you worry that some data are duplicated across two DataFrames? No problem! Pandas has functions for a variety of methods to aggregate multiple DataFrames in whole or in part, and to compare two DataFrames with identical row/column labels. However, to do the comparisons and combine the data sensibly, two the indexes and column names of the two or more data structures to be compared or combined must be in a reproducible order, so I will start with sorting and reindexing.\n",
    "\n",
    "#### Reindexing\n",
    "If indexes & column names are missing or need to be updated, you can modify them in-place with `.reindex(index=rows, columns=cols)` (kwargs optional). You can also use `.reindex()` to select data where some index values used as args may not exist, without raising exceptions. There is a related `.reindex_like()` function that lets you copy the data from the DataFrame you call it on into a DataFrame with the row & column labels of the DataFrame you enter as the function arg.\n",
    "\n",
    "There is a `.searchsorted(values)` Series method that returns indexes at which to insert values to maintain order, like NumPy's `.searchsorted()` version, but `.reindex()` makes it somewhat redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c4d53a2-d7cf-4185-acfe-4cfa15e271f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a  b  c\n",
      "38  0  3  0\n",
      "42  1  1  2\n",
      "36  6  3  4\n",
      "48  2  7  1\n",
      "      a    b    c\n",
      "36  6.0  3.0  4.0\n",
      "38  0.0  3.0  0.0\n",
      "40  NaN  NaN  NaN\n",
      "42  1.0  1.0  2.0\n",
      "44  NaN  NaN  NaN\n",
      "46  NaN  NaN  NaN\n",
      "48  2.0  7.0  1.0\n"
     ]
    }
   ],
   "source": [
    "dummy = pd.DataFrame(np.random.randint(0,high=9,\n",
    "                                       size=(4,3)),\n",
    "                     columns = ['a','b','c'],\n",
    "                     index = [38,42,36,48])\n",
    "print(dummy)\n",
    "print(dummy.reindex(np.arange(36,50,2),\n",
    "                    axis='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9d0ac646-4abc-4d7f-b335-0dfebb12d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    b    c\n",
      "38 NaN  0.0  1.0\n",
      "42 NaN  NaN  NaN\n",
      "36 NaN  NaN  NaN\n",
      "48 NaN  6.0  7.0\n"
     ]
    }
   ],
   "source": [
    "dummy2 = pd.DataFrame(\n",
    "    np.arange(0,9).reshape(3,3),\n",
    "    columns = ['b','c','d'],\n",
    "    index = [38,43,48])\n",
    "print(dummy2.reindex_like(dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a55eaa-f172-498b-8ccc-02dc14167463",
   "metadata": {},
   "source": [
    "#### Sorting\n",
    "Both DataFrames and Series can be sorted by index, by value, or some combination of both. In the case of DataFrames, row indexes and column labels can be sorted simultaneously. There are 2 methods to sort both Series & DataFrames: `.sort_values(by=row_or_col, axis=0, kind='quicksort')` & `.sort_index(axis=0)`.\n",
    "\n",
    "In both cases, the axis given to the `axis` kwarg is the direction along which values will shift, not the fixed axis. Both return copies unless `inplace=True`, and both have a `key` kwarg that accepts a *vectorized* function (more on those shortly) to apply to the input index before sorting. The `key` usage as described in the official documentation is a bit misleading in its brevity: *it does not alter what indexes are printed, but instead internally alters what indexes the sorting algorithm sees.* For example if you have columns you want to sort alphabetically, but only some of the column names are capitalized, the `.sort_index()` function would normally place any column label starting with a lower-case letter after any column with a capitalized label. To treat capital and lower-case letters equally, you would have to give `key` a function that converts the whole list of indexes to lower-case letters, as I demonstrate below.\n",
    "\n",
    "`.sort_values(by=row_or_col, kind='quicksort')` sorts Series or DataFrames by the values of the given column(s)/row(s) passed to the `by` kwarg (optional for Series). If `by` is one or more row indexes, it is mandatory to set `axis=1` or `axis='columns'`. If `by` is a list, the sorting order may depend on the algorithm given for the `kind` kwarg. I leave it as an exercise to the reader to look up various sorting methods to see how they might interact with the material. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21ed8a62-65f2-4976-9793-cfe01db05839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by column C\n",
      "    B  a  C\n",
      "k  7  5  1\n",
      "j  0  3  2\n",
      "h  6  2  5\n",
      "i  0  1  8\n",
      "Sorted by row j\n",
      "    B  C  a\n",
      "h  6  5  2\n",
      "i  0  8  1\n",
      "j  0  2  3\n",
      "k  7  1  5\n"
     ]
    }
   ],
   "source": [
    "dummy = pd.DataFrame(np.random.randint(0,high=9,size=(4,3)),\n",
    "                     columns = ['B','a','C'],\n",
    "                     index = ['h','i','j','k'])\n",
    "print(\"Sorted by column C\\n\",dummy.sort_values('C',axis=0))\n",
    "print(\"Sorted by row j\\n\",dummy.sort_values('j',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "668a3ef8-131a-4c4d-b693-200758369f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns sorted alphabetically\n",
      "    B  C  a\n",
      "h  8  7  1\n",
      "i  6  1  5\n",
      "j  8  6  4\n",
      "k  0  5  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns sorted alphabetically\\n\",\n",
    "      dummy.sort_index(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3391df01-3655-4665-9875-76946315536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns sorted alphabetically with key\n",
      "    a  B  C\n",
      "h  5  3  8\n",
      "i  6  2  1\n",
      "j  7  2  1\n",
      "k  1  8  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns sorted alphabetically with key\\n\",\n",
    "      dummy.sort_index(axis=1,key=lambda c: c.str.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999cefa-e85f-41b5-bff2-8859642fb4c8",
   "metadata": {},
   "source": [
    "#### Combining DataFrames\n",
    "There are 4 Pandas functions and 3 DataFrame methods that can be used to combine 2 (or sometimes more) DataFrames (or Series). The Pandas functions are:\n",
    "1. `.concat()`: combine 2 or more DataFrames or Series along a shared column or index, with optional SQL-style set logic for handling other axes using the `join` kwarg. Most of the time this works just as expected: 2 DataFrames will be stacked either horizontally (axis=1) or vertically (axis=0) depending on your choice of axis. You can also set `ignore_index=True` if you want to combine tables while ignoring abritrary indexes on the specified axis (e.g. the default 0-based indexing does not need to be an obstacle to merging)\n",
    "2. `.merge(left_df, right_df, how='inner')`: combine 2 or more DataFrames with SQL-database-style join options specified using the `how` kwarg.\n",
    "3. `.merge_ordered()`: combine 2 sorted DataFrames or Series with optional interpolation across gaps.\n",
    "4. `.merge_asof()`: left-join 2 sorted DataFrames or Series by the nearest value of given index instead of requiring identical indexes. Since these are typically used for time series, I'll wait and cover it in that section.\n",
    "\n",
    "And the DataFrame methods are:\n",
    "1. `df1.combine_first(df2)`: update missing values of DataFrame `df1` with fill values from DataFrame `df2` at shared index locations, and add rows or columns from `df2` that did not exist in `df1`.\n",
    "2. `df1.combine(df2, func)`: merge 2 DataFrames column-wise based on given function `func` that takes 2 Series & outputs either Series or scalars. Scalar outputs will be propagated to the whole column. The resulting DataFrame's row and column labels will be the union of the row and column labels of both DataFrames.\n",
    "3. `df1.join(df2, on=[cols,or,inds])` (uses `.merge()` internally): join 2 DataFrames on given column(s) or index(es).\n",
    "\n",
    "The easiest to understand is `.concat()`, so I'll start with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0854281e-c14d-4632-b5b6-d95e5cc75adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   B  a  C\n",
      "h  6  2  5\n",
      "i  0  1  8\n",
      "j  0  3  2\n",
      "k  7  5  1\n"
     ]
    }
   ],
   "source": [
    "#start with dummy dataframe from before\n",
    "print(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b31b0c1f-bfa9-42b8-beb1-3002de6d325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   B   a   C\n",
      "q  0   1   2\n",
      "r  3   4   5\n",
      "s  6   7   8\n",
      "t  9  10  11\n"
     ]
    }
   ],
   "source": [
    "#define second dummy with more predicatable values\n",
    "dummy2 = pd.DataFrame(np.arange(0,12).reshape(4,3),\n",
    "                        columns = ['B','a','C'],\n",
    "                        index = ['q','r','s','t'])\n",
    "print(dummy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3efca94-cb42-402f-ab0d-21d1fa075e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   h  i  j  k  q  r  s   t\n",
      "B  6  0  0  7  0  3  6   9\n",
      "a  2  1  3  5  1  4  7  10\n",
      "C  5  8  2  1  2  5  8  11\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat((dummy.T,dummy2.T), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88b70672-08a2-4ba9-8aa4-43c9995ee0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   B   a   C\n",
      "0  6   2   5\n",
      "1  0   1   8\n",
      "2  0   3   2\n",
      "3  7   5   1\n",
      "4  0   1   2\n",
      "5  3   4   5\n",
      "6  6   7   8\n",
      "7  9  10  11\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat((dummy,dummy2), axis=0, ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2308fdc-1e64-4044-80f6-e5969d186bdf",
   "metadata": {},
   "source": [
    "The above didn't combine any rows because none of them had identical indexes, but if we change that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e242592f-76bf-4a7b-b4cc-de30aaf33dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     B     a     C\n",
      "0  6.0   2.0   5.0\n",
      "1  0.0   1.0   8.0\n",
      "2  NaN   NaN   NaN\n",
      "3  NaN   NaN   NaN\n",
      "4  0.0   1.0   2.0\n",
      "5  3.0   4.0   5.0\n",
      "6  6.0   7.0   8.0\n",
      "7  9.0  10.0  11.0\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat((dummy.reindex(index=['h','i','q','r']),dummy2),\n",
    "                axis=0, ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5d599-534a-40a2-9d8a-d68c76fd359d",
   "metadata": {},
   "source": [
    "...Then where indexes overlap, the data in the first DataFrame are discarded, which is not exactly what you would think from the documentation (and probably not what you want either). So be careful.\n",
    "\n",
    "Before demonstrating `.merge()` and `.join()`, it will help to go over the meanings of the different SQL-style join options that you can pass to them with the `how` kwarg. The allowed options for the `how` kwarg are:\n",
    "1. `'inner'` (default): take only the intersection of the 2 DataFrames in terms of their column headers and contents at specific row *positions*, like SQL `inner join`.\n",
    "2. `'outer'`: align on any shared values at shared row and column indexes but keep all contents of both DataFrames, like SQL `full outer join`. The resulting DataFrame will have all non-rendundant permutations of the row and column indexes of both DataFrames, with NaNs inserted wherever those row and column combinations do not point to any existing data.\n",
    "3. `'left'`: keep all contents of the left (first) DataFrame, plus any data from the right (second) that share row and column indexes from the left DataFrame, like SQL `left outer join`.\n",
    "4. `'right'`: keep all contents of the right (second) DataFrame, plus any data from the left (first) that share row and column indexes from the right DataFrame, like SQL `right outer join`.\n",
    "5. `'cross'`: take the Cartesian product\\* of the two DataFrames, prioritizing the order of the left one, like SQL `cross join`.\n",
    "\n",
    "\\*If you don't know/remember what a Cartesian product is, imagine you have 2 sets of data, A=(x,y,z) and B=(1,2,3). The Cartesian product, A$\\times$B, would then be the 9 ordered pairs in the following table:\n",
    "|A$\\times$B| 1 | 2 | 3 |\n",
    "|---|---|---|---|\n",
    "| **x** | (x,1) | (x,2) | (x,3) |\n",
    "| **y** | (y,1) | (y,2) | (y,3) |\n",
    "| **z** | (z,1) | (z,2) | (z,3) |\n",
    "\n",
    "\n",
    "The quickest way to show what the first 4 cases mean intuitively is with the following Venn diagrams, where the contents of each circle are the unique combinations of row and column labels in each DataFrame ([source](https://www.ionos.co.uk/digitalguide/hosting/technical-matters/sql-outer-join/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2a1a372a-132a-46a2-a1af-78a931e75e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.ionos.co.uk/digitalguide/fileadmin/DigitalGuide/Screenshots_2018/Outer-Join.jpg\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://www.ionos.co.uk/digitalguide/fileadmin/DigitalGuide/Screenshots_2018/Outer-Join.jpg',\n",
    "      width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32885b-69e9-4bd7-b6f6-94c9c2dcf5b2",
   "metadata": {},
   "source": [
    "`Cross` joining is best demonstrated like this ([Source](https://www.sqlshack.com/sql-cross-join-with-examples/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ec4e870b-203e-49eb-a56f-255c1382adc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.sqlshack.com/wp-content/uploads/2020/02/sql-cross-join-working-mechanism.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://www.sqlshack.com/wp-content/uploads/2020/02/sql-cross-join-working-mechanism.png',\n",
    "      width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf87843-ce41-4425-b0d6-1ecdefe2da9f",
   "metadata": {},
   "source": [
    "\n",
    "In all but the `'outer'` case, the  the order of indexes or keys in the first DataFrame is preserved unless the `sort` kwarg is True or other kwargs are used to manipulate the indexes. In the `'outer'` case, column indexes are sorted alpha-numerically. In most cases, row indexes/labels are not preserved with `merge()` but for `join()` they often are.\n",
    "\n",
    "By default, `merge()` scans all columns with shared names across both DataFrames looking for rows where identical data share identical column names and row *positions* (not labels, but underlying 0-based numerical indexes), and aligns the DataFrames based on those intersections. If there are multiple possibilities, `inner ` returns all of those rows (and only those rows); `outer` returns all of the content aligned with those rows as much as possible with NaNs for padding; and `left` and `right` return the selected DataFrame with the columns of the other DataFrame with matching data appended at the rows where the data aligned, and NaNs in the rows without matching data.\n",
    "\n",
    "If you only want to align on a subset of the shared data, you can provide column labels to the `on` kwarg, but the merger will only succeed if **both** of the following conditions are met:\n",
    "1. The column labels provided exist in both DataFrames.\n",
    "2. Both DataFrames have the same data in the specified columns *at the same row positions*.\n",
    "\n",
    "You can also specify `left|right_on` alignments separately, and/or `left|right_index=True` to align on row indexes from the left or right DataFrame, but these can be very tricky to use successfully. The examples below will clarify some of these details, but it would take much more time and verbiage than either I or even the official Pandas documentation can demonstrate how all of the different kwargs interact with each other and with varying sizes of DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d150ef-af8b-4219-bc27-2c4649a122ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(re)define more orderly dummy dataframes as needed for examples\n",
    "dummy2 = pd.DataFrame(np.arange(0,12).reshape(4,3),\n",
    "                        columns = ['A','B','C'],\n",
    "                        index = ['e','f','g','h'])\n",
    "dummy3 = pd.DataFrame(np.arange(-5,11).reshape(4,4),\n",
    "                        columns = ['B','C','D', 'E'],\n",
    "                        index = ['f','g','h','i'])\n",
    "#dummy3.loc['g',['B','C']] = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f24dc2f-0f7b-4070-9090-46775c2e197d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B   C\n",
      "e  0   1   2\n",
      "f  3   4   5\n",
      "g  6   7   8\n",
      "h  9  10  11 \n",
      "\n",
      "   B  C  D   E\n",
      "f -5 -4 -3  -2\n",
      "g -1  0  1   2\n",
      "h  3  4  5   6\n",
      "i  7  8  9  10 \n",
      "\n",
      "    A  B_x  C_x  B_y  C_y  D   E\n",
      "0   0    1    2   -5   -4 -3  -2\n",
      "1   0    1    2   -1    0  1   2\n",
      "2   0    1    2    3    4  5   6\n",
      "3   0    1    2    7    8  9  10\n",
      "4   3    4    5   -5   -4 -3  -2\n",
      "5   3    4    5   -1    0  1   2\n",
      "6   3    4    5    3    4  5   6\n",
      "7   3    4    5    7    8  9  10\n",
      "8   6    7    8   -5   -4 -3  -2\n",
      "9   6    7    8   -1    0  1   2\n",
      "10  6    7    8    3    4  5   6\n",
      "11  6    7    8    7    8  9  10\n",
      "12  9   10   11   -5   -4 -3  -2\n",
      "13  9   10   11   -1    0  1   2\n",
      "14  9   10   11    3    4  5   6\n",
      "15  9   10   11    7    8  9  10\n"
     ]
    }
   ],
   "source": [
    "#demonstrate 'outer', 'inner', 'left', 'right', 'cross'\n",
    "print(dummy2,'\\n')\n",
    "print(dummy3,'\\n')\n",
    "print(pd.merge(dummy2,dummy3, how='cross'))#, on=['B','C']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32bd0a-444e-4202-8c1c-cac1e4420246",
   "metadata": {},
   "source": [
    "For the `on` kwarg, you should try to avoid providing more than one column at a time unless you know for sure the values in both DataFrames are the same at those columns *and in the same rows*. In the above example, I can use `['B','C']` with `how='inner'` only because in both DataFrames there exists a row with a 7 in column `B` *and* an 8 in column `C`, i.e. row `g` of the first DataFrame, and row `i` of the second. Therefore, the inner merge result of `pd.merge(dummy2,dummy3, on=['B','C'], how='inner')` is the union of those two rows, which in turn are the intersection of the two DataFrames. In fact, this intersection is the only possible point of alignment given the (original) contents of these DataFrames, so the inner and outer merge results are both unchanged by leaving out `on=['B','C']'`. It would be a different story if I, say, changed row `g` of `dummy3` to `[1, 2, 1, 2]`.\n",
    "\n",
    "I would recommend avoiding any case where you have to use `left|right_index=True` and/or `left|right_on` kwargs, because it quickly becomes difficult to determine which combinations will not raise a `MergeError` without, e.g., working it out on paper.\n",
    "\n",
    "As for `.join()`, it looks more concise in code and it defaults to left-joining instead of inner joining like merge, but there are no significant differences in performance since `.join()` uses `.merge()` under hood. In my opinion, it's not worth the trouble of remembering `.join()` for all the time you are liable to spend troubleshooting combinations that `.merge()` can handle without issue. But here's a quick example just so you know what a left-join looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8ea67ee6-b58c-4fb2-96ae-96d3118c6b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B_l</th>\n",
       "      <th>C_l</th>\n",
       "      <th>B_r</th>\n",
       "      <th>C_r</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B_l  C_l  B_r  C_r    D    E\n",
       "e  0    1    2  NaN  NaN  NaN  NaN\n",
       "f  3    4    5 -5.0 -4.0 -3.0 -2.0\n",
       "g  6    7    8  1.0  2.0  1.0  2.0\n",
       "h  9   10   11  3.0  4.0  5.0  6.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy2.join(dummy3,lsuffix='_l',rsuffix='_r')\n",
    "#merge uses x & y by default, but join has empty defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57b2fa-c582-4c54-942b-3bb9924f1821",
   "metadata": {},
   "source": [
    "Now I'll demonstrate `.combine_first()` and `.combine()`. The former is a bit more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9f756232-f723-4a20-9671-785dd944a8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     B    C    D     E\n",
      "f  NaN  NaN  NaN   NaN\n",
      "g  NaN  2.0  NaN   2.0\n",
      "h  3.0  4.0  5.0   6.0\n",
      "i  7.0  8.0  9.0  10.0\n"
     ]
    }
   ],
   "source": [
    "dummy3.mask(dummy3<=1, inplace=True) #create (more) missing values to replace\n",
    "print(dummy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bcf0ce45-5c35-4115-bf2c-f86316e02287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C    D     E\n",
       "e  0.0  1.0  2.0  NaN   NaN\n",
       "f  3.0  4.0  5.0  NaN   NaN\n",
       "g  6.0  7.0  2.0  NaN   2.0\n",
       "h  9.0  3.0  4.0  5.0   6.0\n",
       "i  NaN  7.0  8.0  9.0  10.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy3.combine_first(dummy2)#overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ce47f-6b03-4e72-914f-fde295f67ad3",
   "metadata": {},
   "source": [
    "Notice that `dummy3.combine_first(dummy2)` not only replaced the NaNs in `dummy3` with numeric values from `dummy2` at the same row and column locations, but also prepended column `A` and row `e` to `dummy3` and moreover, *replaced the valid entries of dummy3 with those in dummy2*. That last behavior is enabled by default with the boolean kwarg `overwrite`; to only allow insertion of missing data, include `overwrite=False` in the parentheses.\n",
    "\n",
    "`.combine()` is a little bit trickier because it relies on you providing a sensible function to handle locations where both DataFrames have data. Functions may be user-defined (named or passed as lambda functions), built-in, or provided by other imported modules like NumPy, but they must accept two 1D arrays and return either a 1D array or a scalar. Functions that return scalars will propagate the same value to the whole column. Once again, I'm starting with that masked `dummy3` DataFrame, and I will show the results of a couple different functions. The first version is a mistake you might make if you're trying to make `combine()` take the higher value at any position where both DataFrames have data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5b5e9be0-3b57-4365-9b8c-408f53ceafa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     B    C    D     E\n",
      "f  NaN  NaN  NaN   NaN\n",
      "g  NaN  NaN  NaN   2.0\n",
      "h  3.0  4.0  5.0   6.0\n",
      "i  7.0  8.0  9.0  10.0 \n",
      "\n",
      "   A   B   C\n",
      "e  0   1   2\n",
      "f  3   4   5\n",
      "g  6   7   8\n",
      "h  9  10  11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B     C    D     E\n",
       "e  9.0  10.0  11.0  9.0  10.0\n",
       "f  9.0  10.0  11.0  9.0  10.0\n",
       "g  9.0  10.0  11.0  9.0  10.0\n",
       "h  9.0  10.0  11.0  9.0  10.0\n",
       "i  9.0  10.0  11.0  9.0  10.0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print these to refresh our memories\n",
    "print(dummy3,'\\n')\n",
    "print(dummy2,'\\n')\n",
    "#print the first way I tried to code up the function\n",
    "# that didn't raise a type error\n",
    "dummy3.combine(dummy2, lambda x,y: np.nanmax([x,y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c33bb5-bb28-43cf-aec1-d813a6ae209c",
   "metadata": {},
   "source": [
    "The following is what is actually required to achieve the desired result without learning any new methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d0beff6-2775-47ac-8067-a331ff4e530f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B     C    D     E\n",
       "e  0.0   1.0   2.0  NaN   NaN\n",
       "f  3.0   4.0   5.0  NaN   NaN\n",
       "g  6.0   7.0   8.0  NaN   2.0\n",
       "h  9.0  10.0  11.0  5.0   6.0\n",
       "i  NaN   7.0   8.0  9.0  10.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elem_wise_nanmax(x,y):\n",
    "    rns = set(list(x.index)+list(y.index)) #get union of row indexes\n",
    "    ser = pd.Series(np.zeros(len(rns)),index=rns) #create Series to store results for return\n",
    "    for i in list(rns): #iterate over rows\n",
    "        try: #take nanmax if values exist in both input series...\n",
    "            ser.loc[i] = np.nanmax([x.loc[i],y.loc[i]])\n",
    "        except IndexError: #...otherwise take whichever one exists\n",
    "            if i in x.index:\n",
    "                ser.loc[i]=x.loc[i]\n",
    "            elif i in y.index:\n",
    "                ser.loc[i]=y.loc[i]\n",
    "    return ser # return results in series form\n",
    "\n",
    "dummy3.combine(dummy2,elem_wise_nanmax) #combine applies elem_wise_nanmax column-by-column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102a54b-6111-49c3-8783-4990e0da70e0",
   "metadata": {},
   "source": [
    "As you can see, this is pretty clunky, and iteration makes this way slower than necessary. Fortunately, Pandas provides ways of broadcasting functions over rows and/or columns simultaneously, without iteration. But before we get into functions, I have to introduce GroupBy objects or else some of the functions won't make sense, and I can show you how GroupBy objects work with some functions that are fairly self-explanatory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028449d4-1b76-4cfa-83b6-7a899ad6db33",
   "metadata": {},
   "source": [
    "## Intro to GroupBy Objects\n",
    "\n",
    "GroupBy objects (more properly DataFrameGroupBy) are the data returned by the function `df.groupby(by, axis=0)`, where the `by` argument can be a column label, a list of column names, row index(es) if you set the axis kwarg equal to 1, or even a Series. There is also a level kwarg if you choose to call `groupby()` on a hierarchical DataFrame (we'll go into that later if we have time).\n",
    "\n",
    "When inspecting a GroupBy object, many DataFrame inspection methods still work, and there is an added `.nth()` method that lets you grab the *n*th row of every group, where *n* is a 0-based index that can also be negative if you want to start from the bottom. For instance, if you wanted just the last row of every group in a DataFrame `df` grouped by some column `col_B`, `df.groupby('col_B').nth(-1)` would return the same rows as `df.groupby('col_B').tail(1)`.\n",
    "\n",
    "You can also extract particular groups by calling the `.get_group('category')` method on a GroupBy object, where `'category'` is one of the values of the original column or row used to do the grouping. There is a `.groups` attribute that lets you get a dict of the groups where the group names are the keys and the values are the rest of the rows where one column contained the cell that was grouped on. That is useful if you need to iterate over the groups. However, if you just want to extract the names for use in other functions, it's usually faster and less complicated to just call `.unique()` on the columns you grouped by and then convert that to a Series or list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fc9ed5d-6f4d-4894-ae27-cb84b9c2f3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>star_mag</th>\n",
       "      <th>planet_type</th>\n",
       "      <th>discovery_yr</th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>detection_method</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LkCa 15 c</th>\n",
       "      <td>516.0</td>\n",
       "      <td>12.025</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.60000</td>\n",
       "      <td>0.999316</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Direct Imaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wolf 503 b</th>\n",
       "      <td>145.0</td>\n",
       "      <td>10.270</td>\n",
       "      <td>Neptune-like</td>\n",
       "      <td>2018</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2.043</td>\n",
       "      <td>0.05706</td>\n",
       "      <td>0.016427</td>\n",
       "      <td>0.41</td>\n",
       "      <td>Transit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YSES 2 b</th>\n",
       "      <td>357.0</td>\n",
       "      <td>10.885</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2021</td>\n",
       "      <td>2003.40</td>\n",
       "      <td>12.768</td>\n",
       "      <td>115.00000</td>\n",
       "      <td>1176.500000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Direct Imaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YZ Ceti b</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.074</td>\n",
       "      <td>Terrestrial</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.01634</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YZ Ceti d</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.074</td>\n",
       "      <td>Super Earth</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.02851</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.07</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            distance  star_mag   planet_type  discovery_yr  mass_ME  \\\n",
       "#name                                                                 \n",
       "LkCa 15 c      516.0    12.025       Unknown          2015      NaN   \n",
       "Wolf 503 b     145.0    10.270  Neptune-like          2018     6.26   \n",
       "YSES 2 b       357.0    10.885     Gas Giant          2021  2003.40   \n",
       "YZ Ceti b       12.0    12.074   Terrestrial          2017     0.70   \n",
       "YZ Ceti d       12.0    12.074   Super Earth          2017     1.09   \n",
       "\n",
       "            radius_RE  orbital_radius_AU  orbital_period_yr  eccentricity  \\\n",
       "#name                                                                       \n",
       "LkCa 15 c         NaN           18.60000           0.999316          0.00   \n",
       "Wolf 503 b      2.043            0.05706           0.016427          0.41   \n",
       "YSES 2 b       12.768          115.00000        1176.500000          0.00   \n",
       "YZ Ceti b       0.913            0.01634           0.005476          0.06   \n",
       "YZ Ceti d       1.030            0.02851           0.012868          0.07   \n",
       "\n",
       "           detection_method  \n",
       "#name                        \n",
       "LkCa 15 c    Direct Imaging  \n",
       "Wolf 503 b          Transit  \n",
       "YSES 2 b     Direct Imaging  \n",
       "YZ Ceti b   Radial Velocity  \n",
       "YZ Ceti d   Radial Velocity  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped1=df.groupby(['planet_type'])\n",
    "grouped1.nth(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e38d65b8-58d1-416b-bc8b-a17f4b3894ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "planet_type\n",
       "Gas Giant       21.515449\n",
       "Neptune-like     0.224902\n",
       "Super Earth      0.109952\n",
       "Terrestrial      0.062381\n",
       "Unknown         16.650000\n",
       "Name: orbital_radius_AU, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped1['orbital_radius_AU'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67471159-daad-4967-86af-c8405427e2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KIC 10001893 b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KIC 10001893 c</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KIC 10001893 d</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LkCa 15 b</th>\n",
       "      <td>14.7</td>\n",
       "      <td>0.999316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LkCa 15 c</th>\n",
       "      <td>18.6</td>\n",
       "      <td>0.999316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                orbital_radius_AU  orbital_period_yr\n",
       "#name                                               \n",
       "KIC 10001893 b                NaN           0.000548\n",
       "KIC 10001893 c                NaN           0.000821\n",
       "KIC 10001893 d                NaN           0.002190\n",
       "LkCa 15 b                    14.7           0.999316\n",
       "LkCa 15 c                    18.6           0.999316"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped1.get_group('Unknown').iloc[:,6:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cccecd5c-6318-471b-806f-f4f043bcaafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2=df.groupby(['detection_method','planet_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29351943-aa5d-46ef-baa0-1a95c9dcb99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('Astrometry', 'Gas Giant'), ('Direct Imaging', 'Gas Giant'), ('Direct Imaging', 'Unknown'), ('Disk Kinematics', 'Gas Giant'), ('Eclipse Timing Variations', 'Gas Giant'), ('Gravitational Microlensing', 'Gas Giant'), ('Gravitational Microlensing', 'Neptune-like'), ('Gravitational Microlensing', 'Super Earth'), ('Gravitational Microlensing', 'Terrestrial'), ('Orbital Brightness Modulation', 'Gas Giant'), ('Orbital Brightness Modulation', 'Terrestrial'), ('Orbital Brightness Modulation', 'Unknown'), ('Pulsar Timing', 'Gas Giant'), ('Pulsar Timing', 'Super Earth'), ('Pulsar Timing', 'Terrestrial'), ('Pulsation Timing Variations', 'Gas Giant'), ('Radial Velocity', 'Gas Giant'), ('Radial Velocity', 'Neptune-like'), ('Radial Velocity', 'Super Earth'), ('Radial Velocity', 'Terrestrial'), ('Transit', 'Gas Giant'), ('Transit', 'Neptune-like'), ('Transit', 'Super Earth'), ('Transit', 'Terrestrial'), ('Transit Timing Variations', 'Gas Giant'), ('Transit Timing Variations', 'Neptune-like'), ('Transit Timing Variations', 'Super Earth'), ('Transit Timing Variations', 'Terrestrial')])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped2.groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21e1e1-14f5-4cee-9088-699f57c17623",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2[['mass_ME','radius_RE', 'orbital_radius_AU', 'orbital_period_yr']].median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aeafb864-7314-4b20-acf1-baee6e11d174",
   "metadata": {},
   "source": [
    "There's a catch to working with GroupBy objects if you group by more than 1 column: grouping by multiple columns results in a hierarchical DataFrame. Hierarchical DataFrames can be tricky to work with because it can be hard to anticipate which DataFrame functions will still work, how the hierarchical structure will change their behavior, and when the hierarchical structure will or will not be preserved after processing with a function. I will only go into them briefly at the end of this course if I have time, because in my opinion, having a DataFrame in hierachical form is only really useful for computing and inspecting group-wise aggregate statistics. Even simple functions frequently collapse the structure in whole or in part, so you don't get the benefits.\n",
    "\n",
    "Indeed, one method that is very handy to use with GroupBy objects even as it fails to preserve the structure is `.filter()`. A classic use of this function is to eliminate groups with very few members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d5f04c4-7087-4424-904e-d3545493233a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gas Giant' 'Super Earth' 'Neptune-like' 'Terrestrial']\n",
      "                      distance  star_mag  planet_type  discovery_yr  mass_ME  \\\n",
      "#name                                                                          \n",
      "11 Comae Berenices b     304.0   4.72307    Gas Giant          2007  6169.20   \n",
      "11 Ursae Minoris b       409.0   5.01300    Gas Giant          2009  4687.32   \n",
      "14 Andromedae b          246.0   5.23133    Gas Giant          2008  1526.40   \n",
      "14 Herculis b             58.0   6.61935    Gas Giant          2002  2588.14   \n",
      "16 Cygni B b              69.0   6.21500    Gas Giant          1996   566.04   \n",
      "...                        ...       ...          ...           ...      ...   \n",
      "XO-7 b                   764.0  10.52100    Gas Giant          2019   225.46   \n",
      "YSES 2 b                 357.0  10.88500    Gas Giant          2021  2003.40   \n",
      "YZ Ceti b                 12.0  12.07400  Terrestrial          2017     0.70   \n",
      "YZ Ceti c                 12.0  12.07400  Super Earth          2017     1.14   \n",
      "YZ Ceti d                 12.0  12.07400  Super Earth          2017     1.09   \n",
      "\n",
      "                      radius_RE  orbital_radius_AU  orbital_period_yr  \\\n",
      "#name                                                                   \n",
      "11 Comae Berenices b    12.0960           1.290000           0.892539   \n",
      "11 Ursae Minoris b      12.2080           1.530000           1.400000   \n",
      "14 Andromedae b         12.8800           0.830000           0.508693   \n",
      "14 Herculis b           12.5440           2.773069           4.800000   \n",
      "16 Cygni B b            13.4400           1.660000           2.200000   \n",
      "...                         ...                ...                ...   \n",
      "XO-7 b                  15.3776           0.044210           0.007940   \n",
      "YSES 2 b                12.7680         115.000000        1176.500000   \n",
      "YZ Ceti b                0.9130           0.016340           0.005476   \n",
      "YZ Ceti c                1.0500           0.021560           0.008487   \n",
      "YZ Ceti d                1.0300           0.028510           0.012868   \n",
      "\n",
      "                      eccentricity detection_method  \n",
      "#name                                                \n",
      "11 Comae Berenices b          0.23  Radial Velocity  \n",
      "11 Ursae Minoris b            0.08  Radial Velocity  \n",
      "14 Andromedae b               0.00  Radial Velocity  \n",
      "14 Herculis b                 0.37  Radial Velocity  \n",
      "16 Cygni B b                  0.68  Radial Velocity  \n",
      "...                            ...              ...  \n",
      "XO-7 b                        0.04          Transit  \n",
      "YSES 2 b                      0.00   Direct Imaging  \n",
      "YZ Ceti b                     0.06  Radial Velocity  \n",
      "YZ Ceti c                     0.00  Radial Velocity  \n",
      "YZ Ceti d                     0.07  Radial Velocity  \n",
      "\n",
      "[5245 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "temp=df.groupby(['planet_type']).filter(lambda x: len(x) > 5)\n",
    "print(temp['planet_type'].unique())\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2f57e-282b-4436-abcf-114400b18fd5",
   "metadata": {},
   "source": [
    "## Operations\n",
    "Pandas DataFrames have method counterparts to most NumPy statistical functions, string methods, plus **many** other convenience functions. We've already covered functions that report basic descriptive data about the DataFrame and functions that handle NaNs. Now we'll focus on the functions that move, change, or do something else with valid data or the Series/DataFrame as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4b3e8-3836-41e1-97ae-9ee939673761",
   "metadata": {},
   "source": [
    "#### Comparisons\n",
    "You can compare a Series or a DataFrame (but not GroupBy objects) to a single scalar value with normal operators (`>=`, `!=`, etc).\n",
    "To compare 1 Series or DataFrame element-wise to another of the same shape (as the arg), use `.gt()`, `.lt()`, `.ge()`, `.le()`, `.eq()`, or `.ne()`. For any of the aforementioned comparison methods (to scalars or other pandas data structures), you can add `.any()` or `.all()` once to collapse the column axis, or twice to get 1 value. This is called Boolean Reduction.\n",
    "\n",
    "To find & print differences between 2 identically indexed Series or DataFrames (let's call them `df1` and `df2`; both objects must have the same row & column labels in the same order), you can use `df1.compare(df2)`. The `.compare()` method will show differences as subtle as extraneous whitespaces or differences of capitalization. However, it will **not** show data type differences if the values are numerically equal; for that, you''l need to use `pd.testing.assert_frame_equal(df1, df2)` or `pd.testing.assert_series_equal(df1, df2)` to see if it raises `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da227c11-1fbc-4698-8f3e-6216bf8675c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dummy = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]], columns = ['a','b','c'], index = [0,1,2])\n",
    "dummy2 = dummy.astype(float)\n",
    "print(dummy.compare(dummy2))\n",
    "print(pd.testing.assert_frame_equal(dummy,dummy2,check_dtype=False)) #assertion error if True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa96ca-7a8b-4fdc-a3ff-3b2078ff57fd",
   "metadata": {},
   "source": [
    "#### Statistics\n",
    "Stats methods (`.mean()`, `.std()`, `.median()`, `.min()`/`.max()`, `.cumsum()`/`.comprod()`, `cov()`, `corr()`, etc.) mostly work like you'd expect based on their NumPy counterparts, and also ignore NaNs by default. If you input a Series or one column of a DataFrame, the result is typically a single value or a Series of the same length. If you input a DataFrame, you will have to choose an axis to evaluate the function over, and the result will be either a Series or, as in the case of the `.corr()` function, a DataFrame in the form of a square matrix with identical row and column labels.\n",
    "\n",
    "These methods also work with GroupBy objects, but be aware that `.corr()` will return a hierarchical DataFrame, which will be hard to work with and may be hard to visualize clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bad4125d-9a90-4934-92de-340b2af4b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average detected exoplanet mass is 1.447 Jupiter masses.\n"
     ]
    }
   ],
   "source": [
    "print('The average detected exoplanet mass is {:.3f} \\\n",
    "Jupiter masses.'.format(df['mass_ME'].mean()/317.907))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2ad66639-5f53-4970-b223-9a05198ef958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most distant planet detected is 27727 light years away.\n"
     ]
    }
   ],
   "source": [
    "print('The most distant planet detected is \\\n",
    "{:.0f} light years away.'.format(df['distance'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "4ea4d017-89d6-4211-8e98-82acc91782f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard deviation and standard error of the mean for orbital radii are 139 and 2 AU, respectively. \n"
     ]
    }
   ],
   "source": [
    "print(\"The standard deviation and standard error of the mean for\\\n",
    " orbital radii are {:.0f} and {:.0f} AU, respectively.\\\n",
    " \".format(df['orbital_radius_AU'].std(), df['orbital_radius_AU'].sem()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "6f984902-a534-412b-9416-b72c4a45bbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Neptune-like\n",
      "Name: planet_type, dtype: object\n",
      "The most common type of exoplanet detected is Neptune-like.\n"
     ]
    }
   ],
   "source": [
    "print(df['planet_type'].mode()) #output is a Series\n",
    "print(\"The most common type of exoplanet detected is \\\n",
    "{}.\".format(df['planet_type'].mode()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47b65d51-0c7b-4fd2-b035-52cf14549bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  mass_ME  radius_RE  \\\n",
      "detection_method                                                       \n",
      "Astrometry                    mass_ME            1.000000  -1.000000   \n",
      "                              radius_RE         -1.000000   1.000000   \n",
      "                              orbital_radius_AU -1.000000   1.000000   \n",
      "                              orbital_period_yr -1.000000   1.000000   \n",
      "Direct Imaging                mass_ME            1.000000   0.851734   \n",
      "                              radius_RE          0.851734   1.000000   \n",
      "                              orbital_radius_AU -0.059104  -0.117667   \n",
      "                              orbital_period_yr -0.042362  -0.089886   \n",
      "Disk Kinematics               mass_ME                 NaN        NaN   \n",
      "                              radius_RE               NaN        NaN   \n",
      "                              orbital_radius_AU       NaN        NaN   \n",
      "                              orbital_period_yr       NaN        NaN   \n",
      "Eclipse Timing Variations     mass_ME            1.000000  -0.909963   \n",
      "                              radius_RE         -0.909963   1.000000   \n",
      "                              orbital_radius_AU -0.181639  -0.057737   \n",
      "                              orbital_period_yr -0.060397  -0.206596   \n",
      "Gravitational Microlensing    mass_ME            1.000000   0.343501   \n",
      "                              radius_RE          0.343501   1.000000   \n",
      "                              orbital_radius_AU  0.033147   0.143905   \n",
      "                              orbital_period_yr  0.002920   0.100694   \n",
      "Orbital Brightness Modulation mass_ME            1.000000   0.805299   \n",
      "                              radius_RE          0.805299   1.000000   \n",
      "                              orbital_radius_AU  0.997765   0.998160   \n",
      "                              orbital_period_yr  0.820547   0.797878   \n",
      "Pulsar Timing                 mass_ME            1.000000   0.822145   \n",
      "                              radius_RE          0.822145   1.000000   \n",
      "                              orbital_radius_AU  0.705797   0.448282   \n",
      "                              orbital_period_yr  0.778651   0.516744   \n",
      "Pulsation Timing Variations   mass_ME            1.000000  -1.000000   \n",
      "                              radius_RE         -1.000000   1.000000   \n",
      "                              orbital_radius_AU       NaN        NaN   \n",
      "                              orbital_period_yr -1.000000   1.000000   \n",
      "Radial Velocity               mass_ME            1.000000   0.327547   \n",
      "                              radius_RE          0.327547   1.000000   \n",
      "                              orbital_radius_AU  0.540958   0.301838   \n",
      "                              orbital_period_yr  0.474518   0.177917   \n",
      "Transit                       mass_ME            1.000000   0.216717   \n",
      "                              radius_RE          0.216717   1.000000   \n",
      "                              orbital_radius_AU  0.009714  -0.016425   \n",
      "                              orbital_period_yr  0.024389   0.032376   \n",
      "Transit Timing Variations     mass_ME            1.000000   0.415997   \n",
      "                              radius_RE          0.415997   1.000000   \n",
      "                              orbital_radius_AU  0.915451   0.522054   \n",
      "                              orbital_period_yr  0.960343   0.454616   \n",
      "\n",
      "                                                 orbital_radius_AU  \\\n",
      "detection_method                                                     \n",
      "Astrometry                    mass_ME                    -1.000000   \n",
      "                              radius_RE                   1.000000   \n",
      "                              orbital_radius_AU           1.000000   \n",
      "                              orbital_period_yr           1.000000   \n",
      "Direct Imaging                mass_ME                    -0.059104   \n",
      "                              radius_RE                  -0.117667   \n",
      "                              orbital_radius_AU           1.000000   \n",
      "                              orbital_period_yr           0.960091   \n",
      "Disk Kinematics               mass_ME                          NaN   \n",
      "                              radius_RE                        NaN   \n",
      "                              orbital_radius_AU                NaN   \n",
      "                              orbital_period_yr                NaN   \n",
      "Eclipse Timing Variations     mass_ME                    -0.181639   \n",
      "                              radius_RE                  -0.057737   \n",
      "                              orbital_radius_AU           1.000000   \n",
      "                              orbital_period_yr           0.951468   \n",
      "Gravitational Microlensing    mass_ME                     0.033147   \n",
      "                              radius_RE                   0.143905   \n",
      "                              orbital_radius_AU           1.000000   \n",
      "                              orbital_period_yr           0.922830   \n",
      "Orbital Brightness Modulation mass_ME                     0.997765   \n",
      "                              radius_RE                   0.998160   \n",
      "                              orbital_radius_AU           1.000000   \n",
      "                              orbital_period_yr           0.999998   \n",
      "Pulsar Timing                 mass_ME                     0.705797   \n",
      "                              radius_RE                   0.448282   \n",
      "                              orbital_radius_AU           1.000000   \n",
      "                              orbital_period_yr           0.989350   \n",
      "Pulsation Timing Variations   mass_ME                          NaN   \n",
      "                              radius_RE                        NaN   \n",
      "                              orbital_radius_AU                NaN   \n",
      "                              orbital_period_yr                NaN   \n",
      "Radial Velocity               mass_ME                     0.540958   \n",
      "                              radius_RE                   0.301838   \n",
      "                              orbital_radius_AU           1.000000   \n",
      "                              orbital_period_yr           0.955723   \n",
      "Transit                       mass_ME                     0.009714   \n",
      "                              radius_RE                  -0.016425   \n",
      "                              orbital_radius_AU           1.000000   \n",
      "                              orbital_period_yr           0.899335   \n",
      "Transit Timing Variations     mass_ME                     0.915451   \n",
      "                              radius_RE                   0.522054   \n",
      "                              orbital_radius_AU           1.000000   \n",
      "                              orbital_period_yr           0.986495   \n",
      "\n",
      "                                                 orbital_period_yr  \n",
      "detection_method                                                    \n",
      "Astrometry                    mass_ME                    -1.000000  \n",
      "                              radius_RE                   1.000000  \n",
      "                              orbital_radius_AU           1.000000  \n",
      "                              orbital_period_yr           1.000000  \n",
      "Direct Imaging                mass_ME                    -0.042362  \n",
      "                              radius_RE                  -0.089886  \n",
      "                              orbital_radius_AU           0.960091  \n",
      "                              orbital_period_yr           1.000000  \n",
      "Disk Kinematics               mass_ME                          NaN  \n",
      "                              radius_RE                        NaN  \n",
      "                              orbital_radius_AU                NaN  \n",
      "                              orbital_period_yr                NaN  \n",
      "Eclipse Timing Variations     mass_ME                    -0.060397  \n",
      "                              radius_RE                  -0.206596  \n",
      "                              orbital_radius_AU           0.951468  \n",
      "                              orbital_period_yr           1.000000  \n",
      "Gravitational Microlensing    mass_ME                     0.002920  \n",
      "                              radius_RE                   0.100694  \n",
      "                              orbital_radius_AU           0.922830  \n",
      "                              orbital_period_yr           1.000000  \n",
      "Orbital Brightness Modulation mass_ME                     0.820547  \n",
      "                              radius_RE                   0.797878  \n",
      "                              orbital_radius_AU           0.999998  \n",
      "                              orbital_period_yr           1.000000  \n",
      "Pulsar Timing                 mass_ME                     0.778651  \n",
      "                              radius_RE                   0.516744  \n",
      "                              orbital_radius_AU           0.989350  \n",
      "                              orbital_period_yr           1.000000  \n",
      "Pulsation Timing Variations   mass_ME                    -1.000000  \n",
      "                              radius_RE                   1.000000  \n",
      "                              orbital_radius_AU                NaN  \n",
      "                              orbital_period_yr           1.000000  \n",
      "Radial Velocity               mass_ME                     0.474518  \n",
      "                              radius_RE                   0.177917  \n",
      "                              orbital_radius_AU           0.955723  \n",
      "                              orbital_period_yr           1.000000  \n",
      "Transit                       mass_ME                     0.024389  \n",
      "                              radius_RE                   0.032376  \n",
      "                              orbital_radius_AU           0.899335  \n",
      "                              orbital_period_yr           1.000000  \n",
      "Transit Timing Variations     mass_ME                     0.960343  \n",
      "                              radius_RE                   0.454616  \n",
      "                              orbital_radius_AU           0.986495  \n",
      "                              orbital_period_yr           1.000000   \n",
      "\n",
      " [MultiIndex([(                   'Astrometry',           'mass_ME'),\n",
      "            (                   'Astrometry',         'radius_RE'),\n",
      "            (                   'Astrometry', 'orbital_radius_AU'),\n",
      "            (                   'Astrometry', 'orbital_period_yr'),\n",
      "            (               'Direct Imaging',           'mass_ME'),\n",
      "            (               'Direct Imaging',         'radius_RE'),\n",
      "            (               'Direct Imaging', 'orbital_radius_AU'),\n",
      "            (               'Direct Imaging', 'orbital_period_yr'),\n",
      "            (              'Disk Kinematics',           'mass_ME'),\n",
      "            (              'Disk Kinematics',         'radius_RE'),\n",
      "            (              'Disk Kinematics', 'orbital_radius_AU'),\n",
      "            (              'Disk Kinematics', 'orbital_period_yr'),\n",
      "            (    'Eclipse Timing Variations',           'mass_ME'),\n",
      "            (    'Eclipse Timing Variations',         'radius_RE'),\n",
      "            (    'Eclipse Timing Variations', 'orbital_radius_AU'),\n",
      "            (    'Eclipse Timing Variations', 'orbital_period_yr'),\n",
      "            (   'Gravitational Microlensing',           'mass_ME'),\n",
      "            (   'Gravitational Microlensing',         'radius_RE'),\n",
      "            (   'Gravitational Microlensing', 'orbital_radius_AU'),\n",
      "            (   'Gravitational Microlensing', 'orbital_period_yr'),\n",
      "            ('Orbital Brightness Modulation',           'mass_ME'),\n",
      "            ('Orbital Brightness Modulation',         'radius_RE'),\n",
      "            ('Orbital Brightness Modulation', 'orbital_radius_AU'),\n",
      "            ('Orbital Brightness Modulation', 'orbital_period_yr'),\n",
      "            (                'Pulsar Timing',           'mass_ME'),\n",
      "            (                'Pulsar Timing',         'radius_RE'),\n",
      "            (                'Pulsar Timing', 'orbital_radius_AU'),\n",
      "            (                'Pulsar Timing', 'orbital_period_yr'),\n",
      "            (  'Pulsation Timing Variations',           'mass_ME'),\n",
      "            (  'Pulsation Timing Variations',         'radius_RE'),\n",
      "            (  'Pulsation Timing Variations', 'orbital_radius_AU'),\n",
      "            (  'Pulsation Timing Variations', 'orbital_period_yr'),\n",
      "            (              'Radial Velocity',           'mass_ME'),\n",
      "            (              'Radial Velocity',         'radius_RE'),\n",
      "            (              'Radial Velocity', 'orbital_radius_AU'),\n",
      "            (              'Radial Velocity', 'orbital_period_yr'),\n",
      "            (                      'Transit',           'mass_ME'),\n",
      "            (                      'Transit',         'radius_RE'),\n",
      "            (                      'Transit', 'orbital_radius_AU'),\n",
      "            (                      'Transit', 'orbital_period_yr'),\n",
      "            (    'Transit Timing Variations',           'mass_ME'),\n",
      "            (    'Transit Timing Variations',         'radius_RE'),\n",
      "            (    'Transit Timing Variations', 'orbital_radius_AU'),\n",
      "            (    'Transit Timing Variations', 'orbital_period_yr')],\n",
      "           names=['detection_method', None]), Index(['mass_ME', 'radius_RE', 'orbital_radius_AU', 'orbital_period_yr'], dtype='object')]\n"
     ]
    }
   ],
   "source": [
    "grouped2=df.groupby(['detection_method'])\n",
    "gp_corr=grouped2[['mass_ME','radius_RE', 'orbital_radius_AU', 'orbital_period_yr']].corr()\n",
    "print(gp_corr,'\\n\\n',gp_corr.axes) #this will be long!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59702d46-1c4d-44e2-b18f-ffad6d9885f0",
   "metadata": {},
   "source": [
    "\n",
    "There are also methods that you typically need SciPy for, like `.skew()` and `.kurtosis()`, and functions for assigning ranks (`.rank()`) and quantiles (`.quantile()`).\n",
    "\n",
    "All of the above can be applied to the whole DataFrame column-wise or row-wise, and with the `.rolling` attribute between the DataFrame and the function, can also be evaluated over rolling windows of $n$ entries instead of the whole axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5254e-924c-4fa5-abd7-b94c85ebdff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd1c867-3a3e-4487-bd88-ec048ef4f378",
   "metadata": {},
   "source": [
    "#### Binary Operations\n",
    "\n",
    "The more exciting features of Pandas are that you can broadcast operations across rows, columns, whole DataFrames, and even do arithmetic on multiple DataFrames. For these purposes, DataFrames have the methods `add()`, `sub()`, `mul()`, `div()`, `pow()`, `mod()`, and `divmod()`, and reversed versions of each (`radd()`, `rsub()`, ...) for if the operations do not commute. To take `div()` for example, the forward version `df1.div(other)` divides a DataFrame `df1` by `other`, where `other` could be a scalar, a Series or vector of the same length as one of the dimensions of `df1`, or another DataFrame of the same shape as `df1`. In the reverse version, `df1.rdiv(other)` divides `other` by `df1`.\n",
    "\n",
    "Remember that to divide a DataFrame by a vector or Series, you must specify an axis. The axis determines which dimension the operation is to be broadcast along, NOT the orientation of the vector. If you set `axis='index'` or 0, each entry in the vector will be paired with a row, and if you set `axis='columns'`, each entry in the vector will be matched to a column.\n",
    "\n",
    "These operators are NOT suitable for GroupBy objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a0bd2e9-959e-4429-b5e9-d92118a1c789",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b   c\n",
      "0  0   1   2\n",
      "1  3   4   5\n",
      "2  6   7   8\n",
      "3  9  10  11 \n",
      "\n",
      "      a          b          c\n",
      "0  0.0   0.250000   0.500000\n",
      "1  1.0   1.333333   1.666667\n",
      "2  3.0   3.500000   4.000000\n",
      "3  9.0  10.000000  11.000000 \n",
      "\n",
      "           a         b         c\n",
      "0       inf  4.000000  2.000000\n",
      "1  1.000000  0.750000  0.600000\n",
      "2  0.333333  0.285714  0.250000\n",
      "3  0.111111  0.100000  0.090909\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame(np.arange(12).reshape([4,3]),\n",
    "                   columns = ['a','b','c'])\n",
    "print(df1,'\\n\\n',df1.div([4.,3.,2., 1.], axis='index'),\n",
    "      '\\n\\n', df1.rdiv([4.,3.,2., 1.], axis='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482bb983-b557-46a5-b866-9fd016fe9e08",
   "metadata": {},
   "source": [
    "The method `.divmod()` is a bit special in that a) it can only be called on dividends of Series or Index type, b) and it returns 2 Series instead of just 1. The first Series is the results of floor division, and the second Series is the list of remainders. The divisor (the argument in parentheses) may be either a scalar or a Series of the same length as the dividend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27db4e84-9fff-448a-be8e-598282834767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nums  rems  mods\n",
      "0    0.0   0.0   0.0\n",
      "1    0.5   0.0   0.5\n",
      "2    1.0   0.0   1.0\n",
      "3    1.5   0.0   1.5\n",
      "4    2.0   0.0   2.0\n",
      "5    2.5   0.0   2.5\n",
      "6    3.0   1.0   0.0\n",
      "7    3.5   1.0   0.5\n",
      "8    4.0   1.0   1.0\n",
      "9    4.5   1.0   1.5\n",
      "10   5.0   1.0   2.0\n",
      "11   5.5   1.0   2.5\n",
      "12   6.0   2.0   0.0\n",
      "13   6.5   2.0   0.5\n",
      "14   7.0   2.0   1.0\n",
      "15   7.5   2.0   1.5\n",
      "16   8.0   2.0   2.0\n",
      "17   8.5   2.0   2.5\n",
      "18   9.0   3.0   0.0\n"
     ]
    }
   ],
   "source": [
    "ser3 = pd.Series(np.linspace(0,9,19))\n",
    "rems,mods=ser3.divmod(3)\n",
    "print(pd.concat((ser3.rename('nums'),\n",
    "                 rems.rename('rems'),\n",
    "                 mods.rename('mods')), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb370ed-c3e7-4267-b3b4-6c5e2c63d13b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Vectorized String Methods\n",
    "Most familiar string methods like `.replace()`, `.lower()`/`.upper()`, `.split()`/`.rsplit()`, and `.strip()` have counterparts in Pandas that can be broadcast to every element with the syntax `.str.<method>`. The available args and kwargs for each of the functions are also mostly unchanged, except in the case of `.str.replace()`: the standard two positional arguments are accepted, as are regular expressions, but it will **not** accept a dictionary where the keys are the existing substrings and the values are the replacement characters. (Note: dictionary replacement also fails for the built-in string `replace()` method if any of the old or new strings contain LaTeX expressions because `replace()` implicitly uses string insertion via `'{}'.format()`, which collides with LaTeX's use of curly braces as delimiters for function arguments.)\n",
    "\n",
    "The function `.str.split(\" \", expand=False, n=None)` and its `.str.rsplit()` counterpart return either a Series in which the items are lists or, with the kwarg `expand=True`, a DataFrame with as many columns as there would have been items in the longest list of substrings (shorter lists will leave Nones in the columns they weren't long enought to fill). If you want to limit the number of length of each list or row, you can set `n` equal to the maximum number of occurences of the character or substring to split on. Unfortunately, if you set `expand=True`, there is not currently a kwarg to set the column names of the resulting DataFrame in the same step, so at least initially both rows and columns will be labeled with 0-based indexes. With `expand=False` (default), you can also use either `.str.get(i)` or just `.str[i]`, where `i` is the list index(es) or slice, to return a Series of subsets of each list of split strings. I'll demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1538ca32-d29a-4610-b743-8286f097065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " 0    Pandas are cute!\n",
      "1      I like trains.\n",
      "2       Hello, world?\n",
      "3               Hams.\n",
      "dtype: object \n",
      "\n",
      "Split and get:\n",
      " 0       [Pandas, are]\n",
      "1           [I, like]\n",
      "2    [Hello,, world?]\n",
      "3             [Hams.]\n",
      "dtype: object \n",
      "\n",
      "Expand=True:\n",
      "         0       1        2\n",
      "0  Pandas     are    cute!\n",
      "1       I    like  trains.\n",
      "2  Hello,  world?     None\n",
      "3   Hams.    None     None\n"
     ]
    }
   ],
   "source": [
    "dummy4 = pd.Series(['Pandas are cute!', 'I like trains.','Hello, world?', 'Hams.'])\n",
    "print('Original:\\n', dummy4, '\\n')\n",
    "dummy5 = dummy4.str.split()\n",
    "print('Split and get:\\n',dummy5.str[:2], '\\n')\n",
    "dummy6 = dummy4.str.split(expand=True)\n",
    "print('Expand=True:\\n', dummy6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88560d2-be4e-432a-a3c8-53b2e108f910",
   "metadata": {},
   "source": [
    "### User-Defined Methods\n",
    "If you find that you need to broadcast more complicated functions to your data, there are 4 methods you can use depending on whether the function is a combination of named NumPy functions or totally user-defined, what shape the function broadcasts or reduces (aggregates) results to, and /or whether the function is to be applied row-wise, column-wise, or element-wise. \n",
    "\n",
    "#### Aggregating (aka reducing) with `.agg()`\n",
    "The `.agg()` method only accepts functions that take all the values along a specified axis (row or column in this case) as input and then output a single value, like `sum()` or `std()`, but you can apply more than one of these reducing functions at a time by passing a list of function names, or a dict of column names as keys and function names as values. Be aware that the only way to apply a different function to every column is to pass a dict of column name as keys and function names as values. Functions passed as lists are interpreted to mean that all of the aggregate functions should be computed for every column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35d51db9-ce73-42ee-9a1b-d038a9bc24d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mass_ME                  8.470000\n",
      "radius_RE                5.627083\n",
      "orbital_radius_AU     7506.000000\n",
      "orbital_period_yr    16804.445318\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#grab a subset of the planets df\n",
    "spam = df[['mass_ME', 'radius_RE',\n",
    "           'orbital_radius_AU', 'orbital_period_yr']]\n",
    "#print(spam.agg('mean'))\n",
    "#print(spam.agg(['mean','median','max','std']))\n",
    "print(spam.agg(dict(zip(spam.columns.values,\n",
    "                        ['median','mean','max','std']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b31c4-2917-49e7-b789-d66eea1fa2de",
   "metadata": {},
   "source": [
    "One of the best things about `.agg()` is that it can be applied to GroupBy objects, and unlike the more generalized alternative, `.apply()`, `.agg()` knows to preserve both groups (which are stacked row-wise) and columns automatically. And this functionality persists even if the GroupBy object is hierarchical. Observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e037ab3-5b7e-4e68-aef7-578c49ca5d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309666/3745766165.py:2: RuntimeWarning: Mean of empty slice\n",
      "  grouped2[['mass_ME','radius_RE']].agg(lambda x: 'avg: {:.2f}, pct err: {:.0%}'.format(np.nanmean(x),\n",
      "/home/rlpitts/anaconda3/lib/python3.11/site-packages/numpy/lib/nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/tmp/ipykernel_309666/3745766165.py:3: RuntimeWarning: Mean of empty slice\n",
      "  np.nanstd(x)/np.nanmean(x)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detection_method</th>\n",
       "      <th>planet_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Astrometry</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 4890.84, pct err: 85%</td>\n",
       "      <td>avg: 12.60, pct err: 6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Direct Imaging</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 7929.95, pct err: 380%</td>\n",
       "      <td>avg: 15.84, pct err: 59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknown</th>\n",
       "      <td>avg: nan, pct err: nan%</td>\n",
       "      <td>avg: nan, pct err: nan%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disk Kinematics</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 795.00, pct err: 0%</td>\n",
       "      <td>avg: 13.22, pct err: 0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eclipse Timing Variations</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 2154.77, pct err: 80%</td>\n",
       "      <td>avg: 12.88, pct err: 4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Gravitational Microlensing</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 1012.92, pct err: 128%</td>\n",
       "      <td>avg: 12.71, pct err: 12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neptune-like</th>\n",
       "      <td>avg: 16.63, pct err: 58%</td>\n",
       "      <td>avg: 4.06, pct err: 35%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Super Earth</th>\n",
       "      <td>avg: 2.96, pct err: 31%</td>\n",
       "      <td>avg: 1.53, pct err: 17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terrestrial</th>\n",
       "      <td>avg: 0.96, pct err: 0%</td>\n",
       "      <td>avg: 1.00, pct err: 0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Orbital Brightness Modulation</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 525.49, pct err: 40%</td>\n",
       "      <td>avg: 14.03, pct err: 6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terrestrial</th>\n",
       "      <td>avg: 0.55, pct err: 20%</td>\n",
       "      <td>avg: 0.81, pct err: 7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unknown</th>\n",
       "      <td>avg: nan, pct err: nan%</td>\n",
       "      <td>avg: nan, pct err: nan%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Pulsar Timing</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 476.46, pct err: 49%</td>\n",
       "      <td>avg: 13.55, pct err: 2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Super Earth</th>\n",
       "      <td>avg: 3.39, pct err: 30%</td>\n",
       "      <td>avg: 1.64, pct err: 18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terrestrial</th>\n",
       "      <td>avg: 0.02, pct err: 0%</td>\n",
       "      <td>avg: 0.34, pct err: 0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulsation Timing Variations</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 2385.00, pct err: 57%</td>\n",
       "      <td>avg: 12.71, pct err: 3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Radial Velocity</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 1464.50, pct err: 127%</td>\n",
       "      <td>avg: 12.92, pct err: 9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neptune-like</th>\n",
       "      <td>avg: 13.80, pct err: 55%</td>\n",
       "      <td>avg: 3.66, pct err: 32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Super Earth</th>\n",
       "      <td>avg: 3.33, pct err: 62%</td>\n",
       "      <td>avg: 1.53, pct err: 21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terrestrial</th>\n",
       "      <td>avg: 0.70, pct err: 0%</td>\n",
       "      <td>avg: 0.91, pct err: 0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Transit</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 957.66, pct err: 430%</td>\n",
       "      <td>avg: 12.79, pct err: 29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neptune-like</th>\n",
       "      <td>avg: 15.47, pct err: 378%</td>\n",
       "      <td>avg: 3.09, pct err: 35%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Super Earth</th>\n",
       "      <td>avg: 5.79, pct err: 481%</td>\n",
       "      <td>avg: 1.58, pct err: 23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terrestrial</th>\n",
       "      <td>avg: 1.65, pct err: 701%</td>\n",
       "      <td>avg: 0.86, pct err: 31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Transit Timing Variations</th>\n",
       "      <th>Gas Giant</th>\n",
       "      <td>avg: 1194.10, pct err: 181%</td>\n",
       "      <td>avg: 12.08, pct err: 16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neptune-like</th>\n",
       "      <td>avg: 14.83, pct err: 55%</td>\n",
       "      <td>avg: 3.32, pct err: 24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Super Earth</th>\n",
       "      <td>avg: 34.54, pct err: 114%</td>\n",
       "      <td>avg: 1.89, pct err: 22%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terrestrial</th>\n",
       "      <td>avg: 0.23, pct err: 87%</td>\n",
       "      <td>avg: 0.58, pct err: 37%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                mass_ME  \\\n",
       "detection_method              planet_type                                 \n",
       "Astrometry                    Gas Giant      avg: 4890.84, pct err: 85%   \n",
       "Direct Imaging                Gas Giant     avg: 7929.95, pct err: 380%   \n",
       "                              Unknown           avg: nan, pct err: nan%   \n",
       "Disk Kinematics               Gas Giant        avg: 795.00, pct err: 0%   \n",
       "Eclipse Timing Variations     Gas Giant      avg: 2154.77, pct err: 80%   \n",
       "Gravitational Microlensing    Gas Giant     avg: 1012.92, pct err: 128%   \n",
       "                              Neptune-like     avg: 16.63, pct err: 58%   \n",
       "                              Super Earth       avg: 2.96, pct err: 31%   \n",
       "                              Terrestrial        avg: 0.96, pct err: 0%   \n",
       "Orbital Brightness Modulation Gas Giant       avg: 525.49, pct err: 40%   \n",
       "                              Terrestrial       avg: 0.55, pct err: 20%   \n",
       "                              Unknown           avg: nan, pct err: nan%   \n",
       "Pulsar Timing                 Gas Giant       avg: 476.46, pct err: 49%   \n",
       "                              Super Earth       avg: 3.39, pct err: 30%   \n",
       "                              Terrestrial        avg: 0.02, pct err: 0%   \n",
       "Pulsation Timing Variations   Gas Giant      avg: 2385.00, pct err: 57%   \n",
       "Radial Velocity               Gas Giant     avg: 1464.50, pct err: 127%   \n",
       "                              Neptune-like     avg: 13.80, pct err: 55%   \n",
       "                              Super Earth       avg: 3.33, pct err: 62%   \n",
       "                              Terrestrial        avg: 0.70, pct err: 0%   \n",
       "Transit                       Gas Giant      avg: 957.66, pct err: 430%   \n",
       "                              Neptune-like    avg: 15.47, pct err: 378%   \n",
       "                              Super Earth      avg: 5.79, pct err: 481%   \n",
       "                              Terrestrial      avg: 1.65, pct err: 701%   \n",
       "Transit Timing Variations     Gas Giant     avg: 1194.10, pct err: 181%   \n",
       "                              Neptune-like     avg: 14.83, pct err: 55%   \n",
       "                              Super Earth     avg: 34.54, pct err: 114%   \n",
       "                              Terrestrial       avg: 0.23, pct err: 87%   \n",
       "\n",
       "                                                           radius_RE  \n",
       "detection_method              planet_type                             \n",
       "Astrometry                    Gas Giant      avg: 12.60, pct err: 6%  \n",
       "Direct Imaging                Gas Giant     avg: 15.84, pct err: 59%  \n",
       "                              Unknown        avg: nan, pct err: nan%  \n",
       "Disk Kinematics               Gas Giant      avg: 13.22, pct err: 0%  \n",
       "Eclipse Timing Variations     Gas Giant      avg: 12.88, pct err: 4%  \n",
       "Gravitational Microlensing    Gas Giant     avg: 12.71, pct err: 12%  \n",
       "                              Neptune-like   avg: 4.06, pct err: 35%  \n",
       "                              Super Earth    avg: 1.53, pct err: 17%  \n",
       "                              Terrestrial     avg: 1.00, pct err: 0%  \n",
       "Orbital Brightness Modulation Gas Giant      avg: 14.03, pct err: 6%  \n",
       "                              Terrestrial     avg: 0.81, pct err: 7%  \n",
       "                              Unknown        avg: nan, pct err: nan%  \n",
       "Pulsar Timing                 Gas Giant      avg: 13.55, pct err: 2%  \n",
       "                              Super Earth    avg: 1.64, pct err: 18%  \n",
       "                              Terrestrial     avg: 0.34, pct err: 0%  \n",
       "Pulsation Timing Variations   Gas Giant      avg: 12.71, pct err: 3%  \n",
       "Radial Velocity               Gas Giant      avg: 12.92, pct err: 9%  \n",
       "                              Neptune-like   avg: 3.66, pct err: 32%  \n",
       "                              Super Earth    avg: 1.53, pct err: 21%  \n",
       "                              Terrestrial     avg: 0.91, pct err: 0%  \n",
       "Transit                       Gas Giant     avg: 12.79, pct err: 29%  \n",
       "                              Neptune-like   avg: 3.09, pct err: 35%  \n",
       "                              Super Earth    avg: 1.58, pct err: 23%  \n",
       "                              Terrestrial    avg: 0.86, pct err: 31%  \n",
       "Transit Timing Variations     Gas Giant     avg: 12.08, pct err: 16%  \n",
       "                              Neptune-like   avg: 3.32, pct err: 24%  \n",
       "                              Super Earth    avg: 1.89, pct err: 22%  \n",
       "                              Terrestrial    avg: 0.58, pct err: 37%  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped2=df.groupby(['detection_method','planet_type'])\n",
    "grouped2[['mass_ME','radius_RE']].agg(lambda x: 'avg: {:.2f}, pct err: {:.0%}'.format(np.nanmean(x),\n",
    "                                                np.nanstd(x)/np.nanmean(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c44348-f0f5-48c4-bc54-53ae7f732371",
   "metadata": {},
   "source": [
    "#### Broadcasting with `.transform()`\n",
    "Like `.agg()`, `.transform()` is a powerful function application method that can be applied group-wise to GroupBy objects, but `.transform()` is designed to broadcast functions to every cell of the data structure it is called upon. The `.transform()` method requires the output to have the same shape as the input, but, as with `.agg`, you can apply more than one function at a time as long as the functions are passed either by name or, if you use lambda functions, by a dictionary of column names and associated functions.\n",
    "\n",
    "Also like `.agg()`, if you call `.transform()` on a DataFrame with a list of functions, it will apply all of the functions to all of the data. Because `.transform()` does not aggregate data, however, if called with list of $n$ unique function names on a DataFrame of $m$ columns, the result will be a 2D projection of a 3D structure, i.e. a hierarchical DataFrame of $n\\times m$ columns grouped by the original column names at the top level, and the function names on the second level. Observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79e8f26f-985d-40a2-9e84-d59a4c68d44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b   c\n",
      "0  0   1   2\n",
      "1  3   4   5\n",
      "2  6   7   8\n",
      "3  9  10  11\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70ab8dc2-1d51-4bc0-9753-971999af8a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      a               b               c          \n",
      "  funcA     funcB funcA     funcB funcA     funcB\n",
      "0     1 -1.000000     4  0.000000     9  0.414214\n",
      "1    16  0.732051    25  1.000000    36  1.236068\n",
      "2    49  1.449490    64  1.645751    81  1.828427\n",
      "3   100  2.000000   121  2.162278   144  2.316625\n",
      "MultiIndex([('a', 'funcA'),\n",
      "            ('a', 'funcB'),\n",
      "            ('b', 'funcA'),\n",
      "            ('b', 'funcB'),\n",
      "            ('c', 'funcA'),\n",
      "            ('c', 'funcB')],\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "def funcA(x):\n",
    "    return x**2+2*x+1\n",
    "def funcB(x):\n",
    "    return x**0.5-1\n",
    "df2 = df1.transform([funcA,funcB])\n",
    "print(df2)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa2e94c-1c4d-41bf-a94d-5f4df2953b48",
   "metadata": {},
   "source": [
    "One other thing to note about this approach: *unlike* with `.agg()`, if you pass a list of lambda functions, all but the last function will be ignored. As shown above, `.transform()` relies on function names to distinguish the outputs of different functions when all columns are affected equally.\n",
    "\n",
    "For completeness, here's an example with lambda functions in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "423df50a-d14a-43c9-a859-81a31ae51615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   b         c\n",
      "0   0   3  1.414214\n",
      "1   9   9  2.236068\n",
      "2  36  15  2.828427\n",
      "3  81  21  3.316625\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.transform({'a':lambda x: x**2,\n",
    "                     'b':lambda y: y*2+1,\n",
    "                     'c':lambda z: z**0.5})\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5843a-9785-4bcf-a1d7-d62a005d9785",
   "metadata": {},
   "source": [
    "#### Elementwise functions with `.map()`\n",
    "The `.map()` method is like a more restricted version of transform, and is typically used for element-wise replacement of values by function. It can only take 1 function at a time, but it is typically cleaner and more intuitive if that's all you need. The function may be passed by name or as a lambda function.\n",
    "\n",
    "All that said, if you can recast your desired operations as one or more built-in or vectorized functions, like `df**0.5` instead of `df.map(np.sqrt)`, the vectorized version will generally be faster than `map()`. Also, of the 4 methods for applying user-defined functions, this is the only one that does **not** work on GroupBy objects.\n",
    "\n",
    "One obvious use case is if your function is piecewise but applies to all columns equally (or if your data is just a Series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d235a64f-df16-4f49-9a41-8445c82a7e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C\n",
      "0  231  426  572\n",
      "1  497  628  410\n",
      "2  375  600  577\n",
      "3  408  206  616 \n",
      "\n",
      "          A         B         C\n",
      "0  0.211211  0.211957  0.306578\n",
      "1  0.260593  0.337479  0.200328\n",
      "2  0.174117  0.322379  0.309452\n",
      "3  0.198858  0.144310  0.331091\n"
     ]
    }
   ],
   "source": [
    "def my_func(T):\n",
    "    if T<=0 or np.isnan(T) is True:\n",
    "        pass\n",
    "    elif T<300:\n",
    "        return 0.2*(T**0.5)*np.exp(-616/T)\n",
    "    elif T>=300:\n",
    "        return 0.9*np.exp(-616/T)\n",
    "    \n",
    "junk = pd.DataFrame(np.random.randint(173,high=675,size=(4,3)),\n",
    "                    columns = ['A', 'B', 'C'])\n",
    "print(junk,'\\n')\n",
    "print(junk.map(my_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2982234-562a-4372-afbd-fa869982192b",
   "metadata": {},
   "source": [
    "#### For most other vector operations, there's `.apply()`\n",
    "\n",
    "It's slower than either `.agg()` or `.transform()`, but `.apply()` can handle a variety of types of functions in terms of the input and output shapes. Not only does it handle aggregating and transforming functions, it can handle expanding operations, i.e. if the function to apply returns list-like output, `.apply()` can be set to automatically make new columns for every list index. Unlike with `.transform()`, these results are not automatically hierarchical. All of that said, `.apply()` is less efficient than `.agg()` or `.transform()`, and unless your function mixes and changes\\* datatypes, you can usually find a way to break down more complicated functions into some combination of other pandas functions that run faster than `.apply()`.\n",
    "\n",
    "\\***General programming rule: never mutate (change the shape or indexing of) a data structure while you iterate through it, even if the iteration is implicit as in these user-defined functions.** If your function will return a data structure of a different shape, either make a copy to work on or assign the output to a new variable.\n",
    "\n",
    "`.apply()` also handles GroupBy objects, but it has to infer whether the function is for aggregation, transformation, filtration, etc, so it doesn't always correctly choose whether to preserve groups or not, and doesn't always return the correct output shape. If your DataFrame is large (~GB), test a subset to make sure it outputs what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eecf5e86-1900-45fc-807b-204a801056b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   mass_ME  radius_RE\n",
      "planet_type #name                                    \n",
      "Gas Giant   11 Comae Berenices b  4.201103   0.933502\n",
      "            11 Ursae Minoris b    3.191972   0.942145\n",
      "            14 Andromedae b       1.039448   0.994006\n",
      "            14 Herculis b         1.762472   0.968076\n",
      "            16 Cygni B b          0.385462   1.037224\n",
      "...                                    ...        ...\n",
      "Terrestrial TOI-700 e             0.509771   1.117866\n",
      "            TRAPPIST-1 d          0.242452   0.924322\n",
      "            TRAPPIST-1 e          0.428954   1.079157\n",
      "            TRAPPIST-1 h          0.205152   0.885613\n",
      "            YZ Ceti b             0.435171   1.070946\n",
      "\n",
      "[5245 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "temp=df.groupby(['planet_type']).filter(lambda x: len(x) > 5)\n",
    "grouped1=temp.groupby(['planet_type'])[['mass_ME', 'radius_RE']]\n",
    "print(grouped1.apply(lambda g: g/g.agg('mean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7bee5-5665-46e4-b8b8-3b9038391e53",
   "metadata": {},
   "source": [
    "Remember waaaaay back to when we were trying to combine 2 DataFrames and take the higher value wherever both DataFrames had data? We can do that faster and with less code using `.apply()`. I'll repost the dummy DataFrames involved and the clunky function that let us combine them as desired, and then show the improved way of doing it with `.apply()`.\n",
    "\n",
    "Note: again, it's going to look like you could just use `.map(np.nanmax)`, but that won't output the expected result because `nanmax()` takes one ND data structure as input and outputs a scalar. The `apply()` method expects the received function to take two 1D data structures and output either a scalar or a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5efc7aaf-d85d-4047-94d0-24d18f9e342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B   C\n",
      "e  0   1   2\n",
      "f  3   4   5\n",
      "g  6   7   8\n",
      "h  9  10  11 \n",
      "\n",
      "     B    C    D     E\n",
      "f  NaN  NaN  NaN   NaN\n",
      "g  NaN  NaN  NaN   2.0\n",
      "h  3.0  4.0  5.0   6.0\n",
      "i  7.0  8.0  9.0  10.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy2 = pd.DataFrame(np.arange(0,12).reshape(4,3),\n",
    "                        columns = ['A','B','C'],\n",
    "                        index = ['e','f','g','h'])\n",
    "dummy3 = pd.DataFrame(np.arange(-5,11).reshape(4,4),\n",
    "                        columns = ['B','C','D', 'E'],\n",
    "                        index = ['f','g','h','i'])\n",
    "dummy3.mask(dummy3<=1, inplace=True) #create more missing values to replace\n",
    "print(dummy2,'\\n')\n",
    "print(dummy3,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2301d2c1-427f-4a39-a91f-138402d5c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B     C    D     E\n",
      "e  0.0   1.0   2.0  NaN   NaN\n",
      "f  3.0   4.0   5.0  NaN   NaN\n",
      "g  6.0   7.0   8.0  NaN   2.0\n",
      "h  9.0  10.0  11.0  5.0   6.0\n",
      "i  NaN   7.0   8.0  9.0  10.0\n",
      "average time [s]: 0.0025677834033966066\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "# Old clunky function:\n",
    "def elem_wise_nanmax(x,y):\n",
    "    rns = set(list(x.index)+list(y.index))\n",
    "    ser = pd.Series(np.zeros(len(rns)),index=rns)\n",
    "    #replacing zeros saves a little time over replacing NaNs too \n",
    "    for i in list(rns):\n",
    "        try:\n",
    "            ser.loc[i] = np.nanmax([x.loc[i],y.loc[i]])\n",
    "        except IndexError:\n",
    "            if i in x.index:\n",
    "                ser.loc[i]=x.loc[i]\n",
    "            elif i in y.index:\n",
    "                ser.loc[i]=y.loc[i]\n",
    "    return ser\n",
    "\n",
    "import warnings # unfortunately the all-NaN axis necessitates the following\n",
    "warnings.simplefilter('ignore', RuntimeWarning)\n",
    "\n",
    "n_runs=5000\n",
    "times = np.zeros(n_runs)\n",
    "for i in range(n_runs):\n",
    "    t0 = time.time()\n",
    "    result = dummy3.combine(dummy2,elem_wise_nanmax)\n",
    "    times[i] = time.time()-t0\n",
    "print(result)\n",
    "print(\"average time [s]:\", times.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf30c9f8-be11-4403-b7aa-64342f2a4a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B     C    D     E\n",
      "e  0.0   1.0   2.0  NaN   NaN\n",
      "f  3.0   4.0   5.0  NaN   NaN\n",
      "g  6.0   7.0   8.0  NaN   2.0\n",
      "h  9.0  10.0  11.0  5.0   6.0\n",
      "i  NaN   7.0   8.0  9.0  10.0\n",
      "average time [s]: 0.00222184534072876\n"
     ]
    }
   ],
   "source": [
    "# new hotness (less intuitive & still clunky, but slightly faster)\n",
    "def elem_wise_nanmax(row,dfr):\n",
    "    if row.name in dfr.index:\n",
    "        return row.combine(dfr.loc[row.name],\n",
    "                           lambda x, y: np.nanmax([x, y]))\n",
    "    else:\n",
    "        return row #only works by using combine_first() beforehand\n",
    "\n",
    "#warnings.simplefilter('ignore', RuntimeWarning)\n",
    "times = np.zeros(n_runs)\n",
    "for i in range(n_runs):\n",
    "    t0 = time.time()\n",
    "    result = dummy3.combine_first(dummy2).apply(lambda z:\n",
    "                                                elem_wise_nanmax(z,dummy2),\n",
    "                                                axis=1)\n",
    "    times[i] = time.time()-t0\n",
    "print(result)\n",
    "print(\"average time [s]:\", times.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82117cbb-6e2f-4097-8efd-c343e0f461bb",
   "metadata": {},
   "source": [
    "The perhaps more intuitive former algorithm is only vectorized along the column axis; it iterates across every row and avoids processing any cell twice, which one might intuitively think was wasteful. However, the function that uses `combine_first()` with `.apply()` is vectorized along both axes, which is slightly harder to envision but turns out to be faster even though some of the cells in the output are computed twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9a0d1-0ef9-4ab0-890a-cc81d7900f0e",
   "metadata": {},
   "source": [
    "### Windowed Operations\n",
    "Pandas has several methods for computing functions over rolling or expanding windows with a similar interface to GroupBy objects, i.e. you call the windowing method on the Series or DataFrame and then call an aggregating method after the windowing method. All windowing methods have a `min_periods` kwarg to specify the minimum number of valid data points a window must contain for the window to be passed to any subsequent functions. There are 4 windowing methods:\n",
    "\n",
    "1. `.rolling(window=<n_units>)`: evaluates any subsequent function or method over a rolling/sliding window of $n$ rows or time increments, or even a variable number of rows based on a fixed temporal offset (e.g. if some datetimes have more associated observations than others). It is even possible to construct it such that it spans multiple columns at a time, with the kwarg `method='table'`, but only if subsequent calls to aggregating or transforming functions specify `engine='numba'`. This is also one of the methods that lets you apply the `.groupby()` method first and apply windowing on a per group basis. \n",
    "2. `.weighted(win_type='scipy_response_func')`: Similar to `.rolling()` but each window can be weighted by any one of 26 `scipy.signal` windowing functions, like `'gaussian'` or `'triang'` (for triangular), which you pass to the kwarg `win_type`. For those of you with a little (but not a lot of) familiarity with signal processing, essentially, this windowing function lets you convolve your data with a standard response function provided over a window of $n$ increments where $n>1$. Unfortunately, for now, you don't get any of the other perks of the basic `.rolling()` function: no time-based windows, no applying the window on a per-group basis, and no applying the window to multiple Series at a time. The available statistical functions are also much more limited: just the mean, sum, standard deviation, and variance.\n",
    "3. `.expanding()`: This is more of a cumulative windowing operation. After the minimum number of rows given by `min_periods`, statistics and other functions are evaluated for all data points up the current one. Only aggregating methods can be called after this method, and time-based windows aren't allowed. However, you can use this method on GroupBy objects, or apply it to multiple columns simultaneously with `method='table'` and `engine='numba'` set in aggregate method applied after the windowing operation.\n",
    "4. `.emw()`: short for exponentially weighted moving window, which is most similar to the expanding window method except that every data point before the current position is down-weighted according to an exponential decay function that can specified one of 4 ways depending on  the physical interpretation of your data and whether or not they constitute a time series. The use of this is outside my field of expertise so I will refer interested readers to [the official documentation on how to specify the parameters and what they mean](https://pandas.pydata.org/docs/user_guide/window.html#exponentially-weighted-window).\n",
    "\n",
    "To summarize:\n",
    "\n",
    "| Method | Windowing type | Allows time-based windows? | Allows 2D windows? | Can apply to GroupBy Objects? |\n",
    "|---|---|---|---|---|\n",
    "| `.rolling()` | rolling (aka sliding) | Yes | Yes | Yes | \n",
    "| `.rolling(win_type='response_func')` | rolling, weighted by `SciPy.signal` functions | No | No | No | \n",
    "| `.expanding()` | expanding (cumulative) | No | Yes | Yes | \n",
    "| `.emw()` | exponentially-weighted moving | only with `halflife` | No | Yes | \n",
    "\n",
    "Most built-in methods that can be called on GroupBy objects can also be called on windows, except for the rather limited `expanding()` window.\n",
    "\n",
    "For these examples I'll make a DataFrame of the monthly average highs, lows, and rainfall of my hometown in Florida, which I imagine will be a fairly interesting change of pace for those of you native to Northern Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d792c4eb-99c3-4252-9c15-a588b412922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   highs_C lows_C precip_mm      season\n",
      "1     18.0    6.0        84        fall\n",
      "2     20.0    8.0        81      spring\n",
      "3     24.0   10.0       100      spring\n",
      "4     27.0   14.0        67      spring\n",
      "5     30.0   18.0        63  dry summer\n",
      "6     32.0   22.0       164  dry summer\n",
      "7     33.0   23.0       166  wet summer\n",
      "8     33.0   23.0       172  wet summer\n",
      "9     31.0   21.0       208  wet summer\n",
      "10    27.0   16.0       100  wet summer\n",
      "11    23.0   11.0        54        fall\n",
      "12    20.0    8.0        71        fall\n"
     ]
    }
   ],
   "source": [
    "j = pd.DataFrame(np.array([[18.,20.,24., 27.,30.,32., 33.,33.,31., 27.,23.,20.],\n",
    "                           [6.,8.,10., 14.,18.,22., 23.,23.,21., 16.,11.,8.],\n",
    "                           [84,81,100, 67,63,164, 166,172,208, 100,54,71],\n",
    "                           ['fall','spring','spring', 'spring','dry summer','dry summer', \n",
    "                           'wet summer','wet summer','wet summer', 'wet summer','fall','fall']]).T,\n",
    "                 columns = ['highs_C', 'lows_C', 'precip_mm', 'season'],\n",
    "                 index=range(1,13))\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "8377e4d3-4d14-4e4f-a544-3afe554028a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               highs_C  lows_C\n",
      "season                        \n",
      "dry summer 5       NaN     NaN\n",
      "           6      31.0    20.0\n",
      "fall       1       NaN     NaN\n",
      "           11     20.5     8.5\n",
      "           12     21.5     9.5\n",
      "spring     2       NaN     NaN\n",
      "           3      22.0     9.0\n",
      "           4      25.5    12.0\n",
      "wet summer 7       NaN     NaN\n",
      "           8      33.0    23.0\n",
      "           9      32.0    22.0\n",
      "           10     29.0    18.5\n"
     ]
    }
   ],
   "source": [
    "print(j.groupby('season')[['highs_C', 'lows_C']].rolling(window=2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "346d3d66-3e02-4bbb-81a9-6eb3acce99e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       84.0\n",
      "2      165.0\n",
      "3      265.0\n",
      "4      332.0\n",
      "5      395.0\n",
      "6      559.0\n",
      "7      725.0\n",
      "8      897.0\n",
      "9     1105.0\n",
      "10    1205.0\n",
      "11    1259.0\n",
      "12    1330.0\n",
      "Name: precip_mm, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(j['precip_mm'].expanding().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "31d20844-f391-4b47-a8c7-16a590f4067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1         NaN\n",
      "2         NaN\n",
      "3         NaN\n",
      "4      86.750\n",
      "5      80.625\n",
      "6      81.750\n",
      "7     114.250\n",
      "8     153.125\n",
      "9     173.250\n",
      "10    175.750\n",
      "11    143.750\n",
      "12     92.625\n",
      "Name: precip_mm, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(j['precip_mm'].rolling(window=4, win_type='triang').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d95ab-8cc4-4cbb-b51c-f20b5856cc77",
   "metadata": {},
   "source": [
    "### Iteration\n",
    "Iteration should generally be avoided if at all possible because it is much slower than comparable vectorized methods. Nevertheless, if you can't figure out how to avoid it, there are 3 iteration methods:\n",
    "\n",
    "- `.items()`: generators iterators of (index, value) pairs for Series and (column, Series) pairs for DataFrames. Of the 3 iteration methods, this is the only one you might also call on a Series. The others only make sense for DataFrames\n",
    "- `.iterrows()`: generates pairs of (row_index/label, row contents) where row contents are returned as Series, which means datatypes are not preserved. For basic dataframes, this iterates orthogonally to the way `.items()` does. \n",
    "- `.itertuples()`: generates an iterator of rows packed into `namedtuple()` objects where the first field is Index and remaining field names are column labels. This method preserves dtypes & is faster than `.iterrows()`, but you have to be somewhat familiar with the `collections.namedtuple()` object factory\n",
    "\n",
    "   - [namedtuple](https://docs.python.org/3/library/collections.html#collections.namedtuple) is a factory function from the `collections` module that lets you create tuples with named fields such that values are accessible like attributes without defining a separate class. Functionally, it's somewhere between a class and a dictionary, but a namedtuple is smaller in memory than a dict containing the same information. However, the the keys (i.e. the field names) are more restricted in the values they can take; since they're accessed as attributes and not as strings in square brackets, leading underscores are disallowed, and you must take care not to use any Python command or keyword ('def', 'is', 'class', 'global', etc.) as a namedtuple key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "355d7e81-abba-4a9e-93ff-bacd873ce1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    food  qtty    unit      g\n",
      "0   eggs     3  whites  147.8\n",
      "1   spam     4      oz  113.4\n",
      "2  toast     2  slices   76.0\n"
     ]
    }
   ],
   "source": [
    "#make a new junk dataframe\n",
    "junk = pd.DataFrame([['eggs',3,'whites',147.8],\n",
    "                     ['spam',4, 'oz',113.4],\n",
    "                     ['toast',2,'slices',76]],\n",
    "                     columns=['food','qtty','unit','g'])\n",
    "print(junk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59b7643-53fb-4d50-bde2-af9e0b705f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food ['eggs' 'spam' 'toast']\n",
      "qtty [3 4 2]\n",
      "unit ['whites' 'oz' 'slices']\n",
      "g [147.8 113.4  76. ]\n"
     ]
    }
   ],
   "source": [
    "for i,c in junk.items():\n",
    "    print(i,c.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d56d272-7b7e-4f05-a456-4b1fab934dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['eggs' 3 'whites' 147.8]\n",
      "1 ['spam' 4 'oz' 113.4]\n",
      "2 ['toast' 2 'slices' 76.0]\n"
     ]
    }
   ],
   "source": [
    "for i,r in junk.iterrows():\n",
    "    print(i,r.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "376bc4b4-3b60-4a37-bd40-895ee2427422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas(Index=0, food='eggs', qtty=3, unit='whites', g=147.8)\n",
      "Pandas(Index=1, food='spam', qtty=4, unit='oz', g=113.4)\n",
      "Pandas(Index=2, food='toast', qtty=2, unit='slices', g=76.0)\n"
     ]
    }
   ],
   "source": [
    "for itp in junk.itertuples():\n",
    "    print(itp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b15952-f9db-42dc-b239-5ec54a7fac97",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "Pandas was designed for financial panel data, which is fundamentally a kind of time series, so Pandas has a well-developed suite of time series functions. These functions are optimally suited for time increments from nanoseconds to weeks; if your smallest time step is larger than a week, you can certainly degrade the native time resolution to span a larger time duration, but the lengths you must go to keep the resolution coarse and adapt other functions to it are such that I would not recommend going through the trouble.  \n",
    "\n",
    "#### Defining, parsing, & creating the main data types\n",
    "Pandas incorporates NumPy `datetime64` & `timedelta64` data types, plus object classes from the `datetime` module & its `dateutil` extension, to define 3 Pandas data types & 1 scalar class:\n",
    "1. `datetime64[ns(,tz)]`: datatype of a Series of Timestamp-type (NumPy datetime64) scalars, which may be just dates, just times, or most often some combination of the two. The default units are ns, and a timezone (tz) can optionally be specified. Coerced to `DatetimeIndex` if used as an Index for a Series or DataFrame.\n",
    "2. `timedelta64[ns]`: datatype of a Series of Timedelta-type (NumPy timedelta64) scalars, associated with a unit that defaults to ns, representing absolute time increments from some user-defined start time. Coerced to `TimedeltaIndex` if used as an Index for a Series or DataFrame. Each index may be treated either as points or bin edges.\n",
    "3. `period[freq]`: like `datetime64`, but lets one specify a start date and a recurrence rate. These are typically treated like bins that data falls *between*, not instantaneous points in time at which data is measured. Coerced to `PeriodIndex` if set as the Index for a Series or DataFrame. \n",
    "\n",
    "- `DateOffset`: not its own Pandas data type, but imported implicitly from `dateutil` to combine `timedelta`-like functionality with calendar rule (e.g. to handle leap years or daylight savings time). I won't go into these much in the interest of time and because these are fairly niche; [the documentation for this is pretty good](https://pandas.pydata.org/docs/user_guide/timeseries.html#dateoffset-objects)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56addf9-20dd-4382-a913-6e9657e96272",
   "metadata": {},
   "source": [
    "Most of the time your job will involve parsing dates and times from other sources rather than generating them from scratch, so Pandas includes a few relevant conversion functions:\n",
    "\n",
    "- `pd.to_datetime(*arg, fmt=None, **kwargs)`: This function accepts strings, arrays, Series, or DataFrames and either attempts to format the data based on a format string passed to `fmt` or tries to infer the typical format with or without the aid of extra kwargs. If the data are associated with time zones and you set the `utc` kwarg to `True`, the data will be automatically converted to UTC. Please see the official documentation for the full list of kwargs, as it is extensive.\n",
    "- `pd.to_timedelta(*arg, unit=None)`: This function accepts strings, arrays, or Series and tries to convert them to time increments based on either the `unit` kwarg or acceptable unit characters that are part of the data in the argument. Please see the official documentation for the full list of acceptable units; as any time units from months to nanseconds are allowed, most have more than one possible format, and several of the most common abbreviations are *disallowed* due to ambiguity. Unfortunately, units of months or larger are entirely disallowed. In those cases, you will need to do additional reformatting.\n",
    "\n",
    "It's a good idea to spend some time digesting the [summary table on time series data types and methods of definition in the Pandas documentation](https://pandas.pydata.org/docs/user_guide/timeseries.html#overview). Still, it can be hard to wrap your head around the meanings of and differences between each time series data type without seeing them in use, so let's dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8b5b8aa-080f-471e-ba14-1f76679f3bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2023-12-01 08:11:00', '2023-12-11 08:25:00',\n",
      "               '2023-12-21 08:34:00', '2023-12-31 08:36:00',\n",
      "               '2024-01-10 08:32:00', '2024-01-20 08:21:00'],\n",
      "              dtype='datetime64[ns]', freq=None) \n",
      "\n",
      "2023-12-01 08:11:00    7.47583\n",
      "2023-12-11 08:25:00    7.14306\n",
      "2023-12-21 08:34:00    7.01583\n",
      "2023-12-31 08:36:00    7.11306\n",
      "2024-01-10 08:32:00    7.42167\n",
      "2024-01-20 08:21:00    7.90278\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#from datetime import datetime as dttm\n",
    "sunrises= ['2023-12-01 08:11:00',\n",
    "           '2023-12-11 08:25:00',\n",
    "           '2023-12-21 08:34:00',\n",
    "           '2023-12-31 08:36:00',\n",
    "           '2024-01-10 08:32:00',\n",
    "           '2024-01-20 08:21:00']\n",
    "daylens = [7.47583, 7.14306, 7.01583,\n",
    "           7.11306, 7.42167, 7.90278]\n",
    "srt = pd.to_datetime(sunrises,\n",
    "                     format=\"%Y-%m-%d %H:%M:%S\")\n",
    "print(srt, '\\n')\n",
    "ts = pd.Series(daylens,index=srt)\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d44cf5e-ad7f-4c95-9d2e-3ca22fcde533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAGYCAYAAACpqFPBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQQUlEQVR4nO3dd3hT9f4H8HfSdE8oHXTQBS0b2gIyRJZsuILIEi2lsvGC13VFvdfrTxT0gqIgCIpFZDgBlb23SqFll9XSQSctdLdJk3x/f5TmUlltSXMy3q/nyePTNMH3J9CTd0/O+R6ZEEKAiIiIyMzJpQ5AREREZAgsPURERGQRWHqIiIjIIrD0EBERkUVg6SEiIiKLwNJDREREFoGlh4iIiCyCQuoAxkKr1SIzMxPOzs6QyWRSxyEiIqJaEEKguLgYPj4+kMsfvC+Hpee2zMxM+Pv7Sx2DiIiI6iE9PR1+fn4PfAxLz23Ozs4Aql40FxcXidMQERFRbRQVFcHf31/3Pv4gLD23VX+k5eLiwtJDRERkYmpzaAoPZCYiIiKLwNJDREREFoGlh4iIiCwCSw8RERFZBJYeIiIisggsPURERGQRWHqIiIjIIrD0EBERkUVg6SEiIiKLwNJDREREFoGlh4iIiBpciVItdQSWHiIiImpYKrUWwz47jL9vSEBeiVKyHLzgKBERETWoNb+nICW/DKUqDeytrSTLwT09RERE1GAKylRYsu8qAOCV/qFwtJVufwtLDxERETWYT/deQWF5JVp6O2N0J39Js7D0EBERUYO4lleKb39PBQC8NbQVrOQySfOw9BAREVGDWLA9EWqtQO8wD/Rs4SF1HJYeIiIi0r8/k/Ox83wOrOQyvDWkldRxALD0EBERkZ5ptQLztiYCAMZ19kcLL2eJE1Vh6SEiIiK9+uV0Bs5mFMLJVoF/9A+VOo4OSw8RERHpTblKg492XAIAzOwTgiZOthIn+h+WHiIiItKbVUeSkVVYAV83e8T0CJI6Tg0sPURERKQXucUVWH4gCQDw+qAw2Em4+vK9sPQQERGRXnyy+wpKVRp08HPF8PY+Use5C0sPERERPbJL2cX4Pi4NAPD2sNaQS7wQ4b2w9BAREdEje39bIrQCGNzWG50DG0sd555YeoiIiOiRHLiUi0OXb8DaSoY3BreUOs59sfQQERFRvak1WnywrWohwondAhHg7ihxovtj6SEiIqJ6++HEdVzOKYGbgzX+3reF1HEeiKWHiIiI6qVEqcbHu6sWIpzdtwVcHawlTvRgLD1ERERUL18cSEJeiQpBTRzxXNcAqeM8FEsPERER1VlmQTm+PJwMAHhjcEvYKIy/Uhh/QiIiIjI6/915CUq1Fl2CGmNAay+p49QKSw8RERHVyZnrBdiUkAEA+NfQ1pDJjG8hwnth6SEiIqJaE0Jg3paqU9SfDvdFOz9XiRPVHksPERER1drO8zk4nnITtgo5Xh0YJnWcOmHpISIiolpRqbVYsL1qL8+UnsHwcbOXOFHdsPQQERFRrXz7RypS8svQxMkW03uHSB2nzlh6iIiI6KEKylT4bO8VAMArA0LhZKuQOFHdsfQQERHRQy3ZdxWF5ZUI83LGmE7+UsepF5YeIiIieqCUvFKs+T0FAPDW0FawkpvGKep/xdJDRERED7Rg+0VUagR6hXrgiVAPqePUG0sPERER3dfxazex43w25LKqvTymzChLT2BgIGQy2V23WbNm3fc569atQ4cOHeDg4ICmTZti0qRJyM/PN2BqIiIi86LVCry/9QIAYFyXZgj1cpY40aMxytITFxeHrKws3W337t0AgNGjR9/z8UeOHEFUVBReeOEFnD9/Hj/++CPi4uIwefJkQ8YmIiIyK7+ezsTp64VwtLHCP54MlTrOIzPK8808PGp+XrhgwQKEhISgV69e93z8H3/8gcDAQMyePRsAEBQUhGnTpuGjjz5q8KxERETmqKJSg492XAQAzOzTHB7OthInenRGuafnTiqVCmvXrkVMTMx9L2jWvXt3XL9+Hdu2bYMQAjk5Ofjpp58wdOjQ+/65SqUSRUVFNW5ERERUZdWRa8gsrICPqx1eeDxI6jh6YfSlZ/PmzSgoKEB0dPR9H9O9e3esW7cOY8eOhY2NDby9veHm5oYlS5bc9znz58+Hq6ur7ubvb5prDhAREenbjWIllu2/CgB4fVBL2FlbSZxIP4y+9KxatQqDBw+Gj4/PfR9z4cIFzJ49G//+979x8uRJ7NixA9euXcP06dPv+5y5c+eisLBQd0tPT2+I+ERERCbnkz2XUarSoL2fK/7W4f7vv6bGKI/pqZaamoo9e/Zg48aND3zc/Pnz0aNHD7z22msAgPbt28PR0RE9e/bEvHnz0LRp07ueY2trC1tb0/98koiISJ8u5xTju+NpAIC3h7aG3EQXIrwXo97TExsbC09PzwcemwMAZWVlkMtrjmJlVbUrTgjRYPmIiIjMzftbE6EVwKA23ugS1FjqOHpltKVHq9UiNjYWEydOhEJRc4fU3LlzERUVpft6+PDh2LhxI5YvX47k5GQcPXoUs2fPRpcuXR74sRgRERH9z6HLN3Dw8g1YW8nwxuCWUsfRO6P9eGvPnj1IS0tDTEzMXd/LyspCWlqa7uvo6GgUFxdj6dKleOWVV+Dm5oa+ffviww8/NGRkIiIik6XRCry/NREA8HzXQAQ2cZQ4kf7JBD//AQAUFRXB1dUVhYWFcHFxkToOERGRQW04noa5G8/C1d4aB1/rDTcHG6kj1Upd3r+N9uMtIiIiMowSpRqLdl0GAMzu18JkCk9dsfQQERFZuBUHk5BXokSguwOe7xogdZwGw9JDRERkwTILyrHyUDIA4I3BrWCjMN9qYL6TERER0UMt3HkJSrUWXQIbY2AbL6njNCiWHiIiIgt19nohNiZkAADeHtbqvte4NBcsPURERBZICIF5Wy8AAEaG+6K9n5u0gQyApYeIiMgC7bqQgz+v3YStQo7XBoZJHccgWHqIiIgsjEqtxYLtFwEAk3sGwcfNXuJEhsHSQ0REZGHW/ZmKa3mlaOJkgxm9m0sdx2BYeoiIiCxIYVklPt17BQDwcv8wONka7RWp9I6lh4iIyIIs2XcFBWWVCPVywphOflLHMSiWHiIiIguRml+Kb35PAQC8OaQVFFaWVQMsa1oiIiILtmD7RVRqBJ4I9UDvME+p4xgcSw8REZEFiEu5ie3nsiGXAW8NaSV1HEmw9BAREZk5rVZg3tZEAMDYzv4I83aWOJE0WHqIiIjM3G9nMnE6vQCONlb4R/9QqeNIhqWHiIjIjFVUavDRjksAgBm9Q+DpbCdxIumw9BAREZmxr49eQ0ZBOZq62uGFx4OljiMplh4iIiIzlVeixLL9SQCA1waGwd7GSuJE0mLpISIiMlOf7L6MEqUa7XxdMaKjr9RxJMfSQ0REZIau5BRjw/E0AMDbQ1tBLpdJnEh6LD1ERERm6INtidAKYEBrLzwW7C51HKPA0kNERGRmDl+5gf2XbkAhl2GuhS5EeC8sPURERGZEoxV4//ZChM93C0BQE0eJExkPlh4iIiIz8tPJdFzMLoaLnQJz+rWQOo5RYekhIiIyE6VKNRbuugwAmN2vBdwcbCROZFxYeoiIiMzEioNJuFGsRIC7A6K6BUodx+iw9BAREZmBrMJyrDycDAB4Y1BL2Cj4Fv9XfEWIiIjMwH93XkJFpRadAxthUFtvqeMYJZYeIiIiE3cuoxAb4zMAAG8PbQ2ZjAsR3gtLDxERkQkTQmDe1gsAgKc6+qCDv5u0gYwYSw8REZEJ25OYiz+Sb8JGIcdrA8OkjmPUWHqIiIhMVKVGi/nbqhYinPx4EPwaOUicyLix9BAREZmodX+kIjmvFE2cbDCjd4jUcYweSw8REZEJKiyrxKd7rwAAXnoyFM521hInMn4sPURERCZo6f4ruFVWiRaeThjX2V/qOCaBpYeIiMjEpOWX4ZtjqQCAN4e2gsKKb+e1wVeJiIjIxHy44yJUGi16tmiC3qEeUscxGSw9REREJuREyk1sPZsFuQx4a2grLkRYByw9REREJqJqIcKqU9THdPJHS28XiROZFpYeIiIiE/HbmSycSi+Ag40VXh4QKnUck8PSQ0REZAIqKjX4cPtFAMCMXiHwdLaTOJHpYekhIiIyAbFHU5BRUA5vFztM7hksdRyTxNJDRERk5PJLlFi2/yoA4LWBYbC3sZI4kWli6SEiIjJyi/dcQbFSjba+LhgZ7it1HJPF0kNERGTEruYWY/3xNADAW0NaQy7nKer1xdJDRERkxD7YdhEarUD/1l7oFuIudRyTxtJDRERkpI5cycO+i7lQyGWYO7il1HFMHksPERGREdJoBeZtvQAAeK5rAII9nCROZPpYeoiIiIzQzyev42J2MVzsFJjTr4XUccwCSw8REZGRKVWqsXDXJQDA3/u2QCNHG4kTmQeWHiIiIiOz4lAycouVaNbYAVHdA6SOYzZYeoiIiIxIdmEFVh5KAgC8MbglbBVciFBfWHqIiIiMyMJdl1BRqUWngEYY3NZb6jhmhaWHiIjISJzLKMTP8dcBAG8NbQWZjAsR6hNLDxERkREQQuD9rYkQAvhbBx+EN2skdSSzY5SlJzAwEDKZ7K7brFmz7vscpVKJt956CwEBAbC1tUVISAi+/vprA6YmIiKqv72Jufg9OR82CjleHxQmdRyzpJA6wL3ExcVBo9Hovj537hz69++P0aNH3/c5Y8aMQU5ODlatWoXmzZsjNzcXarXaEHGJiIgeSaVGiw+2JwIAYnoEwa+Rg8SJzJNRlh4PD48aXy9YsAAhISHo1avXPR+/Y8cOHDx4EMnJyWjcuDGAqr1FREREpmD9n2lIvlEKd0cbzOwTInUcs2WUH2/dSaVSYe3atYiJibnvAV2//vorOnXqhI8++gi+vr4IDQ3Fq6++ivLy8vv+uUqlEkVFRTVuREREhlZYXonFey4DAF7qHwoXO2uJE5kvo9zTc6fNmzejoKAA0dHR931McnIyjhw5Ajs7O2zatAl5eXmYOXMmbt68ed/jeubPn4933323gVITERHVzrL9V3GrrBLNPZ0wvrO/1HHMmkwIIaQO8SADBw6EjY0Nfvvtt/s+ZsCAATh8+DCys7Ph6uoKANi4cSOeeeYZlJaWwt7e/q7nKJVKKJVK3ddFRUXw9/dHYWEhXFxc9D8IERHRX6TfLEO/RQeh0mgRG90ZfVp6Sh3J5BQVFcHV1bVW799GvacnNTUVe/bswcaNGx/4uKZNm8LX11dXeACgVatWEELg+vXraNHi7gu12drawtbWVu+ZiYiIamvBjotQabR4vHkT9A7zePgT6JEY9TE9sbGx8PT0xNChQx/4uB49eiAzMxMlJSW6+y5fvgy5XA4/P7+GjklERFRnJ1NvYeuZLMhkwJtDuBChIRht6dFqtYiNjcXEiROhUNTcITV37lxERUXpvn722Wfh7u6OSZMm4cKFCzh06BBee+01xMTE3POjLSIiIikJITBv6wUAwJhIf7T24WEVhmC0pWfPnj1IS0tDTEzMXd/LyspCWlqa7msnJyfs3r0bBQUF6NSpEyZMmIDhw4fjs88+M2RkIiKiWtlyJgsJaQVwsLHCKwNCpY5jMYz+QGZDqcuBUERERPVVUanBkx8fxPVb5fjHk6GY8+Tdx51S7dXl/dto9/QQERGZo9XHUnD9Vjm8XGwx5YkgqeNYFJYeIiIiA8kvUeLzfVcBAK8NbAkHG6M+idrssPQQEREZyKd7r6BYqUYbHxc8He4rdRyLw9JDRERkAFdzi7Huz6qTcN4a2gpyOU9RNzSWHiIiIgOYv+0iNFqBJ1t5oXtIE6njWCSWHiIiogZ27Goe9l7MhUIuw9whLaWOY7FYeoiIiBqQRiswb2siAGDCY80Q4uEkcSLLxdJDRETUgH6Ov44LWUVwtlNgzpNciFBKLD1EREQNpEylxsKdlwAAf+/bHI0dbSROZNlYeoiIiBrIykPJyC1Wwr+xPSZ2D5Q6jsVj6SEiImoAOUUVWHEwGQDwxqBWsFVYSZyIWHqIiIgawMKdl1BeqUFkQCMMaectdRwCSw8REZHenc8sxE/x1wFULUQok3EhQmPA0kNERKRHQgi8vzURQgDD2jdFRLNGUkei21h6iIiI9GjfxVwcS8qHjUKOfw7iQoTGhKWHiIhITyo1WnywrWohwkk9AuHf2EHiRHQnlh4iIiI92XA8DUk3StHY0Qaz+jSXOg79BUsPERGRHhRVVGLxnisAgH882QIudtYSJ6K/YukhIiLSg8/3X8XNUhVCPBwxvkszqePQPbD0EBERPaL0m2WIPZICAHhzSCsorPj2aoz4t0JERPSIPtxxESqNFj2au6NvS0+p49B9sPQQERE9gvi0W9hyJgsyGfDWkNZciNCIsfQQERHVkxAC87ZcAAA8E+GH1j4uEieiB2HpISIiqqetZ7MQn1YAe2srvDowTOo49BAsPURERPWgVGvw4Y6LAIBpvYLh5WIncSJ6GJYeIiKievjmWArSb5bDy8UWU58IljoO1QJLDxERUR3dLFVhyb6rAIBXBoTBwUYhcSKqDZYeIiKiOvp0z2UUV6jRuqkLRkX4SR2Haomlh4iIqA6u5pZg7Z9pAIC3h7aClZynqJsKlh4iIqI6WLA9ERqtQL+WnujevInUcagOWHqIiIhq6VhSHvYk5sJKLsPcIa2kjkN1xNJDRERUC1qtwPtbEwEAEx5rhuaeThInorpi6TGAEqUa6TfLpI5BRESPYGNCBs5nFsHZVoE5/VpIHYfqgaWngaXll2Hk50cxMfY4iioqpY5DRET1UKZS4787qxYifLFvc7g72UqciOqDpaeB2dtYoUSpRvKNUrz03SlotELqSEREVEdfHrqGnCIl/BrZY2L3QKnjUD2x9DQwD2dbrHg+ErYKOfZdzMWiXZekjkRERHWQU1SBLw4mAQD+Oagl7KytJE5E9cXSYwDt/dzw4aj2AIBlB5Lw2+lMiRMREVFtLdp1CeWVGoQ3c8Ow9k2ljkOPgKXHQEaE++quzfLaT6dxPrNQ4kRERPQwFzKL8OPJ6wCAt4e2hkzGhQhNGUuPAf1zUEv0bNEEFZVaTF1zEvklSqkjERHRfQgh8P62CxACGNq+KSIDGkkdiR4RS48BWcllWDo+AoHuDsgoKMfMdfGo1GiljkVERPew/1Iujl7Nh42VHG8Mail1HNIDlh4Dc3WwxsqoTnC0scKf125i3pYLUkciIqK/UGu0+GBb1Snqk3oEwr+xg8SJSB9YeiQQ6uWMT8Z2BAB883sqvo9LkzYQERHVsCEuHVdzS9DIwRoz+zSXOg7pCUuPRAa08cbL/UMBAG9vPoeTqTclTkRERABQVFGJT3ZfBgC89GQoXO2tJU5E+sLSI6EX+zTHoDbeqNQITF8bj+zCCqkjERFZvGX7k3CzVIVgD0c8+1gzqeOQHrH0SEgul2HRmA4I83LGjWIlpn17AhWVGqljERFZrPSbZfj66DUAwJuDW8Haim+T5oR/mxJztFXgy6hOcHOwxunrhXhz41kIwUtVEBFJ4b87L0Gl1qJbsDv6tfKUOg7pGUuPEWjm7oDPn42AlVyGjQkZWHXkmtSRiIgsTkLaLfx6OhMyGfDW0FZciNAMsfQYiR7Nm+DNIa0AAB9sS8SRK3kSJyIishxCCMzbmggAGBXhh7a+rhInoobA0mNEYnoEYlSEH7QCmLU+Hqn5pVJHIiKyCNvPZeNk6i3YW1vh1QFhUsehBsLSY0RkMhneH9kWHfzdUFheiSlrTqBEqZY6FhGRWVOqNZi/vWovz5QnguHtaidxImooLD1Gxs7aCiuei4SHsy0u55TglR9OQavlgc1ERA1lzbFUpN8sh6ezLabdvjA0mSeWHiPk7WqHL56LhI2VHDvP52DJvqtSRyIiMkvZhRVYsu8KAODVAWFwtFVInIgaEkuPkYoMaIR5I9oCAD7Zcxk7z2dLnIiIyLyUKNWYtDoORRVqtPFxwahIP6kjUQNj6TFiYzr7I7p7IADg5e9P4XJOsbSBiIjMRKVGi5nr4pGYVYQmTjb44rlIWMl5irq5Y+kxcm8NbYVuwe4oVWkwZc0JFJSppI5ERGTShBD41+ZzOHT5BuytrfB1dGdeRd1CsPQYOWsrOT6fEAFfN3uk5pfh7xsSoNZopY5FRGSyPt9/Fd/FpUMuA5aMD0d7PzepI5GBGGXpCQwMhEwmu+s2a9ashz736NGjUCgU6NixY8MHNZDGjjb4MqoT7K2tcPhKHj7ccVHqSEREJmlTwnUs3FV1BfX//K0NnmztJXEiMiSjLD1xcXHIysrS3Xbv3g0AGD169AOfV1hYiKioKPTr188QMQ2qtY8LFo7uAAD48vA1bIy/LnEiIiLTciwpD6//dAYAMPWJYER1C5Q2EBmcUZYeDw8PeHt7625btmxBSEgIevXq9cDnTZs2Dc8++yy6dev20P+HUqlEUVFRjZuxG9q+KWb1CQEAvLHxLM5cL5A2EBGRibicU4xp355EpUZgaLumeGNQS6kjkQSMsvTcSaVSYe3atYiJiXngxd9iY2ORlJSEd955p1Z/7vz58+Hq6qq7+fv76ytyg3qlfxj6tfSESq3F1DUnkVtcIXUkIiKjlltUgUmxcSiuUKNTQCMsGtMBcp6pZZGMvvRs3rwZBQUFiI6Ovu9jrly5gjfeeAPr1q2DQlG7haXmzp2LwsJC3S09PV1PiRuWXC7DJ+M6ItjDEdlFFZixNh5KtUbqWERERqlUqUbMN3HIKChHUBNHfBnVCXbWVlLHIokYfelZtWoVBg8eDB8fn3t+X6PR4Nlnn8W7776L0NDQWv+5tra2cHFxqXEzFS521vgyqhOc7RQ4mXoL//n1PITgpSqIiO6k1mjx4vp4nMsogrujDVZP6oxGjjZSxyIJyYQRv1umpqYiODgYGzduxFNPPXXPxxQUFKBRo0awsvpfc9dqtRBCwMrKCrt27ULfvn0f+v8qKiqCq6srCgsLTaYA7b+Ui5jVcRACeG9EWzzfNUDqSERERkEIgbc2n8P6P9NgZy3HhildEd6skdSxqAHU5f3bqPf0xMbGwtPTE0OHDr3vY1xcXHD27FmcOnVKd5s+fTrCwsJw6tQpPPbYYwZMbFh9wjzx+sCqg/He/fU8/kzOlzgREZFx+OJgMtb/mQaZDPh0XDgLDwEAjPbKalqtFrGxsZg4ceJdx+nMnTsXGRkZWLNmDeRyOdq2bVvj+56enrCzs7vrfnM0vVcwLmQV4bfTmZi5Lh6/vNgDfo24sigRWa5fTmXo1jP797DWGNjGW+JEZCyMdk/Pnj17kJaWhpiYmLu+l5WVhbS0NAlSGR+ZTIaPRrVHGx8X5JeqMHXNSZSreGAzEVmmP5Pz8dqPVWvxxPQIwqQeQRInImNi1Mf0GJIpHtNzp4yCcvxtyRHkl6owrH1TLBkf/sBT/ImIzM3V3BKMWn4MheWVGNTGG59PiOBFRC2A2RzTQ7Xn62aPZRMioJDLsOVMFr44mCx1JCIig7lRrER07HEUllcivJkbFo/ryMJDd2HpMSOPBbvjnb+1AQB8tPMi9l/MlTgREVHDK1Op8cI3cbh+qxwB7g74imvx0H2w9JiZ5x5rhvFd/CEEMPu7BCTdKJE6EhFRg9FoBWZvSMCZ64Vo5GCN1ZO6wN3JVupYZKRYesyMTCbDu39ri04BjVBcocaUNSdQVFEpdSwiIr0TQuDd385jT2IubBRyfDWxE4KaOEodi4wYS48ZslHIsey5CHi72CH5Rile+u4UNFoer05E5uWrw9ew5vfUqrV4xnZEZEBjqSORkWPpMVOeznZYGRUJW4Uc+y7m4uPdl6SORESkN1vPZOH9bYkAgLeGtMLgdk0lTkSmgKXHjLX3c8OCUe0AAJ/vT8KWM5kSJyIienQnUm7iHz+cAgBEdw/EC49zLR6qHZYeMzcy3A9TelZtEF778QzOZxZKnIiIqP6Sb5Rg8poTUKm16N/aC/8a1pprklGtsfRYgH8OaomeLZqgvFKDqWtOIr9EKXUkIqI6yytRIjo2DgVllejg74bPxoVzLR6qE5YeC6CwkmPp+AgEuDsgo6Acs9bHo1KjlToWEVGtlas0mPzNCaTdLIN/Y3usmtgJ9jZci4fqhqXHQrg6WOPLqE5wtLHCH8k3MW/LBakjERHVikYr8NL3CTiVXgC322vxNOFaPFQPLD0WJNTLGZ+M7QgA+Ob3VHwfx4u2EpHxm7f1Anaez4GNlRwrn++EEA8nqSORiWLpsTAD2njjH0+GAgDe3nwOJ1NvSZyIiOj+Vh25htijKQCARWM6oEsQ1+Kh+mPpsUB/79scA9t4oVIjMH3tSWQXVkgdiYjoLjvOZWHe1qqP4t8Y3BLDO/hInIhMHUuPBZLLZVg0piPCvJxxo1iJad+eQEWlRupYREQ68Wm3MOe7UxACeK5rM0x7IljqSGQGWHoslJOtAl9GdYKbgzVOXy/EW5vOQQheqoKIpJeSV4rJ35yAUq1F35ae+M/wNlyLh/SCpceCNXN3wNLxEZDLgJ/jr+Pr25+bExFJ5WapCpNWx+FmqQrtfF2xZHw4FFZ8qyL94L8kC/d4iyZ4a2hrAMAH2xJx5EqexImIyFJVVGowZc0JXMsrha+bPVZFd4KjrULqWGRGWHoIMT0CMSrCDxqtwKz18UjNL5U6EhFZGK1W4OUfTuFk6i242CmwelJneDrbSR2LzAxLD0Emk+H9kW3Rwc8VheWVmLrmJEqVaqljEZEFmb89EdvOZsPaSoaVUZ3QwstZ6khkhlh6CABgZ22FFc93goezLS7lFOPlH05Bq+WBzUTU8L45loIvD18DACwc3QFdg90lTkTmiqWHdLxd7fDFc5GwsZJj5/kcLNl3VepIRGTmdl/Iwbu/nQcAvDYwDE919JU4EZkzlh6qITKgEd4b0QYA8Mmey9h1PlviRERkrk6nF+DvG+KhFcD4Lv6Y2TtE6khk5lh66C5jOzfDxG4BAIB/fH8Kl3OKJU5EROYm/WYZXvgmDhWVWvQK9cB7T7XlWjzU4Fh66J7eHtYaXYMbo1RVdQppQZlK6khEZCYKylSYGHsceSUqtG7qgs8nRHAtHjII/iuje7K2kuPzZyPg62aP1Pwy/H1DAtQardSxiMjEVVRqMHXNSSTfKIWPqx1iJ3WGE9fiIQNh6aH7cneyxcqoSNhbW+HwlTx8tPOS1JGIyIRptQKv/XQGx1NuwtlWgdhJXeDlwrV4yHBYeuiB2vi44r+j2wMAVh5KxqaE6xInIiJT9dHOS/jtdCYUchm+eD4SYd5ci4cMi6WHHmpYex/M6lN1VsU/fz6LM9cLpA1ERCZn7R+p+OJgEgDgw1Ht0aN5E4kTkSVi6aFaeaV/GPq29IRKrcW0b08it7hC6khEZCL2XczBv385BwD4x5OhGBXpJ3EislQsPVQrcrkMi8d1RLCHI7IKKzBzbTxUah7YTEQPdvZ6IV5cnwCtAEZH+mF2v+ZSRyILxtJDteZiZ40vozrB2VaBE6m38M6v5yAEL1VBRPd2/VYZYr6JQ5lKg54tmuCDp9txLR6SFEsP1UmIhxM+Gx8OmQzYcDwda/9MkzoSERmhwrJKRMfG4UaxEi29nbFsQgSsuRYPSYz/AqnO+rT0xGsDwwAA7/56Hn8m50uciIiMiVKtwbS1J3A1twTeLlVr8TjbWUsdi4ilh+pnRq8QDGvfFGqtwMx18cgoKJc6EhEZASEE/vnTGfyRfBNOtgp8Hd0ZTV3tpY5FBIClh+pJJpPhv890QBsfF+SXqjB1zQmUqzRSxyIiiS3adRmbT2XCSi7DsgkRaO3jInUkIh2WHqo3exsrrHg+Eo0dbXA+swiv/3yGBzYTWbANx9OwdP9VAMD8ke3wRKiHxImIamLpoUfi18gByyZEQCGX4bfTmVhxKFnqSEQkgQOXcvH25qq1eGb3bY4xnf0lTkR0N5YeemRdg93xzvDWAIAPd1zE/ou5EiciIkM6n1mIWeviodEKPB3ui3/0D5U6EtE9sfSQXjzXNQDju/hDCGD2dwlIvlEidSQiMoCMgnJMio1DqUqD7iHuWDCqPdfiIaPF0kN6IZPJ8O7f2iIyoBGKK9SYsuYEiioqpY5FRA2oqKISMbFxyC1WItTLCcufi4SNgm8rZLz4r5P0xkYhx/LnIuDtYoekG6X4x3enoNXywGYic6RSazFj7UlcyimGp7MtYid1gas91+Ih48bSQ3rl6WyHlVFVv+3tvZiLj3dfljoSEemZEAJvbDyDo1fz4Whjha+jO8PXjWvxkPFj6SG9a+/nhgVPtwMALN1/FVvPZEmciIj0afGeK9gYnwEruQxLJ0Sgra+r1JGIaoWlhxrE0xF+mPx4EADg1R9P40JmkcSJiEgffjiRjk/3XgEAzBvRFn3CPCVORFR7LD3UYN4Y3BI9WzRBeaUGU9acwM1SldSRiOgRHL5yA29uPAsAmNUnBOO7NJM4EVHdsPRQg1FYybFkfDgC3B2QUVCOWeviUanRSh2LiOohMasIM9bGQ60VeKqjD14dECZ1JKI6Y+mhBuXmYIMvozrB0cYKvyfn4/2tiVJHIqI6yiqsWounRKnGY0GN8dEzXIuHTBNLDzW4UC9nfDy2IwBg9bEU/BCXLm0gIqq14opKTIqNQ3ZRBZp7OmHl851gq7CSOhZRvbD0kEEMbOONl55sAQB4e/M5nEy9JXEiInqYSo0WM9fF42J2MZo42SI2ujNcHbgWD5kulh4ymNl9W2BgGy+oNFpMX3sS2YUVUkciovsQQuCtTWdx+Eoe7K2t8HV0J/g3dpA6FtEjYekhg5HLZVg0piNCvZxwo1iJaWtPoqJSI3UsIrqHpfuu4ocT1yGXAUufDUd7PzepIxE9MpYeMignWwW+jOoEV3trnE4vwFubzkEIXqqCyJhsjL+ORbdXU3/3qbbo18pL4kRE+sHSQwYX4O6Iz5+NgFwG/Bx/HbFHU6SORES3Hbuah3/+fAYAMK1XMJ7vGiBxIiL9YekhSTzeogneHNIKAPD+tkQcvZoncSIiupxTjGlrT6JSIzCsfVP8c2BLqSMR6ZVRlp7AwEDIZLK7brNmzbrn4zdu3Ij+/fvDw8MDLi4u6NatG3bu3Gng1FRXLzwehKcjfKHRCsxaH4+0/DKpIxFZrJyiCkR/fRzFFWp0DmyEhaM7QC7nWjxkXoyy9MTFxSErK0t32717NwBg9OjR93z8oUOH0L9/f2zbtg0nT55Enz59MHz4cCQkJBgyNtWRTCbDByPboYOfKwrKKjFlzQmUKtVSxyKyOKVKNWJWxyGzsALBTRyx8vlOsLPmWjxkfmTCBI4ifemll7BlyxZcuXKl1quAtmnTBmPHjsW///3vWj2+qKgIrq6uKCwshIuLy6PEpTrKLqzA8KVHcKNYicFtvauO9+FvmEQGodZoMXnNCRy4dAPujjbYNLMHmrnz1HQyHXV5/zbKPT13UqlUWLt2LWJiYmpdeLRaLYqLi9G4ceP7PkapVKKoqKjGjaTh7WqHL56LgLWVDNvPZWPp/qtSRyKyCEII/OuX8zhw6QbsrOVYFd2ZhYfMmtGXns2bN6OgoADR0dG1fs6iRYtQWlqKMWPG3Pcx8+fPh6urq+7m7++vh7RUX5EBjTFvRFsAwMe7L2PX+WyJExGZv+UHk7DheBpkMuCzceHo6O8mdSSiBmX0H28NHDgQNjY2+O2332r1+A0bNmDy5Mn45Zdf8OSTT973cUqlEkqlUvd1UVER/P39+fGWxP79yzms+T0VjjZW2DSrB0K9nKWORGSWfjmVgTnfnQIA/Gd4a0T3CJI2EFE9mc3HW6mpqdizZw8mT55cq8d///33eOGFF/DDDz88sPAAgK2tLVxcXGrcSHr/GtYajwU1RqlKg6lrTqCwrFLqSERm54/kfLz2Y9VaPJMfD2LhIYth1KUnNjYWnp6eGDp06EMfu2HDBkRHR2P9+vW1ejwZJ2srOZZNiICvmz1S8svw4oZ4qDVaqWMRmY2rucWYuuYEVBotBrf11q2XRWQJjLb0aLVaxMbGYuLEiVAoFDW+N3fuXERFRem+3rBhA6KiorBo0SJ07doV2dnZyM7ORmFhoaFjkx64O9liZVQk7KzlOHwlDx/tvCR1JCKzkFtcgejYOBRVqBHRzA2fjO3IMyXJohht6dmzZw/S0tIQExNz1/eysrKQlpam+3rFihVQq9WYNWsWmjZtqrvNmTPHkJFJj9r4uGLh6A4AgJWHkrE5IUPiRESmrUylxgurT+D6rXIEujvgq4mduRYPWRyjP5DZULhOj3H6aMdFLDuQBFuFHD9N7452fq5SRyIyOWqNFtO+PYm9F3PR2NEGG2d0R2ATR6ljEemF2RzITPTKgDD0bekJpVqLqd+ewI1i5cOfREQ6Qgi8+9sF7L2YC1uFHF9GdWLhIYvF0kNGzUouw+JxHRHs4YiswgrMWHsSKjUPbCaqrS8PJ+PbP1IhkwGLx3ZEZEAjqSMRSYalh4yei501vozqBGdbBU6k3sJ/fjsvdSQik7DlTCY+2HYRAPDWkFYY3K6pxImIpMXSQyYhxMMJn40Ph0wGrP8zDWv/SJU6EpFRi0u5iZd/OA0AiO4eiBce51o8RCw9ZDL6tPTEawPDAAD/+fU8/kzOlzgRkXFKulGCKWtOQKXWYkBrL/xrWOtaX7uQyJyx9JBJmdErBMPaN4VaKzBzXTwyCsqljkRkVPJKlJgUG4eCskp08HfDp+PCYcW1eIgAsPSQiZHJZPjomfZo3dQF+aUqTF1zAuUqjdSxiIxCuUqDF745gbSbZWjW2AGrJnaCvQ3X4iGqxtJDJsfBRoGVUZFo7GiD85lF+OfPZ8DlpsjSabQCc75LwOn0Arg5WCN2Umc0cbKVOhaRUWHpIZPk18gByyZEQCGX4dfTmVhxKFnqSESSem/LBey6kAOb22vxhHg4SR2JyOiw9JDJ6hrsjneGtwYAfLjjIg5cypU4EZE0Vh25htXHUgAAH4/pgM6BjaUNRGSkWHrIpD3XNQDjOvtDCODvGxKQfKNE6khEBrX9bBbmbb0AAHhzSEsMa+8jcSIi48XSQyZNJpPh3afaIDKgEYor1Jiy5gTOZRRKHYvIIE6m3sJL35+CEMDzXQMwpWew1JGIjBpLD5k8W4UVlj8XAW8XOyTdKMWwJUfw/Ko/8XtSPg9wJrN1La8Uk7+Jg1KtxZOtPPHOcK7FQ/QwLD1kFjyd7fDDtG4Y0dEHVnIZDl/Jw/gv/8DTy49h1/lsaLUsP2Q+bpaqMCn2OG6VVaK9nys+Gx8OhRU350QPIxP8VRhA3S5NT8YtLb8MKw8n4YcT13UXJw31csL0XiEY3sEH1nxzIBNWUanBs1/+gfi0Avg1ssfGmd3h6WwndSwiydTl/Zul5zaWHvNzo1iJr49ew9rfU1GsVAMAfN3sMfWJYIzt7A87ay7aRqZFqxWYtT4e289lw8VOgY0zu6O5p7PUsYgkxdJTDyw95quoohLf/p6K2KPXkFeiAgC4O9og5vEgPNc1AK721hInJKqdeVsu4Ksj12BjJce3L3TBY8HuUkcikhxLTz2w9Ji/ikoNfjyRjhWHknH9VtU1u5xsFZjQtRleeDyIHxGQUVt99Br+81vVqemfjuuIpzr6SpyIyDiw9NQDS4/lqNRoseVMJpYfSMLlnKp1fWwUcoyO9MO0J0LQzN1B4oRENe06n41pa09CCOD1QWGY2bu51JGIjAZLTz2w9FgerVZg38VcLDtwFfFpBQAAuQwY3sEH03uFoFVT/jsg6Z1KL8C4lb+jolKL8V2a4YORbXlqOtEdWHrqgaXHcgkh8Oe1m1h2IAmHLt/Q3d+3pSdm9g5BJy7pTxJJyy/DyGVHkV+qQp8wD3wZ1YmnphP9BUtPPbD0EACcyyjE8oNJ2HY2C9U/GV0CG2NGnxD0DvXgb9hkEBqtwNGrefjPr+eRnFeKNj4u+GFaNzjaKqSORmR0WHrqgaWH7nQtrxQrDibh5/jrqNRU/Yi0auqCGb1DMKStN3/bJr0TQuB8ZhE2J2Tgl9OZuFGsBFC1zMLGmd3h5cID7YnuhaWnHlh66F6yCyuw6kgy1v2ZhjKVBgAQ4O6AqU8EY1SEH9f6oUeWUVCOX05lYFN8Bq7k/u+CuY0crDGsvQ+m9QqGXyMeXE90Pyw99cDSQw9SUKbCN8dSsfrYNdwqqwQAeDrb4oXHgzChawCc+LED1UFheSV2nMvCpoQM/JF8U3e/jUKO/q28MCLcF71CPWCj4B5Foodh6akHlh6qjTKVGt8dT8eXh5ORVVgBAHCxU2Bi90BEdw+Eu5OtxAnJWKnUWhy8fAObEzKwOzFHd4kUAOga3BhPh/thUDtvuNhxsUyiumDpqQeWHqoLlVqLzacy8MXBJCTfKAUA2FnLMa5zM0x5Ihi+bvYSJyRjIIRAfFoBNiVcx5YzWSi4vZcQAFp4OmFkhC+e6ujLfy9Ej4Clpx5Yeqg+NFqB3ReysexAEs5cLwQAKOQyPNXRF9N7BaOFF6+LZImu5ZViU0IGNidkIO1mme5+D2dbPNXBByMjfNG6qQvPBiTSA5aeemDpoUchhMDRq/lYduAqjiXl6+4f0NoLM/s0R0d/N+nCkUHklyix9WwWNsZn4FR6ge5+BxsrDGrjjZERvuge0gRWchYdIn1i6akHlh7Sl1PpBVh+4Cp2ns/R3dc9xB0zezdHj+bu/O3ejFRUarAnMQeb4jNw8PINqLVVm1MruQw9WzTByHBf9G/tBQcbHuhO1FBYeuqBpYf07WpuMZYfSMYvpzJ0b4bt/Vwxo1cIBrbxhpy/8ZskrVbgj+R8bErIwPZz2ShRqnXfa+fripHhvhjewQcezjyoncgQWHrqgaWHGkpGQTm+PJSM7+LSUFFZdcZOsIcjpvcKwYiOvjwt2URcyi7GxoTr+PVUpu7MPaBq8cCR4b4YEe6D5p48hovI0Fh66oGlhxpafokSq4+l4JtjKSiqqNo70NTVDpN7BmN8F39+BGKEcooqqhYOTMhEYlaR7n4XOwWGtvfByHBfdApoxL12RBJi6akHlh4ylOKKSmw4noavDl9D7u1LDTRysEZ09yBM7B4ANwcbiRNathKlGjvOZWNzQgaOJuXprsFmbSVD35aeGBnui95hnlyNm8hIsPTUA0sPGVpFpQYb4zOw4lASUvOrTmt2sLHCs12aYXLPYHi78lpLhlKp0eLIlTxsSsjArgvZuo8hAaBzYCOMCPfF0HZNWUiJjBBLTz2w9JBU1Bottp3LxvIDSbqPUGys5Hg6whfTeoUgqImjxAnNkxACZ64XYlNCBn47nYn8UpXue8FNHDEyvGrhwGbuvO4VkTFj6akHlh6SmhACBy7fwPL9STieUnU9JpkMGNK2KWb0DkFbX1eJE5qH9Jtl2JyQgU2nMnSraQOAu6MNhneoOk6nvZ8rlxYgMhEsPfXA0kPG5ETKTSw/kIS9F3N19z0R6oGZvUPwWFBjviHXUUGZClvPZmFzQgbiUm7p7rezlmNAa2+MDPfF4y2awNqKZ9IRmRqWnnpg6SFjlJhVhC8OJuG305m4vdQPIpq5YUbv5ujX0pNnDT2AUq3B/ou52JSQgf0Xb0ClqTpORyYDeoQ0wYhwXwxq6w0nW541R2TKWHrqgaWHjFlafhlWHk7CDyeu667OHerlhBm9QzC8vQ8U3EMBoGrhwBOpt7ApIQNbz2TqlgYAgFZNXTAy3Ad/6+DLg8SJzAhLTz2w9JApyC2uwNdHUrD2j1TdSsB+jewx7YlgjO7kb7GnUV/NLcHmhAxsPpWB67fKdfd7u9jhqfCq43RaevPnmsgcsfTUA0sPmZLC8kqs/SMVXx+5pjvrqImTDSb1CMLz3QLgYmctccKGd6NYid9OZ2JTQgbOZhTq7neyVWBw26oLfD4W5M4LfBKZOZaeemDpIVNUUanBDyfSseJgMjIKqvZwONsqMKFrAF54PMjsrv9UplJj94UcbIzPwJGredDcPtBJIZehV6gHRkb44slWXha7x4vIErH01ANLD5mySo0Wv53OxPIDSbiSWwIAsFHIMaaTH6Y9EQL/xqa71oxGK3AsKQ+b4jOw43w2ylQa3fc6+rvh6YiqhQPdncyr4BFR7bD01ANLD5kDrVZg78VcLDtwFQlpBQAAK7kMw9s3xYzezRHmbRoXxBRC4EJWETbFZ+DX05m6y3UAQIC7A0Z09MWIcF8u3EhELD31wdJD5kQIgT+Sb2LZgas4fCVPd3+/lp6Y2ScEkQGNJUx3f5kF5dh8KgObEzJwOadEd7+bgzWGt/fBiHBfRDRz4zpFRKTD0lMPLD1krs5lFGL5gSRsO5elu3hml6DGmNE7BL1DPSQvEEUVldh+NgubEjLw57Wbuow2CjmebOWJkeF+6BXqARsFT8snorux9NQDSw+Zu+QbJVh5KBk/x19Hpabqx751UxfM6B2CIe2aGvQsJ5Vai4OXb2BzQgZ2J+bo1h4CgK7BjTEy3BeD2jaFq735n4VGRI+GpaceWHrIUmQXVuCrw8lYfzxNd1BwoLsDpvUKwdMRvrBVNMyZT0IIxKcVYHNCBracycStskrd91p4OmFkRNUFPn3d7Bvk/09E5omlpx5YesjS3CpV4ZvfU7D6WAoKbhcQT2dbTO4ZhGcfC9Db5Rmu5ZXqFg5MzS/T3e/hbIunOlQdp9PGx0Xyj9mIyDSx9NQDSw9ZqjKVGhuOp+Orw8nIKqwAALjaW2NitwBE9whCY0ebOv+ZN0tV2HKmauHA6rPIAMDBxgqD2nhjRLgvejRvwoUDieiRsfTUA0sPWTqVWovNCRn44mASkvNKAVRdhXxc52aY8kTwQz92qqjUYE9iDjYnZODApRtQ3144UC4DerbwwMhwXwxo4wUHG17gk4j0h6WnHlh6iKpotAK7zmdj2YEk3eUdFHIZRoT7YnqvEDT3dNI9VqsV+ONaPjYnZGD72WwUK/93gc92vq4YEe6L4R2awtOZF/gkoobB0lMPLD1ENQkhcORqHpYfSMKxpHwAgEwGDGztjTGd/XD82i38cipD95EYAPi62WPE7Qt8Nvc0jYUQici0sfTUA0sP0f0lpN3C8gNJ2HUh567vOdspMKx9U4zo6IvOgY0h53E6RGRAdXn/5ofrRPRQ4c0aYWVUJ1zJKcYXB5Nx6MoNhPu7YWS4L/q09OQFPonIJBjlEqeBgYGQyWR33WbNmnXf5xw8eBCRkZGws7NDcHAwvvjiCwMmJrIMLbycsWhMB8S99SRWRnXC4HZNWXiIyGQYZemJi4tDVlaW7rZ7924AwOjRo+/5+GvXrmHIkCHo2bMnEhIS8Oabb2L27Nn4+eefDRmbiIiIjJhRfrzl4eFR4+sFCxYgJCQEvXr1uufjv/jiCzRr1gyLFy8GALRq1QonTpzAwoULMWrUqIaOS0RERCbAKPf03EmlUmHt2rWIiYm574qtv//+OwYMGFDjvoEDB+LEiROorKy853OUSiWKiopq3IiIiMh8GX3p2bx5MwoKChAdHX3fx2RnZ8PLy6vGfV5eXlCr1cjLy7vnc+bPnw9XV1fdzd/fX5+xiYiIyMgYfelZtWoVBg8eDB8fnwc+7q97garPxL/f3qG5c+eisLBQd0tPT9dPYCIiIjJKRnlMT7XU1FTs2bMHGzdufODjvL29kZ2dXeO+3NxcKBQKuLu73/M5tra2sLW11VtWIiIiMm5GvacnNjYWnp6eGDp06AMf161bN90ZXtV27dqFTp06wdrauiEjEhERkYkw2tKj1WoRGxuLiRMnQqGouUNq7ty5iIqK0n09ffp0pKam4uWXX0ZiYiK+/vprrFq1Cq+++qqhYxMREZGRMtrSs2fPHqSlpSEmJuau72VlZSEtLU33dVBQELZt24YDBw6gY8eOeO+99/DZZ5/xdHUiIiLS4bW3buO1t4iIiExPXd6/jXZPDxEREZE+sfQQERGRRTDqU9YNqfpTPq7MTEREZDqq37drc7QOS89txcXFAMCVmYmIiExQcXExXF1dH/gYHsh8m1arRWZmJpydne+7inNdFBUVwd/fH+np6RZ3YDRn5+yWNrshWfrrbMnzc/Z7zy6EQHFxMXx8fCCXP/ioHe7puU0ul8PPz0/vf66Li4vF/eOsxtk5OzUcS3+dLXl+zn737A/bw1ONBzITERGRRWDpISIiIovA0tNAbG1t8c4771jkRU05O2enhmPpr7Mlz8/ZH312HshMREREFoF7eoiIiMgisPQQERGRRWDpISIiIovA0kNEREQWgaWHiIiILAJLD9VZSUmJ1BEkEx8fr7tOGxHpF7ct3LY0NJaeOsrJycHWrVtrdTVXc5OVlYUJEyZg/PjxeOGFFxAfHy91JIPJzMzEgAED0KdPH5w6dUrqOAaVnZ2N//u//8OyZcuwbds2qeOYLW5buG3htqXhsfTUwdKlS+Hj44Phw4fj/PnzUscxqLVr16Jt27ZQqVQYPHgw9u3bhw8//BDZ2dlSR2twr7/+OgICAuDg4IDExET07NlT6kgG895776F58+Y4fvw4Vq9ejZEjR2L9+vUAYJFvzg2F2xZuW7htMdC2RdBDabVasXXrVtGvXz+xcOFCERERIZ555hmh0WikjmYQarVa9O/fX8ybN0933/fffy+8vLxEQUGBhMkalkqlEi+++KKQyWTiu+++092fk5MjYSrDUKvVYsGCBaJr165i69atQgghioqKxBtvvCGaNWsmcTrzwW0Lty3cthh228I9PbUgk8ng5eWF559/HtOmTcMnn3yCn3/+GTt37pQ6mkGcOXMGycnJ8PHx0d1XVlaGUaNGobCwUMJkDUcIAWtra/Ts2RNPPPEE8vLycPHiRYwcORKjRo1Cr169sHLlSqhUKqmjNggrKyuoVCr07dsXgwYNAgA4OzujV69eUCgUSEpKkjiheeC2hdsWblsMvG1p8FplggoLC8Xvv/8url+/ft/HjBkzRoSHh4uioiIDJmt495pdqVSKLl26iB49eogVK1aI0aNHC5lMJrp16yZcXV3FnDlzzOI3FKVSKcrKyoQQVb+NCCFEZWWlePHFF4W3t7dwd3cXc+bMEZ9++qmYOnWqsLW1FYsWLdI9x5SVlJSIy5cvi8LCQt19d86l1WqFEEKsW7dOtG7d2uD5zAW3Ldy2cNsi7baFpecvPvjgA+Hi4iLatm0rXFxcxOLFi3U/pGq1WrfbOSkpSdjb24vPPvtMyrh6da/ZU1JShBBCHD58WHz66adiyJAhIiIiQsTFxYnCwkIRGxsrunXrJt555x1pwz+iBQsWiNDQULFjxw7dfdUbp99//11MnDhR/PrrrzWeM3v2bNGhQwdx9uxZg2bVt//7v/8TQUFBomPHjiIoKEi3y7nanR+1vPjii2LChAlCiKpd9FR73LZw21KN25YqUmxbWHrusG3bNtGqVSuxadMmkZycLN5//33Rpk0bERMTo3tMdSsVQoi3335beHl5ifT0dCGEEKWlpaKkpMTgufXhfrNPmjSpxuP69esnli5dWuO+AQMGiOnTp5vkm2B+fr6YPn26aN++vXBxcRFPP/20uHHjhhCi5t/1mTNnREVFhRDifz+o2dnZQiaTiT///NPwwfUgJSVF/O1vfxNt2rQRW7duFXv37hUTJ04UTZs2FdnZ2TUeW/1327lzZ/Hxxx/X+N6drxPdG7ct3LZw22Ic2xaWnjvMnj1bhIeH17hvyZIlIiwsTKxcuVII8b+GLkTVbruAgAAxe/ZssWbNGvH444+LH374waCZ9aU2s1+9elUEBASIuLg43WNKS0tF165dxQcffGDQvPqSnJwsXn/9dbF161Zx+PBhIZPJxIYNG3Qbn3v90FXft2HDBuHp6SlOnz5t0Mz68t1334knnnhCJCYm1rjfxcXlrt88hRAiLS1NeHh46H5D3759uxg/fry4du2aIeKaNG5buG3htsU4ti0sPbdpNBoxY8YMMW7cOF3rFkKIzMxMMW3aNNGhQwdRXFyse2y1d955R8hkMmFjYyPmzp1r8Nz68LDZ27dvrzu+oEOHDqJXr17i22+/FfHx8WLYsGGiTZs2JvvDqVarRWpqqu7rMWPGiPbt29/3h616o5SYmCgGDBggpkyZYoiYelU9w82bN8WPP/5Y43vZ2dkiLCxM7Nq1667nrV27VvTr10+kp6eLwYMHC4VCIV555RWDZDZl3LZw2yIEty3Gsm1h6RH/+4uaP3++8Pf3v+sf5a+//io6deqk+61EiKrfxGbNmiVkMpl44YUXxK1btwyYWH9qO/vy5cuFEFU/kJGRkSIsLEwEBweLMWPGiJs3bxo6tt5Vvw75+fnC2tpazJ8/v8ZGWoiq3zzfffddER0dLRwcHMSECRNM9mDTv/6WWf1me+HCBeHu7i4uX75812OjoqKETCYT1tbWYujQoSIvL89wgU0Uty3ctnDbYlzbFp6yDkCr1QIAXnrpJRQWFmLdunU1vt+7d2/I5XLk5+fr7svLy4OzszMOHz6Mr776Cm5uboaMrDe1nb2goABCCLRs2RL79u3D1q1bsXfvXnz//fdo1KiRFNEfmbhjASyZTAa1Wo3GjRvjrbfewscff4zExMQaj3VwcECjRo1QUlKCgwcPYu3atXB2dpYieoM5dOgQgoKC0KJFi7sWCFMoFGjbti2OHz+OLVu2wN3dXaKUpoPbFm5buG2pYjTblgarU0YkMzNTHDt27J67FSsrK2t8vXDhQuHs7Fzjs2UhhOjYsaOYOXNmQ8ZsEJz94bNXH0tx5zEVvr6+YurUqeLmzZti586dYvXq1UIIYTKLxtV19urfuCZMmCBefvll3ffPnDkjzpw5I4QQuo9g6H/y8/N1B6f+9d+Guf98cfaHz26O25a6zm5s2xazLz1z5swR7u7uonPnzsLBwUF8/vnnd630qdVqxeuvvy6+/fZbIYQQkZGRol+/frrT606ePCk6dOhwz88ijRlnr93sa9eu1f3wVv+gbty4UVhZWYl27doJmUwmPv/8c4PPUF/1mV2r1YqCggLRsmVLsXPnTpGZmalbM2XLli0STWLc3nzzTeHh4SHef//9+z7GXH++OHvtZje3bUt9Zje2bYvZlp7U1FQxfPhw0a1bN3H06FFx7do18eqrr4q2bdvW+CFbvXq1aNy4sejYsaOIj48XQlR99jhy5EhhY2MjBgwYIBwcHMS4ceNEaWmpVOPUCWev2+zVv21Uu379unj99deFTCYT48aN0502bOwedfa4uDjh7e0tpk+fLuzt7cWAAQNqHIhJVW7duiViYmJEly5dRGRkpBg+fLhuD8adxzOY488XZ6/b7OaybXnU2Y1p22K2pWfr1q1i7Nix4sSJEzXu9/b2Ft9//70QomqX2nvvvSeWL19+1664wsJCsWvXLrF06VJx5MgRw4Z/RJy97rNXUyqV4qWXXhKNGzcW+/fvN1RsvXjU2ZcsWSJkMpno0qWLyf323dDu3LBXVFSId999V2zatEkcOHBAREREiLlz59ZYS6a0tNRsfr44e5X6zl7N1LYt+pzdmLYtZlN6qv+Cqj9TzMjIEEePHtV9X6PRCJVKJSIjI8W6detq3G/qOLt+Z//r4lnGSl+zV/85RUVFIjY2toFTm56ysrIaZ9tU766v9sorr4gePXo8cLVZU8XZ9Tu7qWxb9DW7MW5bzKL0LFq0qMZVeu93ylxqaqpwcnIy2XUf7oWzc3YhHn12rqp8b2+88YaIiIgQTz75pPj000911w/SarU1XuMePXqIKVOm3HPFXVPF2Tm7PmY3ttfEpEvP8ePHRe/evYVMJhMRERHi2LFjQoj7N+0NGzaIdu3a3fP7xvYX8zCcnbNb2uyGpFQqxTPPPCNat24tvvvuOxEVFSVat24thg4dWuNx1a/r4sWLRWRkZI3fZqtfX1Pb68HZObs5z27S6/Ts3LkTTZo0wddff637LwDI5fK71gEAgJMnT6Jbt26Qy6vG3r9/P3777TcAVWspmBLOztktbXZDSkpKwunTp7F48WKMHTsW33zzDVauXIl9+/bhv//9712v9fTp0+Hl5YXt27fj7NmzWLduHT744AMA0L32poKzc3aznl26vlV/1W0yNTVV95vu/PnzxWOPPaa7Ps1fm6ZarRbh4eHi+++/F8nJyaJv377CxsZGd4CnqeDsnN3SZpfCyZMnhUwmE/n5+UKImqsLN2rUqMaqstWv++bNm0VwcLBwd3cXNjY2YuHChYYPrgecnbMLYb6zm2TpuZekpCQxYsQIMWLECN3S5Xe+CZw+fVo4OzvrrusxduxYk13m+684O2e3tNkbWkJCgmjTpo1YsmSJEOJ/bwAqlUoEBQXprgtUfZbK1atXdUvpz5gxw2SviC4EZ+fs5j27Ee+Dqj0hBIKDgzF8+HBkZWVh9erVAGruYrty5QpKSkqgVCoRFxeH7777ziyW+ebsnN3SZjeEgIAAtGjRAkeOHEFWVpbuUgLW1tZ48cUXsWHDBmi1WlhZWQEAVqxYgX379uH06dNYtmwZHB0dJZ6g/jg7Zzfr2aXtXPeXk5MjcnNzhVKpFELUXMb7r0ucV39dWFgoJk2aJPr27avbFVe9gFJ6errYt2+fIaI/Ms7O2YWwrNkN6cqVK/ddK+TO13nVqlWiQ4cOYvHixTUe89VXX4k2bdqIlJQU3Z41Yz5w806cnbP/lbnP/ldGV3pUKpWYNm2aCAsLE4899pgYOHCgbr2AOxdCUqlUumuWCPG/v4Bt27aJ3r17iwkTJoi+ffsKmUxmMleD5uyc3dJmN7TTp08LmUwmmjRpIlJSUnT337kBLy8vFxs2bBBCCDFx4kTRrVu3GuXxP//5j+jdu7fhQusJZ+fsljb7vRhV6fnxxx9FSEiI6NWrl9i3b59YuXKlCA4OvuuCdJ9++qlo3LixGDVqlO5YhmqpqakiJCREt8y3qSwGxdk5u6XNLoW4uDgxaNAg4e3tfc8LXVa/zk899ZQQouoNY8KECcLGxkbMmDFDTJ06VTg7O4vly5cLIUzrtH/OztktbfZ7MarSM2vWLPGvf/2rxu62iRMn1rgy65IlS0RgYKBYt27dXS/+3r17hZOTk+jYseNdS/EbO87O2atZyuxSWLFihRg/frzYu3evUCgU4s8//9R9b+nSpbrX+c7fgrVarfjggw/ElClTxJAhQ2qsem1KODtnt7TZ78UoSk/1i52VlSXS0tJ096ekpIiIiAixcOFC3YteWVl536PE8/LyxPr16xs+sB5xds5uabMb2l8viPjPf/5TCCFEt27dxJAhQ4QQ//sYsays7L7PNUWcvQpnt5zZH0YmxD1WNDOAlStXQiaTITQ0FL169ao+qFq3YNqSJUswZ84c9OjRA1ZWVjhz5gz+/ve/Y+7cubCzs7vrz7vzucaOs3N2S5vdkO71Omu1WsjlcsyZMwdarRZLlixBSkoKQkJCMGDAANy6dQuxsbFo1aqVxOkfDWfn7JY2e50ZumWtX79eeHp6im7duomOHTsKDw8P3TWE7jxbZfXq1eLQoUO61rlu3Tphb29f40AsU8PZObulzW5ID3qdq8+IGzdunNizZ48QouqMFHt7e2FtbS1++uknyXLrA2fn7JY2e30ZtPSsW7dOdOjQQXzxxRdCiKqrQi9ZskQ4OjrqFk376yXpqyUmJgorKyvJL0tfX5ydswthWbMbUm1eZyGqjpd6/vnnRefOnYWHh4d47733hJubm1i0aJFU0R8ZZ+fsQljW7I/CIIsTitufoFVWVuKxxx5DVFQUAMDHxwfh4eHw9fVFYmIiAOgWPvqrzZs3o1+/fnj88ccNEVlvODtnt7TZDakur3N5eTmKioqwbds2dOnSBQkJCXj77bfxxhtv4NVXX0VKSopUY9QLZ+fslja7XjRkozp58qS4deuW7uuCgoK7fqs9deqU8Pb2vus0XCGqTsW9evWqmDx5svDx8dGtUWIKB1px9lu6rzm7ZcxuSPV9nY8fPy7Onz9f43EVFRXio48+MpnF1jj7Ld3XnN0yZtenBik9P/30k/Dz8xMhISGiWbNm4l//+leNtUPufKE//vhj0aNHDyFEzYXYLl++LF555RXh5+cn+vTpIy5dutQQUfWOs3N2S5vdkOr7Olcv9mjKODtnt7TZG4LeS09cXJxo2bKlWLx4sTh9+rRYtmyZ8PDwEDNmzNBdvVWj0ejWJRk5cqSYNWvWXX9OWVmZOHDggEmtD8DZObulzW5I+nqdTRFn5+yWNntD0Vvpqd79vnz5cuHn5ycKCwt131u6dKno2rWreO+993T3aTQaodVqRUhIiNiyZYsQQohLly6JcePG1Vi3xBRwds5uabMbkiW/zpyds1va7A1NbwcyV68Xcu3aNYSGhkKhUOi+Fx0djcjISGzfvh3nz58HUHU16Li4ODg4OCAiIgIvvfQS2rdvj/z8fHh6euorlkFwds5uabMbkiW/zpyds1va7A2t3qVn9+7dmD17Nj799FMcP35cd3+PHj1w7NgxZGdnAwA0Gg0cHR3x1FNPQSaTYdeuXbrHbtu2DefOnUNYWBh2796No0ePYteuXbC1tX2EkRoeZ+fslja7IVny68zZObulzW5wdd01lJmZKYYNGyY8PT3FhAkTRLt27YSrq6vueh7l5eWiZcuWYurUqUKImgdZ9ezZs8YFz+bNmyc8PDzEzz///Kh7rAyCs3N2S5vdkCz5debsnN3SZpdKnUpPaWmpmDhxohg7dqxITk7W3d+5c2cRHR0thKhaaG3NmjVCLpffdUDmhAkTalyePjc391GyGxRn5+yWNrshWfLrzNk5u6XNLqU6fbzl4OAAW1tbREdHIygoCGq1GgAwbNiwGgutjRkzBk899RQmT56MgwcPQgiB7OxsXLlyBc8995zuz/Pw8NDjPquGxdk5u6XNbkiW/Dpzds5uabNLqq4t6c51RaqPMH/uuefElClTatxXXl4uevfuLTw9PcWAAQOEj4+P6Nq1q0kfSc7Zq3B2y5ndkCz5debsVTi75cwuFb1cZf2JJ55ATEwMoqOjIYSAVquFlZUVcnJycObMGcTFxSEwMBDPPvusPnqaUeHsnN3SZjckS36dOTtnt7TZDeJRW1NSUpLw8vISJ06c0N1XfXVXc8fZOXs1S5ndkCz5debsnL2apcxuKPU+ZV3c3kF05MgRODk5ITIyEgDw7rvvYs6cOcjNzdVPKzNCnJ2zW9rshmTJrzNn5+yWNruhKR7+kHurXjzp+PHjGDVqFHbv3o2pU6eirKwM3377rVkviMTZObulzW5Ilvw6c3bObmmzG9yj7CYqLy8XzZs3FzKZTNja2ooFCxY86p4nk8HZObulzW5Ilvw6c3bObmmzG9IjH8jcv39/tGjRAh9//DHs7Oz01cVMAmfn7JY2uyFZ8uvM2Tm7pc1uKI9cejQaDaysrPSVx6Rwds5ODceSX2fOztmpYejllHUiIiIiY6e3q6wTERERGTOWHiIiIrIILD1ERERkEVh6iIiIyCKw9BAREZFFYOkhIiIii8DSQ0RERBaBpYeIiIgsAksPERERWQSWHiIiIrII/w9oTn46TmRM9wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ts.rename('day_len_hr').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2c01cbe-0164-48fd-843e-29de5480a616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimedeltaIndex([          '0 days 00:00:00', '0 days 00:00:00.001557807',\n",
      "                '0 days 00:00:00.003115613', '0 days 00:00:00.004673420',\n",
      "                '0 days 00:00:00.006231226', '0 days 00:00:00.007789033',\n",
      "                '0 days 00:00:00.009346839', '0 days 00:00:00.010904646',\n",
      "                '0 days 00:00:00.012462452', '0 days 00:00:00.014020259'],\n",
      "               dtype='timedelta64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(pd.to_timedelta(1.5578065*np.arange(10), unit='ms'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3f720-c6e6-42cc-853b-7bf7d622e272",
   "metadata": {},
   "source": [
    "If you do have to create datetime, timedelta, or period-type objects from scratch, there are also a few convenience functions, all of which can be labelled (for use as a Series or DataFrame column) with a `name` kwarg:\n",
    "\n",
    "1. `pd.date_range(start, end=None, periods=None, freq=None)`: creates a `DatetimeIndex` array; two of the three kwargs after `start` must be specified. The `periods` kwarg has a different meaning depending on whether the other kwarg specified is `end` or `freq`. If you specify `freq` with `periods`, then `freq` gives the time increment to step forward from `start` and `periods` specifies the number of steps to take. If you specify `end` with `periods`, then `periods` is the number of equal-duration steps to divide the range \\[`start`,`end`\\] into, endpoint inclusive.\n",
    "2. `pd.timedelta_range(start=None, end=None, periods=None, freq=None)`: creates a `TimedeltaIndex` array; exactly 3 of the 4 kwargs I have shown must be specified, but you can combine any 3 to different effect.\n",
    "3. `pd.period_range(start=None, end=None, periods=None, freq=None)`: creates a `PeriodIndex` array; must specify exactly 2 of the `start`, `end`, and `periods` kwargs. If `start` and `end` are specified but not `periods` or `freq`, then `freq` defaults to days, even if `end` is less than 1 day after `start`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c08c105c-4331-40c5-a074-97abda314e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex([          '2020-03-05 00:00:00',\n",
      "               '2020-04-21 03:25:42.857142857',\n",
      "               '2020-06-07 06:51:25.714285714',\n",
      "               '2020-07-24 10:17:08.571428572',\n",
      "               '2020-09-09 13:42:51.428571428',\n",
      "               '2020-10-26 17:08:34.285714284',\n",
      "               '2020-12-12 20:34:17.142857144',\n",
      "                         '2021-01-29 00:00:00',\n",
      "               '2021-03-17 03:25:42.857142856',\n",
      "               '2021-05-03 06:51:25.714285712',\n",
      "               '2021-06-19 10:17:08.571428568',\n",
      "               '2021-08-05 13:42:51.428571424',\n",
      "               '2021-09-21 17:08:34.285714288',\n",
      "               '2021-11-07 20:34:17.142857144',\n",
      "                         '2021-12-25 00:00:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(pd.date_range('2020-03-05',end='2021-12-25',periods=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "548088ae-2e9a-416f-a7b5-612d3e62e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2020-03-05 06:00:00'], dtype='datetime64[ns]', freq='D')\n"
     ]
    }
   ],
   "source": [
    "print(pd.date_range('2020-03-05 06:00:00',end='2020-03-05 18:00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5bd86393-4549-4c3a-83ed-8380331e380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimedeltaIndex(['1 days 00:00:00', '1 days 01:30:00', '1 days 03:00:00',\n",
      "                '1 days 04:30:00', '1 days 06:00:00', '1 days 07:30:00',\n",
      "                '1 days 09:00:00', '1 days 10:30:00', '1 days 12:00:00',\n",
      "                '1 days 13:30:00', '1 days 15:00:00', '1 days 16:30:00',\n",
      "                '1 days 18:00:00', '1 days 19:30:00', '1 days 21:00:00',\n",
      "                '1 days 22:30:00', '2 days 00:00:00', '2 days 01:30:00',\n",
      "                '2 days 03:00:00', '2 days 04:30:00', '2 days 06:00:00',\n",
      "                '2 days 07:30:00', '2 days 09:00:00', '2 days 10:30:00'],\n",
      "               dtype='timedelta64[ns]', freq='90T')\n"
     ]
    }
   ],
   "source": [
    "print(pd.timedelta_range(start='1 day', freq='90min', periods=24)) #can't use 1:30 or 1h30m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e4b9d-c0ff-4b30-b8ff-76f34bfb8f0f",
   "metadata": {},
   "source": [
    "#### Timestamp formatting and precision limits\n",
    "Arguments of `date_range()` and `period_range()` where timestamp strings are expected will also take integers and floats. Even a plain year passed as a *string* will be interpreted as a timestamp, specifically midnight January 1 of that year. *Integers or floats,* however, will be interpreted as nanoseconds (default), or the units specified by `unit`, *from midnight on January 1, 1970*. **Time since 1970-01-01 is called Unix time, and it is typically expressed as an integer or float with units of seconds (traditional), microseconds, or nanoseconds (pandas default)**. Since Julian Dates are also expressed as floats, this means you will have to pass any Julian Dates, which must be in units of days, through `pd.to_datetime(<date>, unit='D', origin='julian')` before working with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5ac2e05f-dec1-4644-9477-da409c6eece5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1677-09-21 00:12:43.145224193 = -9223372036854775807 ns in Unix time, or\n",
      " -0b111111111111111111111111111111111111111111111111111111111111111 in binary.\n",
      "\n",
      "2262-04-11 23:47:16.854775807 = 9223372036854775807 ns in Unix time, or\n",
      " 0b111111111111111111111111111111111111111111111111111111111111111 in binary.\n"
     ]
    }
   ],
   "source": [
    "print(pd.Timestamp.min, '=', pd.Timestamp.min.value,\n",
    "      'ns in Unix time, or\\n', bin(pd.Timestamp.min.value), 'in binary.\\n')\n",
    "print(pd.Timestamp.max, '=', pd.Timestamp.max.value,\n",
    "      'ns in Unix time, or\\n', bin(pd.Timestamp.max.value), 'in binary.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39671da9-148d-4859-83b5-502e788cfe88",
   "metadata": {},
   "source": [
    "Unfortunately, dates can only be represented in Unix time in ns on a 64-bit system between about 1677 and 2262. In other words, if you have to parse dates outside that range but later than 0 CE(/AD), you will have to define the out-of-bounds date with a larger unit (e.g. `'s'`) using `pd.Timestamp(<date>, unit=<unit>)`. **For BC(E) dates, only `np.datetime64()` can parse them as Unix time correctly and in a format that `pd.Timestamp()` can then use.** Be careful to check what units the results are in and make sure the same units are supplied to the `unit` kwarg of `pd.Timestamp()`. If you have an array of out-of-bounds times, `pd.to_datetime()` typically won't be able to handle it regardless of unit, so you would have to apply a user-defined function with at least a couple of conversions through `np.datetime64()` and Unix time, **and** change the unit precision to get workable datetimes. **My advice: if you have to work with years BCE and/or your precision is no better than years, Pandas Time Series functionality probably isn't worth the effort to make it work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "64f2427a-5faf-464f-a344-0c3400482127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0476-09-04 12:00:00\n",
      "-882-01-06 08:00:00\n"
     ]
    }
   ],
   "source": [
    "print(pd.Timestamp('476-09-04T12:00:00')) #fall of Rome\n",
    "print(pd.Timestamp(-90000000000, unit='s')) # 882 BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b147150d-b570-461f-a3ce-b1dade09e6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[D]\n",
      "0     -005-03-21 00:00:00\n",
      "1     -004-03-21 00:00:00\n",
      "2     -003-03-21 00:00:00\n",
      "3     -002-03-21 00:00:00\n",
      "4     -001-03-21 00:00:00\n",
      "5     0000-03-21 00:00:00\n",
      "6     0001-03-21 00:00:00\n",
      "7     0002-03-21 00:00:00\n",
      "8     0003-03-21 00:00:00\n",
      "9     0004-03-21 00:00:00\n",
      "10    0005-03-21 00:00:00\n",
      "11    0006-03-21 00:00:00\n",
      "12    0007-03-21 00:00:00\n",
      "13    0008-03-21 00:00:00\n",
      "14    0009-03-21 00:00:00\n",
      "15    0010-03-21 00:00:00\n",
      "16    0011-03-21 00:00:00\n",
      "17    0012-03-21 00:00:00\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "l=['-00{}-03-21'.format(n) for n in range(1,6)[::-1]]+['{}-03-21'.format(str(n).zfill(4)) for n in range(13)]\n",
    "print(np.datetime64(l[0]).dtype)\n",
    "print(pd.Series(l).apply(lambda x: pd.Timestamp(np.datetime64(x).astype(int), unit='D')))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95218df9-d737-49a7-a661-b55675591a7f",
   "metadata": {},
   "source": [
    "Another note about `pd.date_range()`: if you specify `freq='M'`, and your start date is not the beginning of the month, regardless of the input start or end dates, the output start and end dates will be the last day of each month. If you instead specify 12 periods, the year will be subdivided into 12 down to nanosecond precision. If you want a precision of days and recurrence on a particular date of each month, you'll have to create the dates as strings first.\n",
    "\n",
    "#### Changing times and timezones\n",
    "If you have a DatetimeIndex or PeriodIndex, it is very easy to shift the values by a certain amount of time, or, for example duplicate a span of time indexes to other dates. The method for this is `.shift(periods=1, freq=None)`. This moves a Datetime- or PeriodIndex forward or backward by a number of increments specified by `periods` with a size specified by `freq`. If `freq` is left as None, it is taken from the smallest increment between timesteps, and `periods` defaults to 1 unit of these increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "9c214662-a7e2-4a3e-9331-e0f06069c2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2024-02-05 20:00:00', '2024-02-05 20:15:00',\n",
       "               '2024-02-05 20:30:00', '2024-02-05 20:45:00',\n",
       "               '2024-02-05 21:00:00', '2024-02-05 21:15:00',\n",
       "               '2024-02-05 21:30:00', '2024-02-05 21:45:00',\n",
       "               '2024-02-05 22:00:00', '2024-02-05 22:15:00',\n",
       "               '2024-02-05 22:30:00', '2024-02-05 22:45:00',\n",
       "               '2024-02-05 23:00:00', '2024-02-05 23:15:00',\n",
       "               '2024-02-05 23:30:00', '2024-02-06 20:00:00',\n",
       "               '2024-02-06 20:15:00', '2024-02-06 20:30:00',\n",
       "               '2024-02-06 20:45:00', '2024-02-06 21:00:00',\n",
       "               '2024-02-06 21:15:00', '2024-02-06 21:30:00',\n",
       "               '2024-02-06 21:45:00', '2024-02-06 22:00:00',\n",
       "               '2024-02-06 22:15:00', '2024-02-06 22:30:00',\n",
       "               '2024-02-06 22:45:00', '2024-02-06 23:00:00',\n",
       "               '2024-02-06 23:15:00', '2024-02-06 23:30:00',\n",
       "               '2024-02-07 20:00:00', '2024-02-07 20:15:00',\n",
       "               '2024-02-07 20:30:00', '2024-02-07 20:45:00',\n",
       "               '2024-02-07 21:00:00', '2024-02-07 21:15:00',\n",
       "               '2024-02-07 21:30:00', '2024-02-07 21:45:00',\n",
       "               '2024-02-07 22:00:00', '2024-02-07 22:15:00',\n",
       "               '2024-02-07 22:30:00', '2024-02-07 22:45:00',\n",
       "               '2024-02-07 23:00:00', '2024-02-07 23:15:00',\n",
       "               '2024-02-07 23:30:00'],\n",
       "              dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day1i = pd.date_range('2024-02-05T20:00:00', end='2024-02-05T23:30:00', freq='15min')\n",
    "#day1i.shift(periods=2,freq='1D')\n",
    "day1i.append([day1i.shift(freq='{}D'.format(str(i))) for i in range(1,3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b198bf38-767e-4040-916d-29316ac70bc0",
   "metadata": {},
   "source": [
    "There are 2 important time series methods you need to know if you plan to include time zones:\n",
    "\n",
    "1. `.tz_localize(<TZ>)`: append this to a datetime, DatetimeIndex, PeriodIndex, Series, or DataFrame of datetimes to either **a)** specify a time zone when one is *not* already present, or **b)** if `None` is passed instead of a time zone string, remove all time zone information.\n",
    "2. `.tz_convert(<TZ>)`: append this to a datetime, DatetimeIndex, PeriodIndex, Series, or DataFrame of datetimes to either **a)** convert to the specified time zone, or **b)** if `None` is passed instead of a time zone string, convert to UTC and then remove all time zone information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a4854e2e-b41f-463a-81f3-a09e9392d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2020-03-05 17:00:00+01:00', '2020-05-27 05:00:00+02:00',\n",
      "               '2020-08-17 17:00:00+02:00', '2020-11-08 05:00:00+01:00',\n",
      "               '2021-01-29 17:00:00+01:00', '2021-04-22 05:00:00+02:00',\n",
      "               '2021-07-13 17:00:00+02:00', '2021-10-04 05:00:00+02:00',\n",
      "               '2021-12-25 17:00:00+01:00'],\n",
      "              dtype='datetime64[ns, CET]', freq=None)\n",
      "DatetimeIndex(['2020-03-05 11:00:00-05:00', '2020-05-26 22:00:00-05:00',\n",
      "               '2020-08-17 10:00:00-05:00', '2020-11-07 23:00:00-05:00',\n",
      "               '2021-01-29 11:00:00-05:00', '2021-04-21 22:00:00-05:00',\n",
      "               '2021-07-13 10:00:00-05:00', '2021-10-03 22:00:00-05:00',\n",
      "               '2021-12-25 11:00:00-05:00'],\n",
      "              dtype='datetime64[ns, EST]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "dts = pd.date_range('2020-03-05T17:00:00',end='2021-12-25T17:00:00',\n",
    "              periods=9).tz_localize('CET')\n",
    "print(dts)\n",
    "print(dts.tz_convert('EST'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34dd14-483a-47cb-a805-a7a827e97e6e",
   "metadata": {},
   "source": [
    "#### Resampling\n",
    "Time Series in Pandas inherently come with methods to resample, expand, and shift data to the desired temporal indexing scheme.\n",
    "\n",
    "**Resampling** means taking data from one time series and interpolating to other time steps within the same bounds, whether those time steps are more closely spaced than the original (*upsampling*), more widely spaced (*downsampling*), or merely shifted.\n",
    "\n",
    "To shift or downsample, just call the method `.resample('<unit>')` on your time Series (or DataFrame, as long as indexes are timestamps). **`.resample()` is fundamentally a time-based GroupBy.** That means any built-in method you can call on a GroupBy method can be called on the output of `.resample()`.\n",
    "\n",
    "To upsample, `.resample()` is not enough by itself: you must choose a fill/interpolation method. The most basic method is to use `.resample('<unit>').asfreq()`, but if the chosen upsampled unit does not evenly divide into or align with the original unit, most of the resampled points will be `NaN`. There is also the forward-fill method, `.resample('<unit>').ffill(limit=limit)`, where every data point is propagated forward to intervening sample points either up to the number of points specified by the `limit` kwarg or until the next point in the original series is reached. However, I am willing to bet most of you will want or need genuine interpolation that attempts to match the progression of the original data. For that you have `.resample('<unit>').interpolate(method='linear')`, in which the `method` can be any method string accepted by either `scipy.interpolate.interp1d` or `scipy.interpolate.UnivariateSpline`, plus several others. \n",
    "\n",
    "Let's say you have data collected every month for a year. That would give you 12 data points. If you wanted weekly data (roughly 52 data points) and the data are well-behaved, you could upsample from a monthly frequency to a weekly frequency. Here, I'll do an example using the average monthly high temperatures for my hometown, and since these data are from 2021, I'll pretend these are data collected on the 15th of every month of 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fffac603-3c99-4d44-ade2-f8bde9559f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.Series([18.,20.,24.,27.,30.,32.,33.,33.,31.,27.,23.,20.],\n",
    "               index=[pd.to_datetime('2021-{}-15'.format(str(i).zfill(2)))\n",
    "                                     for i in range(1,13)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96041447-6d2f-4484-b748-85eee340b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-15    18.0\n",
      "2021-02-15    20.0\n",
      "2021-03-15    24.0\n",
      "2021-04-15    27.0\n",
      "2021-05-15    30.0\n",
      "2021-06-15    32.0\n",
      "2021-07-15    33.0\n",
      "2021-08-15    33.0\n",
      "2021-09-15    31.0\n",
      "2021-10-15    27.0\n",
      "2021-11-15    23.0\n",
      "2021-12-15    20.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1bc7c0d-ce45-448b-9ad9-03c3b1b8137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-17     NaN\n",
      "2021-01-24     NaN\n",
      "2021-01-31     NaN\n",
      "2021-02-07     NaN\n",
      "2021-02-14     NaN\n",
      "2021-02-21     NaN\n",
      "2021-02-28     NaN\n",
      "2021-03-07     NaN\n",
      "2021-03-14     NaN\n",
      "2021-03-21     NaN\n",
      "2021-03-28     NaN\n",
      "2021-04-04     NaN\n",
      "2021-04-11     NaN\n",
      "2021-04-18     NaN\n",
      "2021-04-25     NaN\n",
      "2021-05-02     NaN\n",
      "2021-05-09     NaN\n",
      "2021-05-16     NaN\n",
      "2021-05-23     NaN\n",
      "2021-05-30     NaN\n",
      "2021-06-06     NaN\n",
      "2021-06-13     NaN\n",
      "2021-06-20     NaN\n",
      "2021-06-27     NaN\n",
      "2021-07-04     NaN\n",
      "2021-07-11     NaN\n",
      "2021-07-18     NaN\n",
      "2021-07-25     NaN\n",
      "2021-08-01     NaN\n",
      "2021-08-08     NaN\n",
      "2021-08-15    33.0\n",
      "2021-08-22    33.0\n",
      "2021-08-29    33.0\n",
      "2021-09-05    33.0\n",
      "2021-09-12    33.0\n",
      "2021-09-19    33.0\n",
      "2021-09-26    33.0\n",
      "2021-10-03    33.0\n",
      "2021-10-10    33.0\n",
      "2021-10-17    33.0\n",
      "2021-10-24    33.0\n",
      "2021-10-31    33.0\n",
      "2021-11-07    33.0\n",
      "2021-11-14    33.0\n",
      "2021-11-21    33.0\n",
      "2021-11-28    33.0\n",
      "2021-12-05    33.0\n",
      "2021-12-12    33.0\n",
      "2021-12-19    33.0\n",
      "Freq: W-SUN, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(ts.resample('W').interpolate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf7f36-7a75-416c-b90f-a5216dde7eef",
   "metadata": {},
   "source": [
    "Naively you would think `.interpolate()` would be able to make a decent sinusoid out of this after upsampling to weeks, even with just linear interpolation. And it would if every 4th week started on the 15th. Unfortunately, for most of the year, the existing data will not align with the interpolated times; the only overlapping date is 15-Aug.\n",
    "\n",
    "The proper way to fix this would be to convert to day numbers and interpolate with SciPy or Numpy or `df.interpolate()` removed from its time series context. However, if the precision of your upsampled data is low enough for that not to matter, you could instead use the original data to forward fill the next available date in the upsampled time series, and *then* interpolate, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "855b74d0-6bef-476f-aef7-7ca3d8c04a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-17    18.00\n",
      "2021-01-24    18.40\n",
      "2021-01-31    18.80\n",
      "2021-02-07    19.20\n",
      "2021-02-14    19.60\n",
      "2021-02-21    20.00\n",
      "2021-02-28    21.00\n",
      "2021-03-07    22.00\n",
      "2021-03-14    23.00\n",
      "2021-03-21    24.00\n",
      "2021-03-28    24.75\n",
      "2021-04-04    25.50\n",
      "2021-04-11    26.25\n",
      "2021-04-18    27.00\n",
      "2021-04-25    27.75\n",
      "2021-05-02    28.50\n",
      "2021-05-09    29.25\n",
      "2021-05-16    30.00\n",
      "2021-05-23    30.40\n",
      "2021-05-30    30.80\n",
      "2021-06-06    31.20\n",
      "2021-06-13    31.60\n",
      "2021-06-20    32.00\n",
      "2021-06-27    32.25\n",
      "2021-07-04    32.50\n",
      "2021-07-11    32.75\n",
      "2021-07-18    33.00\n",
      "2021-07-25    33.00\n",
      "2021-08-01    33.00\n",
      "2021-08-08    33.00\n",
      "2021-08-15    33.00\n",
      "2021-08-22    33.00\n",
      "2021-08-29    32.50\n",
      "2021-09-05    32.00\n",
      "2021-09-12    31.50\n",
      "2021-09-19    31.00\n",
      "2021-09-26    30.00\n",
      "2021-10-03    29.00\n",
      "2021-10-10    28.00\n",
      "2021-10-17    27.00\n",
      "2021-10-24    26.20\n",
      "2021-10-31    25.40\n",
      "2021-11-07    24.60\n",
      "2021-11-14    23.80\n",
      "2021-11-21    23.00\n",
      "2021-11-28    22.25\n",
      "2021-12-05    21.50\n",
      "2021-12-12    20.75\n",
      "2021-12-19    20.00\n",
      "Freq: W-SUN, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tsr = ts.resample('W').ffill(limit=1).interpolate()\n",
    "print(tsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "004194d5-1885-4140-b9e4-e4daac1c3247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tsr.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9e0a8-3423-4065-b62a-b83d6230ba01",
   "metadata": {},
   "source": [
    "**Be aware that upsampling, like interpolation in general, is usually only meaningful when the original data are smooth and well-sampled.** If the data are stochastic, discontinuous, or excessively noisy, it will be garbage in, garbage out. Data following a well-defined curve may yield decent results even if sparsely sampled, but if the sparse data are clumpy, it's most efficient to avoid upsampling where naive methods would yield long stretches of NaNs.\n",
    "\n",
    "I've tried several other methods to find something more intuitive than the [cookbook method in the documentation](https://pandas.pydata.org/docs/user_guide/timeseries.html#sparse-resampling), but the latter is the only one that seems to work for efficient upsampling. It skips the `.resample()` wrapper and uses the underlying `.groupby()` method directly, at the cost of being able to use some methods that only `.resample()` has among the two, like `.interpolate()`. It also heavily relies on the conversion of `Timestamps` and `Timedeltas` to Unix epochs in nanoseconds for appropriate rounding.\n",
    "\n",
    "To demonstrate in another vaguely realistic example: since the night sky is only visible at, well, night, it's easy to have only partial light curves for variable objects observed from the ground that have periods of a couple days or less (similarly, large fractions of the sky are only visible for a little over half the year, which causes the same issue for variable objects with periods of months to a couple years). So let's say that you got to observe a variable star with a 5-hour pulsation period for 5 weeknights between the hours 20:00 and 23:30, and in those 3-hour blocks, you took data every 15 minutes. Your data might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a9a0689e-8f2f-4345-a635-fa00522bf695",
   "metadata": {},
   "outputs": [],
   "source": [
    "day1i = pd.date_range('2024-02-05T20:00:00', end='2024-02-05T23:30:00', freq='15min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "be66582e-df12-4a80-9b1f-72a3bd9e4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = day1i.append([day1i.shift(freq='{}D'.format(str(i))) for i in range(1,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "9d247261-424f-4370-84dc-53e3384a19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [np.linspace(4.88, 4.7, 15),\n",
    "     np.hstack((np.linspace(4.82, 4.69,7), np.array(4.68),\n",
    "                           np.linspace(4.68, 4.83,7))),\n",
    "     np.linspace(4.66, 4.87, 15),\n",
    "     np.hstack((np.linspace(4.77, 4.89,7), np.array(4.89),\n",
    "                           np.linspace(4.88, 4.76,7))),\n",
    "     np.linspace(4.87, 4.69, 15)]\n",
    "#data loosely based on Delta Scuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "4ee87ba0-2937-4fb6-b606-d9bff724dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fudge=pd.Series(np.concatenate(l),index = inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "55e2530d-9bde-46ed-88c8-edea35181aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024-02-05 20:00:00    4.880000\n",
       "2024-02-05 20:14:00    4.867143\n",
       "2024-02-05 20:30:00    4.854286\n",
       "2024-02-05 20:44:00    4.841429\n",
       "2024-02-05 21:00:00    4.828571\n",
       "                         ...   \n",
       "2024-02-09 22:30:00    4.741429\n",
       "2024-02-09 22:44:00    4.728571\n",
       "2024-02-09 23:00:00    4.715714\n",
       "2024-02-09 23:14:00    4.702857\n",
       "2024-02-09 23:30:00    4.690000\n",
       "Length: 75, dtype: float64"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "def round(t, freq):\n",
    "    freq = to_offset(freq)\n",
    "    td = pd.Timedelta(freq)\n",
    "    return pd.Timestamp((t.value // td.value) * td.value)\n",
    "# What the .value attributes do above:\n",
    "### Timestamp.value = nanoseconds past 01-01-1970 (Unix epoch)\n",
    "### Timedelta.value = time offset in nanoseconds \n",
    "\n",
    "fudge.groupby(partial(round, freq=\"2min\")).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadd11a-fce0-4464-8dfd-4b0f2cadb83b",
   "metadata": {},
   "source": [
    "\n",
    "As a final note, I want to mention a core function called `pd.Grouper()` (note the capitalization!) that can be passed to `df.groupby()` to provide additional instructions. With ordinary grouping on column or row names, it's not needed; for a DataFrame `df` to be grouped based on some column `colA`, `df.groupby(pd.Grouper(key=\"colA\"))` is the same as `df.groupby('colA')`. However, to group by *datetimes or timedeltas in addition* to another key/column/other criterion, `pd.Grouper()` must be used. Essentially this is how you do GroupBy and resampling at the same time. It's a rather niche topic, so I will refer readers to [the official documents](https://pandas.pydata.org/docs/reference/api/pandas.Grouper.html#pandas-grouper) and some helpful extra detail on [stack overflow](https://stackoverflow.com/a/45156301/3934626)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398a510-761b-493c-b8cb-ca7d728b4735",
   "metadata": {},
   "source": [
    "## Quick visualization with the .plot accessor\n",
    "By now you've seen me call `.plot()` on a Series or DataFrame a couple of times. What's that about? Glad you asked! Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca310b-fc62-4378-a6b6-9104fb3fbde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fc90a-5108-41c2-a397-fda94e5aadc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f26e1fef-7be2-4ee4-9e09-3732cc5c7f4d",
   "metadata": {},
   "source": [
    "## Preparing input for Machine Learning (ML)\n",
    "ML programs like TensorFlow and PyTorch are highly compatible with Series/DataFrame inputs, but there are still limitations to the data types that will produce useful results. For example (and this doesn't make much sense given what we actually know about exoplanets, but) let's say you wanted to build a neural network model that predicts the planet type based on a subset of the physical or orbital parameters. The physical and orbital parameters of the existing data are numerical, but Planet Type is a categorical variable with 5 unique values. What we would want to do, then, is turn each of those categories into columns where the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df652548-0664-4e4f-98f2-7c6b0f082964",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f5182-6c75-46b2-bfcc-cc6e2870ea05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f268b-8973-4ff9-abc3-e0b257db4372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
