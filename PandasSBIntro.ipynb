{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce116b0-1ba0-4355-95c9-3480dcca960e",
   "metadata": {},
   "source": [
    "# Introduction to Pandas\n",
    "Pandas is an extensive, powerful library for manipulating and analyzing serial and/or tabular data structures with column and row labels. It comes with a suite of built-in statistical functions, time series construction and analysis tools, grouping and sorting functions, hierarchical indexing functions, and even some built-in plotting methods. The data structures that Pandas is built around are highly mutable, adaptable to many data types, and can be loaded from or saved to a wide variety of text, binary, or database files. Several of Python's machine learning libraries and large language modeling packages are built around pandas data structures, especially SciKitLearn and TensorFlow. [Some accelerated operations are supported using the `bottleneck` and `numexpr` libraries.](https://pandas.pydata.org/docs/user_guide/basics.html#accelerated-operations)\n",
    "\n",
    "In the interest of ensuring you know about the best tools for your data, I should mention that Pandas has a couple of potentially deal-breaking limitations: support for data structures with >2 dimensions is limited and hard to use, and there is little native support for parallelization. If you have N-dimensional data where N>2, [`Xarray` is probably the better choice of software](https://xarray.dev/). Similarly, if speed is enough of an issue that you need to distribute your array processing over multiple CPUs or nodes, including `bottleneck` and `numexpr` may help, but [you might consider `Polars` instead](https://docs.pola.rs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaed61b3-0344-4714-9b62-e0abb0110aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sb \n",
    "###^standard import abbr is sns, after a West Wing character's initials\n",
    "###I use sb so I don't have to remember that. I never saw West Wing.\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib as mpl\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf54ef-79ea-43f4-96c2-e6d018725ed6",
   "metadata": {},
   "source": [
    "### Pandas Object Classes\n",
    "Most data structures you will use in Pandas will be one of the following two object classes:\n",
    "\n",
    "1. `Series`: a 1D array that can optionally have an \"index\" assigned to every entry and a \"name\" for the whole distinct from the variable name in your code. The indexes (and name) need only to be hashable; they don't have to be numeric or even unique. Data can also be of any type.\n",
    "2. `DataFrame`: a 2D array or table of values where every row and column can be assigned a label and/or index; like a `dict` of `Series` stacked column-wise, but with different data selection syntax. Arithmetic operations can be performed along either axis, where axis=0 typically refers to row-wise operations and axis=1 refers to column-wise operations. The data type of each column is determined separately. DataFrames can also be grouped by column or row values with the `groupby()` method, or rearranged with the `pivot()` or `pivot_table()` methods, such that the result is hierarchical, like a 2D projection of higher-dimensional data structures.\n",
    "\n",
    "These are the workhorses of data analysis with Pandas. Other Pandas data structures you may encounter include:\n",
    "\n",
    "3. `Index`: the Pandas datatype of attributes that store the row and column labels of a DataFrame or Series. You could convert a 1D array to an `Index` type object, but more often you will extract `Index` objects from existing Series or DataFrames and, if needed, convert them to NumPy arrays or lists.\n",
    "4. `DataFrameGroupBy` (abbr. GroupBy): a structure returned by the `df.groupby()` method. It may look like the input DataFrame sorted by the values of the column label(s) entered as arguments, but for any basic function or statistical method called on this object, the number of return values will equal the number of unique values or value combinations from the column(s) used to group the DataFrame contents. If two or more labels are used for grouping, the result will be hierarchical, and will have MultiIndexes instead of just Indexes.\n",
    "5. `MultiIndex`: the hierarchical analogue of `Index` for GroupBy objects and other hierarchical DataFrames. Each list item is a tuple instead of a string or scalar value. We will talk about MultiIndexing and hierarchical DataFrames rather late in this course because, in my view, their complexity makes their usage awkward and error-prone. If you need such functionality and don't like the constraints of working strictly in 2D, Xarray might be a better alternative.\n",
    "6. `NumpyExtensionArray`, `DatetimeArray`, and `TimedeltaArray`: to extend NumPy functionality, especially to time series, Pandas includes an `array(data, dtype=dtype)` function that can be used to declare simple arrays or store timestamps or time increments depending on what you enter for the `dtype` kwarg. Most familiar datatypes will produce an array of the `NumpyExtensionArray`, but if you set `dtype` equal to `'datetime64[ns]'` or `'timedelta64[ns]'`, the resulting array will be of `DatetimeArray` or `TimedeltaArray` type, respectively. \n",
    "\n",
    "DataFrames are the primary structure that Pandas is designed to work with, so we'll focus on those. **Functions that work on a DataFrame will also work on a Series unless they explicitly require multiple columns.** For the sake of demonstration, however, I will mock up a couple of example Series and DataFrames.\n",
    "\n",
    "Series construction is simple: just call `pandas.Series()`. It is best if you can provide at least a 1D data array/list, if not also a corresponding list of indexes—for the same reason that allocating and then replacing values in a NumPy array is faster than appending to a list—but it is possible to create an empty Series and fill it later. If you provide the data but no index list, indexes will be assigned automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "fc63b0f4-cede-4c4e-852c-2e25ade27a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    a\n",
      "2    b\n",
      "3    c\n",
      "Name: abc, dtype: object\n"
     ]
    }
   ],
   "source": [
    "ser1 = pd.Series(['a','b','c'], index=[1,2,3], name='abc')\n",
    "print(ser1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ffd101-b814-425d-a864-257cd66ca24d",
   "metadata": {},
   "source": [
    "Note that the left column changes depending on whether we include the index kwarg (default index is 0-based), and that you can optionally assign the series a name.\n",
    "\n",
    "The indexes also do not have to be numeric. They can be letters or even datatime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "526cb1fd-ea19-4aee-84cc-e150d0f34dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    1\n",
      "b    2\n",
      "c    3\n",
      "Name: abc123, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ser2 = pd.Series([1,2,3], index=['a','b','c'], name='abc123')\n",
    "print(ser2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d54e38b5-3e8f-4205-a1d8-e837fc81e792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2023-12-21 08:34:00', '2023-12-31 08:36:00',\n",
      "               '2024-01-11 08:31:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "2023-12-21 08:34:00    7.01583\n",
      "2023-12-31 08:36:00    7.11306\n",
      "2024-01-11 08:31:00    7.46278\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#from datetime import datetime as dttm\n",
    "sunrises= ['2023-12-21 08:34:00',\n",
    "           '2023-12-31 08:36:00',\n",
    "           '2024-01-11 08:31:00']\n",
    "daylens = [7.01583, 7.11306, 7.46278]\n",
    "srt = pd.to_datetime(sunrises,\n",
    "                     format=\"%Y-%m-%d %H:%M:%S\")\n",
    "#much easier to use pandas to convert to datetime objects than\n",
    "# to go through the datetime module - more on this later\n",
    "print(srt)\n",
    "print(pd.Series(daylens,index=srt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d436b4-ca2e-4e5c-95f8-aa11507ae752",
   "metadata": {},
   "source": [
    "Construction of a DataFrame is similar to that of a Series, but now you have the option to specify both the column names and the row indexes. If you leave these kwargs empty, pandas will assign 0-based integer indexes to both. Technically, even the first positional argument, the data, is optional. You can leave it blank and fill it later, although this is not recommended (see sub-section on initializing DataFrames with NumPy vs. pure Pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "e15dba54-9c47-4978-8ba3-31d1915703b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    b    c     d\n",
      "1  0.5  1.0  1.5   2.0\n",
      "2  2.5  3.0  3.5   4.0\n",
      "3  4.5  5.0  5.5   6.0\n",
      "4  6.5  7.0  7.5   8.0\n",
      "5  8.5  9.0  9.5  10.0\n"
     ]
    }
   ],
   "source": [
    "dummy_df = pd.DataFrame(np.linspace(0.5,10,20).reshape(5,4),\n",
    "                        columns=['a','b','c','d'], index=list(range(1,6)))\n",
    "print(dummy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbacbb-54f7-4333-9333-4d7c72c8a319",
   "metadata": {},
   "source": [
    "#### Basic Attributes\n",
    "DataFrames and Series have more than a dozen attributes besides those used to select subsets of the data, and several hundred object methods to transform, aggregate, and broadcast data and functions thereof. We cannot possibly cover all the object methods, but I will do my best to cover the ones I have experience with and/or can demonstrate within the limits of the presentation format.\n",
    "\n",
    "There are a small enough number of attributes that they can be listed here and in the upcoming section on data selection:\n",
    "| Attribute Syntax | Value |\n",
    "| --- | --- |\n",
    "| `df.axes` | nested list of row & column indexes (labels) |\n",
    "| `df.columns` | Index-type, list of column labels (add `.values` to get an array) |\n",
    "| `df.index` | Index-type, list of row indexes (labels; add `.values` to get an array) |\n",
    "| `df.dtypes` | list of datatypes by column |\n",
    "| `df.empty` | boolean, True if df is empty |\n",
    "| `df.ndim` | number of axes (1 for a Series, 2 for a DataFrame) |\n",
    "| `df.shape` | tuple, length of df along each axis |\n",
    "| `df.size` | integer, total number of data entries |\n",
    "| `df.values` | returns df converted to a NumPy array (can also be applied to `.columns` & `.index`) |\n",
    "\n",
    "Here are a few of the most basic attributes in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "372c66aa-a4a6-46ca-a017-3ae64fc1b6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Index([1, 2, 3, 4, 5], dtype='int64'),\n",
       " Index(['a', 'b', 'c', 'd'], dtype='object')]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.axes #returns both row and column labels/indexes in a nested list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "00bbbcab-7ae4-48ff-b5c2-c8ff943de34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (5, 4) 20\n"
     ]
    }
   ],
   "source": [
    "print(dummy_df.ndim, dummy_df.shape, dummy_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7369f990-0567-44ee-81bb-ff6b99e9b73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/F - Series abc contains NaNs: False\n"
     ]
    }
   ],
   "source": [
    "print('T/F - Series', ser1.name, 'contains NaNs:', ser1.hasnans) #Series only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275817f6-cfae-4136-89a0-a56b54d2d750",
   "metadata": {},
   "source": [
    "#### Initializing DataFrames with NumPy vs. Pure Pandas\n",
    "If you have to build a DataFrame from scratch within a Python program (e.g. as the output of a bunch of simulations), allocating and filling a NumPy array and then converting to a DataFrame is usually faster than creating an empty DataFrame and filling that, particularly if the NumPy array is purely comprised of floats. It's up to you to determine the best balance of speed and fool-proofing depending on the method you use to fill your (future) DataFrame, and you may need to test subsets of your data to find the best construction method depending on how you assign entries, rows, or columns to the DataFrame.\n",
    "\n",
    "Let's say you're simulating 100 dust grains in an interstellar UV radiation field over 10000 hours, and the following DataFrame is to keep track of the instantaneous surface temperature of each grain in Kelvin (something vaguely like this could show up in an astrochemistry paper). Let's compare a NumPy-first instantiation to pure Pandas. To make this demo a little more realistically inefficient to construct, I assume that each particle's data comes from a separate data structure and so must be added to the final array 1 column at a time. Note that I'm using one of several time series constructor functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "18167724-a21a-492d-94b5-e2232ea0b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimedeltaIndex(['1 days 00:00:00', '1 days 01:00:00', '1 days 02:00:00',\n",
      "                '1 days 03:00:00', '1 days 04:00:00', '1 days 05:00:00',\n",
      "                '1 days 06:00:00', '1 days 07:00:00', '1 days 08:00:00',\n",
      "                '1 days 09:00:00', '1 days 10:00:00', '1 days 11:00:00',\n",
      "                '1 days 12:00:00', '1 days 13:00:00', '1 days 14:00:00',\n",
      "                '1 days 15:00:00', '1 days 16:00:00', '1 days 17:00:00',\n",
      "                '1 days 18:00:00', '1 days 19:00:00', '1 days 20:00:00',\n",
      "                '1 days 21:00:00', '1 days 22:00:00', '1 days 23:00:00',\n",
      "                '2 days 00:00:00', '2 days 01:00:00', '2 days 02:00:00',\n",
      "                '2 days 03:00:00', '2 days 04:00:00', '2 days 05:00:00'],\n",
      "               dtype='timedelta64[ns]', freq='H')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "partid = np.arange(1,101)\n",
    "ts = pd.timedelta_range(start='1 day',periods=10000, freq='1H')\n",
    "print(ts[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3155fb7c-ab2c-43b4-b219-19d4a8fe65d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        1          2          3          4          5    \\\n",
      "1 days 00:00:00   45.132719  18.706452  23.681400  77.297622  29.968109   \n",
      "1 days 01:00:00   42.414091  15.198667   8.644502  26.829536   4.930699   \n",
      "1 days 02:00:00   87.395789  22.426079  85.479914  17.824291  13.695420   \n",
      "1 days 03:00:00  150.882766  39.727747   4.926684  35.183574   7.326537   \n",
      "1 days 04:00:00   11.864747  40.819665   8.266819  31.063394  20.859097   \n",
      "\n",
      "                       6          7          8          9          10   ...  \\\n",
      "1 days 00:00:00  13.487936  10.816268  27.426948  11.739450  22.612319  ...   \n",
      "1 days 01:00:00  57.313018  86.826926  21.660628   5.241199  19.713096  ...   \n",
      "1 days 02:00:00  43.080175  10.783574  67.239200  64.652637  16.542422  ...   \n",
      "1 days 03:00:00  15.930251   7.082038  76.137979  15.832700  59.018916  ...   \n",
      "1 days 04:00:00  10.197294  24.008260  15.544460   6.295166  13.939077  ...   \n",
      "\n",
      "                        91         92         93         94         95   \\\n",
      "1 days 00:00:00   10.939851   3.291486   9.435333  11.810341   3.315489   \n",
      "1 days 01:00:00  108.537795  55.559438  30.980446  27.835846   8.280404   \n",
      "1 days 02:00:00   59.607540  15.339047  25.697686  22.142811   3.005843   \n",
      "1 days 03:00:00   12.973617  67.507364  20.214948  10.163416  37.390247   \n",
      "1 days 04:00:00    4.470187  12.616930   4.513192  11.804861  14.863749   \n",
      "\n",
      "                       96         97         98         99          100  \n",
      "1 days 00:00:00  16.063714  35.353971  14.638000  67.405534   17.235578  \n",
      "1 days 01:00:00  65.949796  41.274419  33.294003  28.657865   11.447813  \n",
      "1 days 02:00:00  13.625674  14.613779  23.204835  43.946803   14.651313  \n",
      "1 days 03:00:00  12.163849   7.085532  12.231080  48.379254  121.697870  \n",
      "1 days 04:00:00  17.964688  39.006968   5.784223  24.185061   36.215125  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "NumPy-first approach took 28.268 ms\n",
      "[33.277151 31.981383 28.88302  28.679584 28.562468 27.947251 27.84787\n",
      " 28.113462 28.455603 29.401997]\n"
     ]
    }
   ],
   "source": [
    "#Start with NumPy implementation\n",
    "runtimes = np.zeros(100)\n",
    "for i in range(100):\n",
    "    t0 = time.time_ns()\n",
    "    data = np.zeros((len(ts), len(partid)))\n",
    "    for j,v in enumerate(partid):\n",
    "        data[:,j] = 10*np.random.chisquare(3,len(ts))\n",
    "        #this gives fairly believably values for such a quick and dirty demo\n",
    "    dummy_df = pd.DataFrame(data,columns=partid,index=ts)\n",
    "    if i==1:\n",
    "        print(dummy_df.head())\n",
    "    runtimes[i] = (time.time_ns()-t0)\n",
    "    del data #trying to stop NumPy from skewing the results with cached data\n",
    "    del dummy_df\n",
    "    #doesn't seem to help\n",
    "print('NumPy-first approach took {:.3f} ms'.format(np.mean(runtimes)*10**-6))\n",
    "print(runtimes[:10]*10**-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0ad31970-4e23-425e-bf4e-f5c933e32dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       1          2          3          4          5    \\\n",
      "1 days 00:00:00  35.313166  11.996560  62.050499  23.940570  34.210093   \n",
      "1 days 01:00:00  33.586879  23.129506  21.573475   6.287035  26.682972   \n",
      "1 days 02:00:00  24.858662   3.823288  94.689335  40.610323  36.974038   \n",
      "1 days 03:00:00  12.265710  22.941618  21.732078  35.709924  53.274873   \n",
      "1 days 04:00:00   1.740359  27.318770  55.228072  45.982285  28.161902   \n",
      "\n",
      "                       6          7          8          9          10   ...  \\\n",
      "1 days 00:00:00   9.948517   3.366698   2.798905  40.368338  16.360232  ...   \n",
      "1 days 01:00:00  24.520840  35.693240  16.916476  66.463949  55.664016  ...   \n",
      "1 days 02:00:00  69.898572  12.921330  26.019181  30.696951  42.910638  ...   \n",
      "1 days 03:00:00  25.931199  43.234775  11.747854  79.388570  66.596115  ...   \n",
      "1 days 04:00:00  30.965781  20.444539  48.600102  15.508818  27.792224  ...   \n",
      "\n",
      "                       91         92         93         94          95   \\\n",
      "1 days 00:00:00  58.689731  42.315043  47.237271  48.676583   95.429496   \n",
      "1 days 01:00:00  23.903999  26.485876  17.799468  15.116500   73.350708   \n",
      "1 days 02:00:00  11.790501  13.564586  17.529895  29.710533  136.428804   \n",
      "1 days 03:00:00  28.046393  42.743310  16.103425  48.058843   51.853363   \n",
      "1 days 04:00:00  85.993026  48.157024  20.143773  10.863239   48.796472   \n",
      "\n",
      "                       96          97         98         99          100  \n",
      "1 days 00:00:00  17.153895  125.002056  19.904787  61.643117   28.489922  \n",
      "1 days 01:00:00  80.354984    5.244301  45.035488  29.738625   20.350957  \n",
      "1 days 02:00:00   6.821208   60.462630  13.538492  72.474699  109.363960  \n",
      "1 days 03:00:00  59.056258    2.505843  25.548857  39.263672   14.384252  \n",
      "1 days 04:00:00   8.185418   24.642326  11.370654  20.015998   19.529786  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "Pure Pandas approach took 37.713 ms\n",
      "[42.652386 42.760195 38.047364 37.792336 37.9014   38.177575 37.870374\n",
      " 38.344151 39.544501 38.350958]\n"
     ]
    }
   ],
   "source": [
    "#now the pure Pandas approach\n",
    "runtimes = np.zeros(100)\n",
    "for i in range(len(runtimes)):\n",
    "    t0 = time.time_ns()\n",
    "    dummy_df2=pd.DataFrame(columns=partid,index=ts)\n",
    "    for pid in partid:\n",
    "        dummy_df2[pid]=10.*np.random.chisquare(3,len(ts))\n",
    "    if i==1:\n",
    "        print(dummy_df2.head())\n",
    "    runtimes[i] = (time.time_ns()-t0)\n",
    "    del dummy_df2  #try to avoid skewing the results with cached data\n",
    "    #doesn't seem to help\n",
    "print('Pure Pandas approach took {:.3f} ms'.format(np.mean(runtimes)*10**-6))\n",
    "print(runtimes[:10]*10**-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586b49b-a41a-4ca1-a4db-3058c7047cd7",
   "metadata": {},
   "source": [
    "As you can see, the NumPy-first approach can do about 4 iterations of the program in the time it takes the pure-Pandas version to do about 3 iterations. You can also see that the first attempt (or 2) to run either set of calculations usually takes longer than subsequent calculations despite my best efforts to eliminate cached data, though this is more consistently true for the Pandas approach.\n",
    "\n",
    "**Note on converting DataFrames to Numpy.** If you need to convert a DataFrame to a numpy array, you can use `df.to_numpy()`. However, both column and index labels will then be lost. If you used a data column as indexes and want to keep it, you can append the indexes as a column before converting to an array, but remember that NumPy arrays have limited support for mixed data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bef4ee-e52c-4d64-8618-1d20375c4519",
   "metadata": {},
   "source": [
    "### Basic I/O\n",
    "The most used read/write combos are `df = pd.read_csv()`/`df.to_csv()` and `df = pd.read_excel()`/`df.to_excel()`, but Pandas can read and write to a wide variety of text and binary formats, including HDF5 and Python pickle files. Even SQL tables can be loaded or output. [I'll refer you to the documentation for the full list of readers and writers](https://pandas.pydata.org/docs/user_guide/io.html#). Most of these readers also accept a URL as a filepath if you want to pull data from a public online repository like Kaggle.\n",
    "\n",
    "**Most pandas data readers default to assuming the top row is a row of column names**. If you have data where you know this not to be the case, you can either choose a different row by setting the `header` kwarg to a different number or override the column names by setting `columns` equal to a list of your choosing. For row indexes/labels, **the default behavior of most pandas data readers is to assume there is no index column and assign 0-based indexes to every row below the header row.** Often there is a column in the dataset that makes more sense to index rows by, so you can change the default behavior by setting `index_col` equal to index of the column you want to use as row indexes. Remember that columns are indexed left to right starting from 0.\n",
    "\n",
    "The table in the documentation does not include it, but for standard tab-delimited text files, you can use either `pd.read_table(filepath_or_buffer,sep='\\t')` or `pd.read_csv(filepath_or_buffer,sep='\\t')`. Many of the keyword arguments (kwargs) in these and other reader functions are similar to those of `np.gen_from_txt()`, but are much less fussy about text encoding, missing values, and mixed data types. \n",
    "\n",
    "I'll load one of my favorite datasets, the 5250 exoplanet dataset. Those of you who attended my Matplotlib for Publication workshop will remember this from the exercises. (I like it because it's very clean for real data but it's big enough and just realistically flawed/\"ugly\" enough to provide some practice at data cleanup.) Notice that I've kept the default behavior for the column names, but have changed the row indexes to the `name` column, which was the leftmost column in the csv file. It didn't make sense to assign a number to every planet when each planet comes with a more meaningful and unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9050d85b-6805-46ab-92e1-5d2e460cc0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>star_mag</th>\n",
       "      <th>planet_type</th>\n",
       "      <th>discovery_yr</th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>detection_method</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11 Comae Berenices b</th>\n",
       "      <td>304.0</td>\n",
       "      <td>4.72307</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2007</td>\n",
       "      <td>6169.20</td>\n",
       "      <td>12.096</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>0.892539</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11 Ursae Minoris b</th>\n",
       "      <td>409.0</td>\n",
       "      <td>5.01300</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2009</td>\n",
       "      <td>4687.32</td>\n",
       "      <td>12.208</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14 Andromedae b</th>\n",
       "      <td>246.0</td>\n",
       "      <td>5.23133</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2008</td>\n",
       "      <td>1526.40</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.508693</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14 Herculis b</th>\n",
       "      <td>58.0</td>\n",
       "      <td>6.61935</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2002</td>\n",
       "      <td>2588.14</td>\n",
       "      <td>12.544</td>\n",
       "      <td>2.773069</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.37</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16 Cygni B b</th>\n",
       "      <td>69.0</td>\n",
       "      <td>6.21500</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>1996</td>\n",
       "      <td>566.04</td>\n",
       "      <td>13.44</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.68</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      distance  star_mag planet_type  discovery_yr  mass_ME  \\\n",
       "#name                                                                         \n",
       "11 Comae Berenices b     304.0   4.72307   Gas Giant          2007  6169.20   \n",
       "11 Ursae Minoris b       409.0   5.01300   Gas Giant          2009  4687.32   \n",
       "14 Andromedae b          246.0   5.23133   Gas Giant          2008  1526.40   \n",
       "14 Herculis b             58.0   6.61935   Gas Giant          2002  2588.14   \n",
       "16 Cygni B b              69.0   6.21500   Gas Giant          1996   566.04   \n",
       "\n",
       "                     radius_RE  orbital_radius_AU  orbital_period_yr  \\\n",
       "#name                                                                  \n",
       "11 Comae Berenices b    12.096           1.290000           0.892539   \n",
       "11 Ursae Minoris b      12.208           1.530000           1.400000   \n",
       "14 Andromedae b          12.88           0.830000           0.508693   \n",
       "14 Herculis b           12.544           2.773069           4.800000   \n",
       "16 Cygni B b             13.44           1.660000           2.200000   \n",
       "\n",
       "                      eccentricity detection_method  \n",
       "#name                                                \n",
       "11 Comae Berenices b          0.23  Radial Velocity  \n",
       "11 Ursae Minoris b            0.08  Radial Velocity  \n",
       "14 Andromedae b               0.00  Radial Velocity  \n",
       "14 Herculis b                 0.37  Radial Velocity  \n",
       "16 Cygni B b                  0.68  Radial Velocity  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('exoplanets_5250_EarthUnits.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736eaae8-7fbc-4ab1-948b-c50bd0f7fb65",
   "metadata": {},
   "source": [
    "The columns of this data table are, from left to right:\n",
    "- **name**: exoplanet name\n",
    "- **distance**: distance to the planetary system in light years\n",
    "- **star_mag**: apparent brightness of the star as seen from Earth in the astronomical magnitude system (which is admittedly awful)\n",
    "- **planet_type**: values include \"Gas Giant\" (like Jupiter or Saturn), \"Neptune-like\" (sort of a mini gas giant or ice giant), \"Super Earth\" (thought to be rocky but up to a few times larger than Earth), and \"Terrestrial\" (comparable to or smaller than Earth)\n",
    "- **discovery_yr**: year that the discovery of the planet was published\n",
    "- **mass_ME**: mass of the planet in units of Earth masses (1 Earth mass = $5.972 \\times 10^{24}$ kg)\n",
    "- **radius_RE**: radius of the planet in units of Earth radii (1 Earth radius = 6371 km)\n",
    "- **orbital_radius_AU**: the exoplanet's orbital semi-major axis in units of the average distance between the Earth and Sun (1 Astronomical Unit, or AU)\n",
    "- **orbital_period_yr**: the time taken for the exoplanet to orbit its star in units of Earth's orbital period (1 year)\n",
    "- **eccentricity**: measure of the deviation of the exoplanet's orbit from a perfect circle. Values range from 0 to 1 where 0 = perfect circle and 1 indicates a parabolic (just barely unbound) orbit.\n",
    "- **detection_method**: principle method used to detect the planet. See [this educational slideshow by NASA](https://exoplanets.nasa.gov/alien-worlds/ways-to-find-a-planet/) or [this Wikipedia page](https://en.wikipedia.org/wiki/Methods_of_detecting_exoplanets) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9e05b-c9d3-4dd3-908f-ab323044000b",
   "metadata": {},
   "source": [
    "When I loaded the DataFrame above, note that I set `index_col=0`, that is, I told `read_csv()` to use the leftmost column as the indexes of the DataFrame. That means I can now grab the entries of any planet by name, if I know the name of the planet as it is rendered in this database. Without that command, `'#name'` is just another column, and every row is assigned a numerical index in the order in which it appears, starting at 0. The default can be useful for regularly sampled time series, but these data  The default behavior for setting column names is to assume the first row is a row of column labels, so I did not need to do anything else. If your data have column labels on another row, `read_csv()` also has the `header` and `skiprows` kwargs to specify the row with the column names and rows to ignore, respectively. The list of other kwargs for parsing and formatting data, and managing memory while you do it, is very long, so [I will refer you to the onliner documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html#pandas.read_csv) and move on.\n",
    "\n",
    "If for some reason I wanted to, for example, save this dataframe as a plain text file with pipes (|) for separators and commas in place of decimal points (don't do this with regular CSV files!), I would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82364b80-817f-49fd-b3e3-5efba81a0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('exoplanets_5250_EarthUnits.txt', sep='|',\n",
    "          decimal=',', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419c7d5-9d0f-4843-8a76-061dd635ab2c",
   "metadata": {},
   "source": [
    "Note: the index kwarg is `True` by default, but I draw it to your attention because if you allow default indexing and don't want those indexes to be saved, you may want to set that kwarg to `False`.\n",
    "\n",
    "If I wanted to append the table as a new sheet in an existing Excel file, the command would be something like:\n",
    "`df.to_csv('other_file.xlsx', mode='a', sheet_name='Sheet2')`\n",
    "\n",
    "We will go more in depth into I/O with more complex and hierarchical data later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193d500-60b4-4499-b548-5c4e32c13642",
   "metadata": {},
   "source": [
    "### Inspecting Data\n",
    "Recall that the first thing I did after importing was call this function `df.head()`. That's a good way to get an overview of your data without loading the whole thing: it displays the first 5 rows of the table with all column names and row indexes. For tables larger than 2 GB, your first view of the data is likely to be with `df.head()` simply because Excel and other text editors/viewers may refuse to load a file of that size. There is also a `df.tail()` function that outputs the last 5 rows of your data instead. Both `head()` and `tail()` accept an integer argument for the number of rows to return if you want a different number than the default 5. E.g. if you only wanted the last row, you would call `df.tail(1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17df5d54-44b9-4851-9c54-d6b38df571d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>star_mag</th>\n",
       "      <th>planet_type</th>\n",
       "      <th>discovery_yr</th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>detection_method</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>YZ Ceti b</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.074</td>\n",
       "      <td>Terrestrial</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.01634</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YZ Ceti c</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.074</td>\n",
       "      <td>Super Earth</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.02156</td>\n",
       "      <td>0.008487</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YZ Ceti d</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.074</td>\n",
       "      <td>Super Earth</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.02851</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.07</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           distance  star_mag  planet_type  discovery_yr mass_ME radius_RE  \\\n",
       "#name                                                                        \n",
       "YZ Ceti b      12.0    12.074  Terrestrial          2017    0.70     0.913   \n",
       "YZ Ceti c      12.0    12.074  Super Earth          2017    1.14      1.05   \n",
       "YZ Ceti d      12.0    12.074  Super Earth          2017    1.09      1.03   \n",
       "\n",
       "           orbital_radius_AU  orbital_period_yr  eccentricity detection_method  \n",
       "#name                                                                           \n",
       "YZ Ceti b            0.01634           0.005476          0.06  Radial Velocity  \n",
       "YZ Ceti c            0.02156           0.008487          0.00  Radial Velocity  \n",
       "YZ Ceti d            0.02851           0.012868          0.07  Radial Velocity  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fb9f5-6d14-4af3-bfed-9fdef322f7bd",
   "metadata": {},
   "source": [
    "If you have a smaller or more symmetrical table that might be more intuitively reorganized if the row and column orders were switched, you can transpose the table with `df.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ce4c9f8d-7b63-4a4f-a589-c9235b81fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    b    c     d\n",
      "0  0.5  1.0  1.5   2.0\n",
      "1  2.5  3.0  3.5   4.0\n",
      "2  4.5  5.0  5.5   6.0\n",
      "3  6.5  7.0  7.5   8.0\n",
      "4  8.5  9.0  9.5  10.0 \n",
      "\n",
      "      0    1    2    3     4\n",
      "a  0.5  2.5  4.5  6.5   8.5\n",
      "b  1.0  3.0  5.0  7.0   9.0\n",
      "c  1.5  3.5  5.5  7.5   9.5\n",
      "d  2.0  4.0  6.0  8.0  10.0\n"
     ]
    }
   ],
   "source": [
    "dummy = pd.DataFrame(np.linspace(0.5,10,20).reshape(5,4),columns=['a','b','c','d'])\n",
    "print(dummy,'\\n\\n',dummy.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923888c6-4c16-4d66-a735-23e585145637",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Other great tools for getting an overview (and sanity check) of your data are `df.info()` and `df.describe()`. `df.info()` prints the zero-based index, name, count of non-Null data, and the data type of each colum for all columns, and also briefly describes the row-indexing system and the size of the DataFrame in memory. `df.describe()` immediately outputs the count, mean, standard deviation, minimum, maximum, and quartiles of all numeric columns, automatically excluding NaNs. **Note:** integer columns are treated as floats and object-type columns are ignored even if most of the data are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "1b5523e6-5c1e-4232-983c-3604e3868c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>star_mag</th>\n",
       "      <th>discovery_yr</th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "      <th>eccentricity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5233.000000</td>\n",
       "      <td>5089.000000</td>\n",
       "      <td>5250.000000</td>\n",
       "      <td>5227.000000</td>\n",
       "      <td>5233.000000</td>\n",
       "      <td>4961.000000</td>\n",
       "      <td>5.250000e+03</td>\n",
       "      <td>5250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2167.168737</td>\n",
       "      <td>12.683738</td>\n",
       "      <td>2015.732190</td>\n",
       "      <td>460.035267</td>\n",
       "      <td>5.627083</td>\n",
       "      <td>6.962942</td>\n",
       "      <td>4.791509e+02</td>\n",
       "      <td>0.063924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3245.522087</td>\n",
       "      <td>3.107571</td>\n",
       "      <td>4.307336</td>\n",
       "      <td>3761.458727</td>\n",
       "      <td>5.315522</td>\n",
       "      <td>138.673600</td>\n",
       "      <td>1.680445e+04</td>\n",
       "      <td>0.141402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>1992.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.296000</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>2.737850e-04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>389.000000</td>\n",
       "      <td>10.939000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>3.970000</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>1.259411e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1371.000000</td>\n",
       "      <td>13.543000</td>\n",
       "      <td>2016.000000</td>\n",
       "      <td>8.470000</td>\n",
       "      <td>2.732800</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>3.449692e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2779.000000</td>\n",
       "      <td>15.021000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>11.715200</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>1.442163e-01</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27727.000000</td>\n",
       "      <td>44.610000</td>\n",
       "      <td>2023.000000</td>\n",
       "      <td>239136.000000</td>\n",
       "      <td>77.280000</td>\n",
       "      <td>7506.000000</td>\n",
       "      <td>1.101370e+06</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           distance     star_mag  discovery_yr        mass_ME    radius_RE  \\\n",
       "count   5233.000000  5089.000000   5250.000000    5227.000000  5233.000000   \n",
       "mean    2167.168737    12.683738   2015.732190     460.035267     5.627083   \n",
       "std     3245.522087     3.107571      4.307336    3761.458727     5.315522   \n",
       "min        4.000000     0.872000   1992.000000       0.020000     0.296000   \n",
       "25%      389.000000    10.939000   2014.000000       3.970000     1.760000   \n",
       "50%     1371.000000    13.543000   2016.000000       8.470000     2.732800   \n",
       "75%     2779.000000    15.021000   2018.000000     159.000000    11.715200   \n",
       "max    27727.000000    44.610000   2023.000000  239136.000000    77.280000   \n",
       "\n",
       "       orbital_radius_AU  orbital_period_yr  eccentricity  \n",
       "count        4961.000000       5.250000e+03   5250.000000  \n",
       "mean            6.962942       4.791509e+02      0.063924  \n",
       "std           138.673600       1.680445e+04      0.141402  \n",
       "min             0.004400       2.737850e-04      0.000000  \n",
       "25%             0.053000       1.259411e-02      0.000000  \n",
       "50%             0.102800       3.449692e-02      0.000000  \n",
       "75%             0.286000       1.442163e-01      0.060000  \n",
       "max          7506.000000       1.101370e+06      0.950000  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9a06e907-f497-4c08-87c5-7fb50afa3bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5250 entries, 11 Comae Berenices b to YZ Ceti d\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   distance           5233 non-null   float64\n",
      " 1   star_mag           5089 non-null   float64\n",
      " 2   planet_type        5250 non-null   object \n",
      " 3   discovery_yr       5250 non-null   int64  \n",
      " 4   mass_ME            5250 non-null   object \n",
      " 5   radius_RE          5250 non-null   object \n",
      " 6   orbital_radius_AU  4961 non-null   float64\n",
      " 7   orbital_period_yr  5250 non-null   float64\n",
      " 8   eccentricity       5250 non-null   float64\n",
      " 9   detection_method   5250 non-null   object \n",
      "dtypes: float64(5), int64(1), object(4)\n",
      "memory usage: 451.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b963da0-784d-42e1-8155-879276e552f6",
   "metadata": {},
   "source": [
    "The count lists of both descriptive functions immediately tell me that most columns have some missing data, and the data types for `mass_ME` and `radius_RE` are a red flag. The metrics returned by `.describe()` also tell me, given some subject-matter expertise, that at least half of the eccentricity values should be taken as assumed filler values (i.e. with a mountain of salt). In a previous iteration I also saw negative values in that column, which are physically impossible and indicated a need to update or nullify those values.\n",
    "\n",
    "**Memory Usage.** Another stat that you should take with a mountain of salt: the memory usage stat. Since most lecture attendees are HPC users, let's take a closer look with a function that many of you will need if you plan to use Pandas on NAISS resources: `.memory_usage()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "60b2065c-d534-4d74-8128-1fae00d8de9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                174136\n",
       "distance              42000\n",
       "star_mag              42000\n",
       "planet_type           42000\n",
       "discovery_yr          42000\n",
       "mass_ME               42000\n",
       "radius_RE             42000\n",
       "orbital_radius_AU     42000\n",
       "orbital_period_yr     42000\n",
       "eccentricity          42000\n",
       "detection_method      42000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage()#deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928cd81-54d8-4234-82a2-0c3f5de3ecc2",
   "metadata": {},
   "source": [
    "This function returns the size in memory of every column, plus the size of the indexes by default. You can hide the contribution of the row and column labels by setting `index=False`, and you can get an estimate of how much system-level memory the object-type data columns consume by setting `deep=True`. The latter is important to check because object-type columns can be much larger in memory than initially reported. Watch what happens when the command is rerun with `deep=True` uncommented.\n",
    "\n",
    "What's going on? [This article](https://pythonspeed.com/articles/pandas-dataframe-series-memory-usage/) provides a fuller and perhaps better explanation, but the short answer is that numerical and Boolean datatypes have fixed size in memory (e.g. an int64 or float64 number will always be 8 bytes, whether it's 1 or Avogadro's number), whereas object-type data (strings) are variable in size and usually must be stored somewhere else because they're almost always bigger. When memory is allocated for the DataFrame itself, that parcel of memory also contains all of the numerical or Boolean data, but only pointers to the object-type data. The actual values of the object-type data are stored wherever space can be found in memory, which requires significantly more overhead. When `deep=False`, as is the default, *the memory usage reported for object-type data is only what is used by the pointers*.\n",
    "\n",
    "That said, even when `deep=True`, you only get a *worst-case estimate* of the memory usage of object-type data. The actual usage as reported by a dedicated memory profile will typically be somewhat smaller because Python has some built-in string optimization routines that cache frequently used strings, and because the estimate often includes temporary structures that get deallocated. Still, if `.memory_usage()` is your only tool to check the size of your data in memory, the results when `deep=True` will generally be closer to the real value than the default output (and it's better to design around the worst-case scenario)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003b1bf-1a30-4a9c-92e8-29a3c807601e",
   "metadata": {},
   "source": [
    "Back to basics: some of the other returns of `.info()` can also be called individually for the whole DataFrame or individual columns or rows, specifically `.count()` and, as we mentioned in the attributes section, `.dtypes`. Whether you need the count(s) for the whole DataFrame or just one row or column, with the correct data selection syntax, the method is just `.count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2929e524-723a-478a-ae94-7210a8be1112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['55 Cancri e'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c1691-7e01-434e-b917-c4fdf3740b8b",
   "metadata": {},
   "source": [
    "That basically means the row labelled '55 Cancri e' has 10 data entries. If you suspect there are duplicates, you can replace `.count()` with `.nunique()`, which will return a single value for a Series or Index list, and a Series when given a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "e2256151-cc04-443b-9ab0-65097ac02cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of 5250 eccentricity values, only 175 are unique.\n"
     ]
    }
   ],
   "source": [
    "print('Of', df['eccentricity'].count(), 'eccentricity values, only', \n",
    "      df['eccentricity'].nunique(), 'are unique.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b92b9-151f-4dad-8c1c-4ad7888ae230",
   "metadata": {},
   "source": [
    "There is also a `.value_counts()` method that counts every unique row-wise combination of values for however many columns you give it. It is mainly used for GroupBy objects, which we will discuss later. For now, just observe the behavior below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "265b47d6-db6b-4322-a05d-8b653a1689ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection_method\n",
      "Transit                          3945\n",
      "Radial Velocity                  1027\n",
      "Gravitational Microlensing        154\n",
      "Direct Imaging                     62\n",
      "Transit Timing Variations          24\n",
      "Eclipse Timing Variations          17\n",
      "Orbital Brightness Modulation       9\n",
      "Pulsar Timing                       7\n",
      "Astrometry                          2\n",
      "Pulsation Timing Variations         2\n",
      "Disk Kinematics                     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Compare:\n",
      "\n",
      "discovery_yr  detection_method           \n",
      "2016          Transit                        1453\n",
      "2014          Transit                         802\n",
      "2021          Transit                         415\n",
      "2018          Transit                         253\n",
      "2022          Transit                         188\n",
      "                                             ... \n",
      "2013          Astrometry                        1\n",
      "2007          Pulsation Timing Variations       1\n",
      "              Direct Imaging                    1\n",
      "2018          Transit Timing Variations         1\n",
      "              Eclipse Timing Variations         1\n",
      "Name: count, Length: 118, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['detection_method'].value_counts())\n",
    "print('\\nCompare:\\n')\n",
    "print(df[['discovery_yr', 'detection_method']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818457d-462d-4cc4-83ea-52ff53a69d0f",
   "metadata": {},
   "source": [
    "To get the datatypes of a single column, a singular version of the datatypes attribute is used, `df['col_name'].dtype` because column data are expected to all be of the same datatype. For a row, which is expected to have varying data types, the syntax is more like that used for the whole DataFrame: `df.loc['row_name'].dtypes` or `df.iloc['row_index'].dtypes` depending on whether you select the row by a custom label or the 0-based numerical index, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c79199-c6ec-4a3b-9df8-8d009c51a7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance             float64\n",
       "star_mag             float64\n",
       "planet_type           object\n",
       "discovery_yr           int64\n",
       "mass_ME               object\n",
       "radius_RE             object\n",
       "orbital_radius_AU    float64\n",
       "orbital_period_yr    float64\n",
       "eccentricity         float64\n",
       "detection_method      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96b479-89c0-4123-ab35-fb3099197892",
   "metadata": {},
   "source": [
    "As for why the mass and radius columns have type `object` instead of `float64`, let's see what happens if we try to coerce the data of those columns to the expected type (don't worry about the syntax just yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a883f9cf-d2ce-48f9-b744-3cf332e26f95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ' '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[245], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmass_ME\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:6534\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6530\u001b[0m     results \u001b[38;5;241m=\u001b[39m [ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   6532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6533\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6534\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   6535\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:414\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    412\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    416\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    417\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    418\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    419\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[1;32m    420\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py:616\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    614\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 616\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m    618\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    620\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:238\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:183\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:134\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ' '"
     ]
    }
   ],
   "source": [
    "df['mass_ME'].astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae5aba-eeb2-489e-884e-112096513138",
   "metadata": {},
   "source": [
    "Aha. There's at least one whitespace in that column. Pandas was initially designed for economic data where cells might contain dates, currency symbols, addresses, and other types of data where numbers might appear with spaces, so Pandas assumes all whitespace is intentional and marks the column as `object` even if the whitespace is in an otherwise totally numerical column. Pandas will only coerce columns of numbers and missing values to `float64` if there are no non-numeric characters.\n",
    "\n",
    "Prior inspection of the data revealed that the mass and radius columns both had a mix of floats, integers (as a result of rounding to significant digits), missing values, and *cells that look empty but actually contain a space character*. Let's take a quick look at an entry where I know this has happened from looking at the original Excel file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a8d14dfb-97a4-49fa-904e-551b27a8679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance                      1308.0\n",
      "star_mag                      12.994\n",
      "planet_type                Gas Giant\n",
      "discovery_yr                    2014\n",
      "mass_ME                       343.44\n",
      "radius_RE                           \n",
      "orbital_radius_AU                NaN\n",
      "orbital_period_yr                2.2\n",
      "eccentricity                     0.0\n",
      "detection_method     Radial Velocity\n",
      "Name: Kepler-97 c, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.loc['Kepler-97 c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4bee3cee-ffc7-4caa-af8d-39fbba391e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['Kepler-97 c','radius_RE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3664e8c3-6ecf-485d-b424-6159cd6a3c96",
   "metadata": {},
   "source": [
    "If I select the whole row or a subset of the columns, the value in the radius column just looks empty, but if I look at just the *'radius_RE'* column for this planet, it shows the single space in quotes.\n",
    "\n",
    "Pandas devotes a substantial fraction of its functional library to making it easy to deal with malformed data. An issue like this could be fixed at import with the `converters` kwarg of `read_csv()` if you know about it a priori. I held off so I can show you a little later how to find and handle different types of missing data within Python, as you might if your typical choice of file viewer chokes on the size of the data.\n",
    "\n",
    "To get there, we need to dive into how subsets of Pandas DataFrames are selected, so you can understand the use of `df.loc[...]` and `df.iloc[...]` among other selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca36a0-fc5a-475a-a5b0-a1dd5ff0e479",
   "metadata": {},
   "source": [
    "### Accessing Data\n",
    "Access by column(s) and/or row(s) is simple in principle, though it can be error prone if you're used to NumPy arrays and Python dictionaries. The official documentation can be a bit verbose if you're just trying to remember when to use `.loc`, `.iloc`, or neither. The following summary table should help:\n",
    "\n",
    "| To Access... | Syntax |\n",
    "| :--- | :--- |\n",
    "| 1 column | `df['col_name']` |\n",
    "| 1 named row | `df.loc['row_name']` |\n",
    "| 1 row by index | `df.iloc[index]` |\n",
    "| 1 column by index (rarely used) | `df.iloc[:,index]` |\n",
    "| subset of columns | `df[['col0', 'col1', 'col2']]` |\n",
    "| subset of named rows | `df.loc[['rowA','rowB','rowC']]` |\n",
    "| subset of rows by index | `df.iloc[i_m:i_n]`  where *i_m* & *i_n* are the m<sup>th</sup> & n<sup>th</sup> integer indexes |\n",
    "| rows & columns by name | `df.loc['row','col']` or `df.loc[['rowA','rowB', ...],['col0', 'col1', ...]]` |\n",
    "| rows & columns by index | `df.iloc[i_m:i_n, j_p:j_q]` where *i* & *j* are row & column indexes, respectively |\n",
    "| columns by name & rows by index | `df[['col0', 'col1', 'col2']].iloc[i_m:i_n]` |\n",
    "\n",
    "**Columns** alone can be selected by name in square brackets (`[]`) like ordinary dictionary entries. **Rows**, with or without columns, must be accessed by adding either `.loc` if selection is by name, or `.iloc` if selection is by index, between the name of the DataFrame and the `[]`. If you need to select both rows and columns, the row and column names must be given in row-major order, as with most other Python array functions: `[row(s), col(s)]`. Also note that `.iloc[]` is endpoint-exclusive like regular Python array slicing operations, while `.loc[]` is endpoint-inclusive. \n",
    "\n",
    "As a reminder, if you just want to view the column labels or row labels/indexes, the the commands are `df.columns` and `df.index`, respectively. Both return Pandas Series of type `Index`, which can be used with any Pandas method that works on Series but *cannot* be directly input into NumPy functions or list comprehension. If you need the output of either command to be a list or an array, add `.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "eedac922-ab2b-4d83-92f5-20f235c0d0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance                       409.0\n",
      "star_mag                       5.013\n",
      "planet_type                Gas Giant\n",
      "discovery_yr                    2009\n",
      "mass_ME                      4687.32\n",
      "radius_RE                     12.208\n",
      "orbital_radius_AU               1.53\n",
      "orbital_period_yr                1.4\n",
      "eccentricity                    0.08\n",
      "detection_method     Radial Velocity\n",
      "Name: 11 Ursae Minoris b, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e1c6cddf-1732-4686-b2e5-6d878818be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                planet_type  mass_ME\n",
      "#name                               \n",
      "51 Eridani b      Gas Giant   636.00\n",
      "51 Pegasi b       Gas Giant   146.28\n",
      "55 Cancri b       Gas Giant   264.13\n",
      "55 Cancri c       Gas Giant    54.51\n",
      "55 Cancri d       Gas Giant  1233.20\n",
      "55 Cancri e     Super Earth     7.99\n",
      "55 Cancri f       Gas Giant    44.84\n",
      "61 Virginis b  Neptune-like     5.10\n",
      "61 Virginis c  Neptune-like    18.20\n",
      "61 Virginis d  Neptune-like    22.90\n"
     ]
    }
   ],
   "source": [
    "print(df[['planet_type','mass_ME']].iloc[25:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d98a8-2d27-45e3-a0e8-1e6680d7c9d5",
   "metadata": {},
   "source": [
    "#### Conditional Data Selection\n",
    "In practice, you will very often need to filter rows (and columns) by conditions rather than names or indexes. Conditional operators (`==`, `>`, `<`, `=>`, `=<`, and `!=`) return boole-type Pandas Series that are used to select and return subsets of the input data for which the condition is `True`. For a single condition, the syntax is fairly intuitive once you've memorized the previous syntax table. To filter by multiple conditions, however, there are a few extra things to remember: \n",
    "1. To combine multiple conditions, you must use the \"bitwise or\" pipe operator `|` and the \"bitwise and\" ampersand operator `&`, instead of the usual `or` or `and`, respectively. The \"bitwise exclusive or\" operator `^` and \"bitwise not\" operator `~` are also available.\n",
    "2. Each condition must be enclosed in parentheses `()` so that all conditions will be evaluated to boole-type Pandas Series that the bitwise operators can safely combine. Typically, forgetting the `()` will show up as a TypeError since conditions are often combined across differently-typed columns, but if you want to understand why errors result from missing `()`, it is helpful to know that the filtering expression `df['A']>2 & df['B']<=5` would be evaluated as `df['A']>(2 & df['B']<=5)`. (Believe me, if I knew why, I would submit a push request to change this behavior.)\n",
    "3. If you want to filter by a list of values, it is better to use the `.isin()` attribute than a bitwise chain of conditions. Syntax: `df['col'].isin([value1, value2, ...])`. Note that this does *not* work the other way around, i.e. you cannot use `.isin()` to check if column, row, or cell entries contain a substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b5146eff-1a08-45a0-bf5b-e396b676d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        distance  star_mag  planet_type  discovery_yr  mass_ME\n",
      "#name                                                                         \n",
      "16 Cygni B b                69.0   6.21500    Gas Giant          1996   566.04\n",
      "47 Ursae Majoris b          45.0   5.03352    Gas Giant          1996   804.54\n",
      "51 Pegasi b                 50.0   5.45309    Gas Giant          1995   146.28\n",
      "55 Cancri b                 41.0   5.95084    Gas Giant          1996   264.13\n",
      "70 Virginis b               58.0   4.96808    Gas Giant          1996  2381.82\n",
      "GJ 876 b                    15.0  10.16000    Gas Giant          1998   723.64\n",
      "HD 168443 b                129.0   6.92122    Gas Giant          1998  2424.15\n",
      "HD 187123 b                150.0   7.83000    Gas Giant          1998   166.31\n",
      "HD 195019 b                123.0   6.87591    Gas Giant          1998  1265.64\n",
      "HD 210277 b                 69.0   6.54348    Gas Giant          1998   410.22\n",
      "HD 217107 b                 65.0   6.15500    Gas Giant          1998   413.40\n",
      "PSR B1257+12 b            1957.0       NaN  Terrestrial          1994     0.02\n",
      "PSR B1257+12 c            1957.0       NaN  Super Earth          1992     4.30\n",
      "PSR B1257+12 d            1957.0       NaN  Super Earth          1992     3.90\n",
      "Rho Coronae Borealis b      57.0   5.40816    Gas Giant          1997   332.28\n",
      "Tau Bootis b                51.0   4.48635    Gas Giant          1996  1892.10\n",
      "Upsilon Andromedae b        44.0   4.09565    Gas Giant          1996   218.66 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[df['discovery_yr'] < 1999].iloc[:, :5],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "576e7b97-6826-4285-b310-d8ebffe21882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#name\n",
      "55 Cancri e              Super Earth\n",
      "GJ 436 b                Neptune-like\n",
      "GJ 581 b                Neptune-like\n",
      "GJ 876 d                Neptune-like\n",
      "HD 160691 d             Neptune-like\n",
      "HD 190360 c             Neptune-like\n",
      "HD 4308 b               Neptune-like\n",
      "HD 49674 b              Neptune-like\n",
      "HD 69830 b              Neptune-like\n",
      "HD 69830 c              Neptune-like\n",
      "HD 69830 d              Neptune-like\n",
      "HD 99492 b              Neptune-like\n",
      "OGLE-2005-BLG-169L b    Neptune-like\n",
      "OGLE-2005-BLG-390L b    Neptune-like\n",
      "PSR B1257+12 b           Terrestrial\n",
      "PSR B1257+12 c           Super Earth\n",
      "PSR B1257+12 d           Super Earth\n",
      "Name: planet_type, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[ (df['discovery_yr'] < 2007) &\n",
    "              (df['planet_type'] != 'Gas Giant'),\n",
    "      'planet_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "dd3c084a-5ab8-475a-a29f-c98458ce49b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#name\n",
      "PSR B1257+12 b    Terrestrial\n",
      "PSR B1257+12 c    Super Earth\n",
      "PSR B1257+12 d    Super Earth\n",
      "Name: planet_type, dtype: object\n",
      "\n",
      "...looks the same as...\n",
      "\n",
      "#name\n",
      "PSR B1257+12 b    Terrestrial\n",
      "PSR B1257+12 c    Super Earth\n",
      "PSR B1257+12 d    Super Earth\n",
      "Name: planet_type, dtype: object\n",
      "\n",
      "...but only use the first version!\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[(df.index.str.contains('PSR')) &\n",
    "             (df['discovery_yr'] < 2000), 'planet_type'])\n",
    "print('\\n...looks the same as...\\n')\n",
    "print(df[(df.index.str.contains('PSR')) &\n",
    "         (df['discovery_yr'] < 2000)]['planet_type'])\n",
    "print(\"\\n...but only use the first version!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec1bdc5-8acb-4b68-a151-5f193e39be1c",
   "metadata": {},
   "source": [
    "Here I'm touching on a rather complex topic: [chain indexing](https://pandas.pydata.org/docs/user_guide/indexing.html#why-does-assignment-fail-when-using-chained-indexing). We don't have time to get into this in depth, but the gist is that if it looks like you can access data either with `df[y][x]` or `df.loc[y,x]`, where *x* is a column or subset of columns and *y* is a row label or row-filtering condition, **you should always prefer the .loc[] format**. The syntax that looks like standard nested list or nested dict selection is called chain indexing, and with DataFrames, it's hard to know whether it will return a copy of the selected data or a view of the original data. It's also not always this obvious when you've used chained indexing, so Pandas is programmed to raise a `SettingWithCopy` warning to help you avoid the frustration.\n",
    "\n",
    "I also want to call your attention to this snippet:\n",
    "`df.index.str.contains('PSR')`\n",
    "Here we took advantage of one of the more brilliant features of Pandas: **string vectorization**. Many string functions can be called on and broadcast to any Pandas Series of type `object` (`string`) or `Index` by adding the attribute `.str` and then a string method of your choice. You can also append `.str.method()`, where `method` is any string method, directly after another method that returns a Series with string-like contents, like the example above. The official documentation contains a helpful [summary table of allowed string operations](https://pandas.pydata.org/docs/user_guide/text.html#method-summary) and detailed discussions of how to use methods with multiple input/output options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39fa88-3611-4099-9d8a-7fdd1f5367aa",
   "metadata": {},
   "source": [
    "#### The .query() method\n",
    "Pandas has a `.query()` method that can make conditional selection simpler and more readable. It works a bit like the built-in `eval()` and `exec()` functions (in fact, it uses these functions under-the-hood) in that it takes your filter conditions as a string, and allows you to use the plain-English versions\\* (`and`, `or` etc.) of the bitwise binary operators discussed in the previous section. It lets you use the word `index` if you don't know the name of the index label you're looking for, and lets you filter either index or column\\*\\* values with the same syntax *as long as all column and index labels are unique.* You can even reference variables within the query statement by prefixing variable names with the `@` symbol.\n",
    "\n",
    "\\*Binary operations are not implemented for some data types. If the array is coerced to `ExtensionArray` type, the plain-English forms of certain binary operators (namely `is` and `is not`) may raise a `NotImplemented` error.\n",
    "\n",
    "\\*\\*Note that you can both select and filter by rows, but can only filter by columns. Column selection must be made via chain indexing, which means the `.query()` method is not suitable for assigning values.\n",
    "\n",
    "Let's rewrite the previous data selection command with the `.query()` function, and let the year vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "70275e56-fb13-49d1-8e50-e9caea2a5bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#name\n",
       "PSR B1257+12 b    Terrestrial\n",
       "PSR B1257+12 c    Super Earth\n",
       "PSR B1257+12 d    Super Earth\n",
       "PSR B1620-26 b      Gas Giant\n",
       "Name: planet_type, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2009\n",
    "df.query(\"index.str.contains('PSR') and \\\n",
    "        `discovery_yr` < @y\")['planet_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea6dc-2f9b-4348-9138-0f77e106b11d",
   "metadata": {},
   "source": [
    "\n",
    "**Very important:** to distinguish column and row names from other strings within the query statement, you must bracket column and row labels with *grave accents* (\\`\\`) instead of single or double quotes.\n",
    "\n",
    "It does not matter whether you use single quotes for strings within the query and double quotes for the whole statement, or vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3e82e-39af-4ecf-b676-1d9703bc0c9e",
   "metadata": {},
   "source": [
    "#### Finding and handling invalid data\n",
    "Pandas provides a couple of convenience functions for selecting only invalid or only valid data from your DataFrame or any subset of it that is at least a Series: `.isna()` and `.notna()`. You may also see `.isnull()` and `.notnull()`, but these are aliases for `.isna()` and `.notna()`, respectively, and the use of the `na` versions is generally preferred over `null`. The `.isna()` method selects both NaNs and None values, but not $\\pm$infinity and, as I mentioned earlier, not whitespaces in otherwise numerical columns. Infinite values and whitespaces-as-placeholders need to be replaced with NaN or None in order to take advantage of Pandas' built-in filters for invalid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6d0e49-9942-4565-b603-b402e9e7934b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>planet_type</th>\n",
       "      <th>discovery_yr</th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CI Tauri b</th>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2019</td>\n",
       "      <td>3688.80</td>\n",
       "      <td>12.432</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoRoT-7 d</th>\n",
       "      <td>Neptune-like</td>\n",
       "      <td>2022</td>\n",
       "      <td>17.14</td>\n",
       "      <td>4.3008</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DS Tucanae A b</th>\n",
       "      <td>Neptune-like</td>\n",
       "      <td>2019</td>\n",
       "      <td>413.40</td>\n",
       "      <td>5.7008</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPIC 201238110 b</th>\n",
       "      <td>Super Earth</td>\n",
       "      <td>2019</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPIC 201427007 b</th>\n",
       "      <td>Super Earth</td>\n",
       "      <td>2021</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   planet_type  discovery_yr  mass_ME radius_RE  \\\n",
       "#name                                                             \n",
       "CI Tauri b           Gas Giant          2019  3688.80    12.432   \n",
       "CoRoT-7 d         Neptune-like          2022    17.14    4.3008   \n",
       "DS Tucanae A b    Neptune-like          2019   413.40    5.7008   \n",
       "EPIC 201238110 b   Super Earth          2019     4.16      1.87   \n",
       "EPIC 201427007 b   Super Earth          2021     2.86       1.5   \n",
       "\n",
       "                  orbital_radius_AU  \n",
       "#name                                \n",
       "CI Tauri b                      NaN  \n",
       "CoRoT-7 d                       NaN  \n",
       "DS Tucanae A b                  NaN  \n",
       "EPIC 201238110 b                NaN  \n",
       "EPIC 201427007 b                NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['orbital_radius_AU'].isna()].iloc[:5,2:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d632d8-11b7-4568-88f9-063f424aa547",
   "metadata": {},
   "source": [
    "Now, let's return to the example of Kepler-97 c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9cb0e269-739c-4521-8875-2a0185c88cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance                      1308.0\n",
      "star_mag                      12.994\n",
      "planet_type                Gas Giant\n",
      "discovery_yr                    2014\n",
      "mass_ME                       343.44\n",
      "radius_RE                           \n",
      "orbital_radius_AU                NaN\n",
      "orbital_period_yr                2.2\n",
      "eccentricity                     0.0\n",
      "detection_method     Radial Velocity\n",
      "Name: Kepler-97 c, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.loc['Kepler-97 c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259ce46-564b-4ac2-adda-0eb4f50a9c7a",
   "metadata": {},
   "source": [
    "As you can see, there's a NaN in the orbital radius entry and a blank in the planet radius entry. Let's also double-check which columns have the whitespace filler problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f2bb5425-40ce-4fe1-8250-07814651d0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance             float64\n",
      "star_mag             float64\n",
      "planet_type           object\n",
      "discovery_yr           int64\n",
      "mass_ME               object\n",
      "radius_RE             object\n",
      "orbital_radius_AU    float64\n",
      "orbital_period_yr    float64\n",
      "eccentricity         float64\n",
      "detection_method      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96468ddb-f06a-4393-a10b-6cd3e84ee98e",
   "metadata": {},
   "source": [
    "The only unexpected types are for mass and radius, which should be floats, so let's go ahead and fix those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e1a04092-d4e1-4d00-b483-c45e4a4d180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mass_ME'] = df['mass_ME'].replace(' ', np.nan).astype('float64')\n",
    "df['radius_RE'] = df['radius_RE'].replace(' ', np.nan).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1f1a9-a1e9-4610-b675-dc584548db2e",
   "metadata": {},
   "source": [
    "Note that the `.replace()` method I used here is **not** the vector string method, but a Series/DataFrame method of the same name. We'll get to more Series and DataFrame methods later when we talk about operations.\n",
    "\n",
    "Once all the white-spaces have been converted to NaN and the type is converted to float, it becomes possible to use `.isna()` and `.notna()`, and a number of other methods and function kwargs for handling NaNs become available:\n",
    "- `df.fillna(fill_value, inplace=False)` systematically replaces NaNs or Nones with the specified fill value. If the data are object- or string-type, *fill_value* can be a string.\n",
    "- `df.dropna(axis=axis, inplace=False)` drops rows (axis=0) or columns (axis=1) with missing values.\n",
    "- If you need to interpolate over missing data, you can use `ser.interpolate(method=method)` where ser is a Series of numerical or time-like data and method can be 'linear', 'time', 'index', or a SciPy interpolation method. Technically you can use `interpolate()` on a DataFrame as well, but a DataFrame may not be the best format to store image-like data. If you need access-by-label capabilities for images and image cubes, the module you probably want is [Xarray](https://xarray.dev/).\n",
    "- Many numerical methods like `.mean()` and `.cumsum()` have a `skipna` kwarg to control whether or not NaNs are included in the calculation (default is to skip them).\n",
    "\n",
    "**Real-valued bad data.** If you have numerical data that you know to be bad (e.g. all those perfect 0's in the `eccentricity` column of the example DataFrame), or if there are infinities that you would prefer to be masked, there is the `df.mask(condition, other=None)` function, where you can pass conditional selection criteria and use the `other` kwarg to provide substitute values in the form of a scalar, Series, DataFrame, or callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c30baab8-8464-460c-a966-a2c7228408dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#name\n",
      "11 Comae Berenices b    0.23\n",
      "11 Ursae Minoris b      0.08\n",
      "14 Andromedae b         0.00\n",
      "14 Herculis b           0.37\n",
      "16 Cygni B b            0.68\n",
      "Name: eccentricity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['eccentricity'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "29049e8d-1427-4b1c-b904-20be8c7f39e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#name\n",
      "11 Comae Berenices b    0.23\n",
      "11 Ursae Minoris b      0.08\n",
      "14 Andromedae b          NaN\n",
      "14 Herculis b           0.37\n",
      "16 Cygni B b            0.68\n",
      "Name: eccentricity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['eccentricity'].mask(df['eccentricity']==0.0).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b1bba-f6f8-4d88-86d7-e9ccbc187692",
   "metadata": {},
   "source": [
    "### Reindexing, Sorting, Comparing, and Combining\n",
    "Have more than one DataFrame? Do they have partially overlapping column or index labels? Do you worry that some data are duplicated across two DataFrames? No problem! Pandas has functions for a variety of methods to aggregate multiple DataFrames in whole or in part, and to compare two DataFrames with identical row/column labels. However, to do the comparisons and combine the data sensibly, two the indexes and column names of the two or more data structures to be compared or combined must be in a reproducible order, so I will start with sorting and reindexing.\n",
    "\n",
    "#### Reindexing\n",
    "If indexes & column names are missing or need to be updated, you can modify them in-place with `.reindex(index=rows, columns=cols)` (kwargs optional). You can also use `.reindex()` to select data where some index values used as args may not exist, without raising exceptions. There is a related `.reindex_like()` function that lets you copy the data from the DataFrame you call it on into a DataFrame with the row & column labels of the DataFrame you enter as the function arg.\n",
    "\n",
    "There is a `.searchsorted(values)` Series method that returns indexes at which to insert values to maintain order, like NumPy's `.searchsorted()` version, but `.reindex()` makes it somewhat redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c4d53a2-d7cf-4185-acfe-4cfa15e271f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a  b  c\n",
      "38  0  3  0\n",
      "42  1  1  2\n",
      "36  6  3  4\n",
      "48  2  7  1\n",
      "      a    b    c\n",
      "36  6.0  3.0  4.0\n",
      "38  0.0  3.0  0.0\n",
      "40  NaN  NaN  NaN\n",
      "42  1.0  1.0  2.0\n",
      "44  NaN  NaN  NaN\n",
      "46  NaN  NaN  NaN\n",
      "48  2.0  7.0  1.0\n"
     ]
    }
   ],
   "source": [
    "dummy = pd.DataFrame(np.random.randint(0,high=9,\n",
    "                                       size=(4,3)),\n",
    "                     columns = ['a','b','c'],\n",
    "                     index = [38,42,36,48])\n",
    "print(dummy)\n",
    "print(dummy.reindex(np.arange(36,50,2),\n",
    "                    axis='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9d0ac646-4abc-4d7f-b335-0dfebb12d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     a    b    c\n",
      "38 NaN  0.0  1.0\n",
      "42 NaN  NaN  NaN\n",
      "36 NaN  NaN  NaN\n",
      "48 NaN  6.0  7.0\n"
     ]
    }
   ],
   "source": [
    "dummy2 = pd.DataFrame(\n",
    "    np.arange(0,9).reshape(3,3),\n",
    "    columns = ['b','c','d'],\n",
    "    index = [38,43,48])\n",
    "print(dummy2.reindex_like(dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a55eaa-f172-498b-8ccc-02dc14167463",
   "metadata": {},
   "source": [
    "#### Sorting\n",
    "Both DataFrames and Series can be sorted by index, by value, or some combination of both. In the case of DataFrames, row indexes and column labels can be sorted simultaneously. There are 2 methods to sort both Series & DataFrames: `.sort_values(by=row_or_col, axis=0, kind='quicksort')` & `.sort_index(axis=0)`.\n",
    "\n",
    "In both cases, the axis given to the `axis` kwarg is the direction along which values will shift, not the fixed axis. Both return copies unless `inplace=True`, and both have a `key` kwarg that accepts a *vectorized* function (more on those shortly) to apply to the input index before sorting. The key usage as described in the official documentation is a bit misleading in its brevity: it does not alter what indexes are printed, but instead internally alters what indexes the sorting algorithm sees. For example if you have columns you want to sort alphabetically, but only some of the column names are capitalized, the .sort_index() function would normally place any column label starting with a lower-case letter after any column with a capitalized label. To treat capital and lower-case letters equally, you would have to give `key` a function that converts the whole list of indexes to lower-case letters, as I demonstrate below.\n",
    "\n",
    "`.sort_values(by=row_or_col, kind='quicksort')` sorts Series or DataFrames by the values of the given column(s)/row(s) passed to the `by` kwarg (optional for Series). If `by` is one or more row indexes, it is mandatory to set `axis=1` or `axis='columns'`. If `by` is a list, the sorting order may depend on the algorithm given for the `kind` kwarg. I leave it as an exercise to the reader to look up various sorting methods to see how they might interact with the material. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ed8a62-65f2-4976-9793-cfe01db05839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by column C\n",
      "    B  a  C\n",
      "k  8  3  1\n",
      "i  2  1  7\n",
      "j  6  8  7\n",
      "h  0  1  8\n",
      "Sorted by row j\n",
      "    B  C  a\n",
      "h  0  8  1\n",
      "i  2  7  1\n",
      "j  6  7  8\n",
      "k  8  1  3\n"
     ]
    }
   ],
   "source": [
    "dummy = pd.DataFrame(np.random.randint(0,high=9,size=(4,3)),\n",
    "                     columns = ['B','a','C'],\n",
    "                     index = ['h','i','j','k'])\n",
    "print(\"Sorted by column C\\n\",dummy.sort_values('C',axis=0))\n",
    "print(\"Sorted by row j\\n\",dummy.sort_values('j',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "668a3ef8-131a-4c4d-b693-200758369f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns sorted alphabetically\n",
      "    B  C  a\n",
      "h  3  8  5\n",
      "i  2  1  6\n",
      "j  2  1  7\n",
      "k  8  3  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns sorted alphabetically\\n\",\n",
    "      dummy.sort_index(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3391df01-3655-4665-9875-76946315536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns sorted alphabetically with key\n",
      "    a  B  C\n",
      "h  5  3  8\n",
      "i  6  2  1\n",
      "j  7  2  1\n",
      "k  1  8  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns sorted alphabetically with key\\n\",\n",
    "      dummy.sort_index(axis=1,key=lambda c: c.str.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999cefa-e85f-41b5-bff2-8859642fb4c8",
   "metadata": {},
   "source": [
    "#### Combining DataFrames\n",
    "There are 4 Pandas functions and 3 DataFrame methods that can be used to combine 2 (or sometimes more) DataFrames (or Series). The Pandas functions are:\n",
    "1. `.concat()`: combine 2 or more DataFrames or Series along a shared column or index, with optional set logic for handling other axes using the `join` kwarg.\n",
    "2. `.merge(left_df, right_df, how='inner')`: combine 2 or more DataFrames with SQL-database-style join options specified using the `how` kwarg.\n",
    "3. `.merge_ordered()`: combine 2 sorted DataFrames or Series with optional interpolation across gaps.\n",
    "4. `.merge_asof()`: left-join 2 sorted DataFrames or Series by the nearest value of given index instead of requiring identical indexes. Typically used for time series.\n",
    "\n",
    "And the DataFrame methods are:\n",
    "1. `df1.combine_first(df2)`: update missing values of DataFrame `df1` with fill values from DataFrame `df2` at shared index locations, and add rows or columns from `df2` that did not exist in `df1`.\n",
    "2. `df1.combine(df2, func)`: merge 2 DataFrames column-wise based on given function `func` that takes 2 Series & outputs either Series or scalars. Scalar outputs will be propagated to the whole column. The resulting DataFrame's row and column labels will be the union of the row and column labels of both DataFrames.\n",
    "3. `df1.join(df2, on=[cols,or,inds])` (uses `.merge()` internally): join 2 DataFrames on given column(s) or index(es).\n",
    "\n",
    "The easiest to understand is `.concat()`, so I'll start with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0854281e-c14d-4632-b5b6-d95e5cc75adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   B  a  C\n",
      "h  0  1  8\n",
      "i  2  1  7\n",
      "j  6  8  7\n",
      "k  8  3  1\n"
     ]
    }
   ],
   "source": [
    "#start with dummy dataframe from before\n",
    "print(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b31b0c1f-bfa9-42b8-beb1-3002de6d325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   B   a   C\n",
      "q  0   1   2\n",
      "r  3   4   5\n",
      "s  6   7   8\n",
      "t  9  10  11\n"
     ]
    }
   ],
   "source": [
    "#define second dummy with more predicatable values\n",
    "dummy2 = pd.DataFrame(np.arange(0,12).reshape(4,3),\n",
    "                        columns = ['B','a','C'],\n",
    "                        index = ['q','r','s','t'])\n",
    "print(dummy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e3efca94-cb42-402f-ab0d-21d1fa075e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   h  i  j  k  q  r  s   t\n",
      "B  0  2  6  8  0  3  6   9\n",
      "a  1  1  8  3  1  4  7  10\n",
      "C  8  7  7  1  2  5  8  11\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat((dummy.T,dummy2.T), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5d599-534a-40a2-9d8a-d68c76fd359d",
   "metadata": {},
   "source": [
    "Before demonstrating `.merge()` and `.join()`, it will help to go over the meanings of the different SQL-style join options that you can pass to them with the `how` kwarg. The allowed options for the `how` kwarg are:\n",
    "1. `'inner'` (default): take only the intersection of the 2 DataFrames in terms of their column headers and contents at specific row *positions*, like SQL `inner join`.\n",
    "2. `'outer'`: align on any shared values at shared row and column indexes but keep all contents of both DataFrames, like SQL `full outer join`. The resulting DataFrame will have all non-rendundant permutations of the row and column indexes of both DataFrames, with NaNs inserted wherever those row and column combinations do not point to any existing data.\n",
    "3. `'left'`: keep all contents of the left (first) DataFrame, plus any data from the right (second) that share row and column indexes from the left DataFrame, like SQL `left outer join`.\n",
    "4. `'right'`: keep all contents of the right (second) DataFrame, plus any data from the left (first) that share row and column indexes from the right DataFrame, like SQL `right outer join`.\n",
    "5. `'cross'`: take the Cartesian product\\* of the two DataFrames, prioritizing the order of the left one, like SQL `cross join`.\n",
    "\n",
    "\\*If you don't know/remember what a Cartesian product is, imagine you have 2 sets of data, A=(x,y,z) and B=(1,2,3). The Cartesian product, A$\\times$B, would then be the 9 ordered pairs in the following table:\n",
    "|A$\\times$B| 1 | 2 | 3 |\n",
    "|---|---|---|---|\n",
    "| **x** | (x,1) | (x,2) | (x,3) |\n",
    "| **y** | (y,1) | (y,2) | (y,3) |\n",
    "| **z** | (z,1) | (z,2) | (z,3) |\n",
    "\n",
    "\n",
    "The quickest way to show what the first 4 cases mean intuitively is with the following Venn diagrams, where the contents of each circle are the unique combinations of row and column labels in each DataFrame ([source](https://www.ionos.co.uk/digitalguide/hosting/technical-matters/sql-outer-join/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2a1a372a-132a-46a2-a1af-78a931e75e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.ionos.co.uk/digitalguide/fileadmin/DigitalGuide/Screenshots_2018/Outer-Join.jpg\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://www.ionos.co.uk/digitalguide/fileadmin/DigitalGuide/Screenshots_2018/Outer-Join.jpg',\n",
    "      width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32885b-69e9-4bd7-b6f6-94c9c2dcf5b2",
   "metadata": {},
   "source": [
    "`Cross` joining is best demonstrated like this ([Source](https://www.sqlshack.com/sql-cross-join-with-examples/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ec4e870b-203e-49eb-a56f-255c1382adc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.sqlshack.com/wp-content/uploads/2020/02/sql-cross-join-working-mechanism.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://www.sqlshack.com/wp-content/uploads/2020/02/sql-cross-join-working-mechanism.png',\n",
    "      width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf87843-ce41-4425-b0d6-1ecdefe2da9f",
   "metadata": {},
   "source": [
    "\n",
    "In all but the `'outer'` case, the  the order of indexes or keys in the first DataFrame is preserved unless the `sort` kwarg is True or other kwargs are used to manipulate the indexes. In the `'outer'` case, column indexes are sorted alpha-numerically. In most cases, row indexes/labels are not preserved with `merge()` but for `join()` they often are.\n",
    "\n",
    "By default, `merge()` scans all columns with shared names across both DataFrames looking for rows where identical data share identical column names and row *positions* (not labels, but underlying 0-based numerical indexes), and aligns the DataFrames based on those intersections. If there are multiple possibilities, `inner ` returns all of those rows (and only those rows); `outer` returns all of the content aligned with those rows as much as possible with NaNs for padding; and `left` and `right` return the selected DataFrame with the columns of the other DataFrame with matching data appended at the rows where the data aligned, and NaNs in the rows without matching data.\n",
    "\n",
    "If you only want to align on a subset of the shared data, you can provide column labels to the `on` kwarg, but the merger will only succeed if **both** of the following conditions are met:\n",
    "1. The column labels provided exist in both DataFrames.\n",
    "2. Both DataFrames have the same data in the specified columns *at the same row positions*.\n",
    "\n",
    "You can also specify `left|right_on` alignments separately, and/or `left|right_index=True` to align on row indexes from the left or right DataFrame, but these can be very tricky to use successfully. The examples below will clarify some of these details, but it would take much more time and verbiage than either I or even the official Pandas documentation can demonstrate how all of the different kwargs interact with each other and with varying sizes of DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b0d150ef-af8b-4219-bc27-2c4649a122ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(re)define more orderly dummy dataframes as needed for examples\n",
    "dummy2 = pd.DataFrame(np.arange(0,12).reshape(4,3),\n",
    "                        columns = ['A','B','C'],\n",
    "                        index = ['e','f','g','h'])\n",
    "dummy3 = pd.DataFrame(np.arange(-5,11).reshape(4,4),\n",
    "                        columns = ['B','C','D', 'E'],\n",
    "                        index = ['f','g','h','i'])\n",
    "dummy3.loc['g',['B','C']] = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3f24dc2f-0f7b-4070-9090-46775c2e197d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B   C\n",
      "e  0   1   2\n",
      "f  3   4   5\n",
      "g  6   7   8\n",
      "h  9  10  11 \n",
      "\n",
      "   B  C  D   E\n",
      "f -5 -4 -3  -2\n",
      "g  1  2  1   2\n",
      "h  3  4  5   6\n",
      "i  7  8  9  10 \n",
      "\n",
      "    A  B_x  C_x  B_y  C_y  D   E\n",
      "0   0    1    2   -5   -4 -3  -2\n",
      "1   0    1    2    1    2  1   2\n",
      "2   0    1    2    3    4  5   6\n",
      "3   0    1    2    7    8  9  10\n",
      "4   3    4    5   -5   -4 -3  -2\n",
      "5   3    4    5    1    2  1   2\n",
      "6   3    4    5    3    4  5   6\n",
      "7   3    4    5    7    8  9  10\n",
      "8   6    7    8   -5   -4 -3  -2\n",
      "9   6    7    8    1    2  1   2\n",
      "10  6    7    8    3    4  5   6\n",
      "11  6    7    8    7    8  9  10\n",
      "12  9   10   11   -5   -4 -3  -2\n",
      "13  9   10   11    1    2  1   2\n",
      "14  9   10   11    3    4  5   6\n",
      "15  9   10   11    7    8  9  10\n"
     ]
    }
   ],
   "source": [
    "#demonstrate 'outer', 'inner', 'left', 'right', 'cross'\n",
    "print(dummy2,'\\n')\n",
    "print(dummy3,'\\n')\n",
    "print(pd.merge(dummy2,dummy3, how='cross'))#, on=['B','C']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32bd0a-444e-4202-8c1c-cac1e4420246",
   "metadata": {},
   "source": [
    "For the `on` kwarg, you should try to avoid providing more than one column at a time unless you know for sure the values in both DataFrames are the same at those columns *and in the same rows*. In the above example, I can use `['B','C']` with `how='inner'` only because in both DataFrames there exists a row with a 7 in column `B` *and* an 8 in column `C`, i.e. row `g` of the first DataFrame, and row `i` of the second. Therefore, the inner merge result of `pd.merge(dummy2,dummy3, on=['B','C'], how='inner')` is the union of those two rows, which in turn are the intersection of the two DataFrames. In fact, this intersection is the only possible point of alignment given the (original) contents of these DataFrames, so the inner and outer merge results are both unchanged by leaving out `on=['B','C']'`. It would be a different story if I, say, changed row `g` of `dummy3` to `[1, 2, 1, 2]`.\n",
    "\n",
    "I would recommend avoiding any case where you have to use `left|right_index=True` and/or `left|right_on` kwargs, because it quickly becomes difficult to determine which combinations will not raise a `MergeError` without, e.g., working it out on paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8ea67ee6-b58c-4fb2-96ae-96d3118c6b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B_l</th>\n",
       "      <th>C_l</th>\n",
       "      <th>B_r</th>\n",
       "      <th>C_r</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B_l  C_l  B_r  C_r    D    E\n",
       "e  0    1    2  NaN  NaN  NaN  NaN\n",
       "f  3    4    5 -5.0 -4.0 -3.0 -2.0\n",
       "g  6    7    8  1.0  2.0  1.0  2.0\n",
       "h  9   10   11  3.0  4.0  5.0  6.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy2.join(dummy3,lsuffix='_l',rsuffix='_r')\n",
    "#merge uses x & y by default, but join has empty defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57b2fa-c582-4c54-942b-3bb9924f1821",
   "metadata": {},
   "source": [
    "Now I'll demonstrate `.combine_first()` and `.combine()`. The former is a bit more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9f756232-f723-4a20-9671-785dd944a8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     B    C    D     E\n",
      "f  NaN  NaN  NaN   NaN\n",
      "g  NaN  2.0  NaN   2.0\n",
      "h  3.0  4.0  5.0   6.0\n",
      "i  7.0  8.0  9.0  10.0\n"
     ]
    }
   ],
   "source": [
    "dummy3.mask(dummy3<=1, inplace=True) #create (more) missing values to replace\n",
    "print(dummy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bcf0ce45-5c35-4115-bf2c-f86316e02287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C    D     E\n",
       "e  0.0  1.0  2.0  NaN   NaN\n",
       "f  3.0  4.0  5.0  NaN   NaN\n",
       "g  6.0  7.0  2.0  NaN   2.0\n",
       "h  9.0  3.0  4.0  5.0   6.0\n",
       "i  NaN  7.0  8.0  9.0  10.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy3.combine_first(dummy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ce47f-6b03-4e72-914f-fde295f67ad3",
   "metadata": {},
   "source": [
    "Notice that `dummy3.combine_first(dummy2)` not only replaced the NaNs in `dummy3` with numeric values from `dummy2` at the same row and column locations, but also prepended column `A` and row `e` to `dummy3`.\n",
    "\n",
    "`.combine()` is a little bit trickier because it relies on you providing a sensible function to handle locations where both DataFrames have data. Functions may be user-defined (named or passed as lambda functions), built-in, or provided by other imported modules like NumPy, but they must accept two 1D arrays and return either a 1D array or a scalar. Functions that return scalars will propagate the same value to the whole column. Once again, I'm starting with that masked `dummy3` DataFrame, and I will show the results of a couple different functions. The first version is a mistake you might make if you're trying to make `combine()` take the higher value at any position where both DataFrames have data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5b5e9be0-3b57-4365-9b8c-408f53ceafa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     B    C    D     E\n",
      "f  NaN  NaN  NaN   NaN\n",
      "g  NaN  NaN  NaN   2.0\n",
      "h  3.0  4.0  5.0   6.0\n",
      "i  7.0  8.0  9.0  10.0 \n",
      "\n",
      "   A   B   C\n",
      "e  0   1   2\n",
      "f  3   4   5\n",
      "g  6   7   8\n",
      "h  9  10  11 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B     C    D     E\n",
       "e  9.0  10.0  11.0  9.0  10.0\n",
       "f  9.0  10.0  11.0  9.0  10.0\n",
       "g  9.0  10.0  11.0  9.0  10.0\n",
       "h  9.0  10.0  11.0  9.0  10.0\n",
       "i  9.0  10.0  11.0  9.0  10.0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print these to refresh our memories\n",
    "print(dummy3,'\\n')\n",
    "print(dummy2,'\\n')\n",
    "#print the first way I tried to code up the function\n",
    "# that didn't raise a type error\n",
    "dummy3.combine(dummy2, lambda x,y: np.nanmax([x,y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c33bb5-bb28-43cf-aec1-d813a6ae209c",
   "metadata": {},
   "source": [
    "The following is what is actually required to achieve the desired result without learning any new methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3d0beff6-2775-47ac-8067-a331ff4e530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10410/776801286.py:6: RuntimeWarning: All-NaN axis encountered\n",
      "  ser.loc[i] = np.nanmax([x.loc[i],y.loc[i]])\n",
      "/tmp/ipykernel_10410/776801286.py:6: RuntimeWarning: All-NaN axis encountered\n",
      "  ser.loc[i] = np.nanmax([x.loc[i],y.loc[i]])\n",
      "/tmp/ipykernel_10410/776801286.py:6: RuntimeWarning: All-NaN axis encountered\n",
      "  ser.loc[i] = np.nanmax([x.loc[i],y.loc[i]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A     B     C    D     E\n",
       "e  0.0   1.0   2.0  NaN   NaN\n",
       "f  3.0   4.0   5.0  NaN   NaN\n",
       "g  6.0   7.0   8.0  NaN   2.0\n",
       "h  9.0  10.0  11.0  5.0   6.0\n",
       "i  NaN   7.0   8.0  9.0  10.0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elem_wise_nanmax(x,y):\n",
    "    rns = set(list(x.index)+list(y.index)) #get union of row indexes\n",
    "    ser = pd.Series(index=rns) #create Series to store results for return\n",
    "    for i in list(rns): #iterate over rows\n",
    "        try: #take nanmax if values exist in both input series...\n",
    "            ser.loc[i] = np.nanmax([x.loc[i],y.loc[i]])\n",
    "        except IndexError: #...otherwise take whichever one exists\n",
    "            if i in x.index:\n",
    "                ser.loc[i]=x.loc[i]\n",
    "            elif i in y.index:\n",
    "                ser.loc[i]=y.loc[i]\n",
    "    return ser # return results in series form\n",
    "\n",
    "dummy3.combine(dummy2,elem_wise_nanmax) #combine applies elem_wise_nanmax column-by-column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102a54b-6111-49c3-8783-4990e0da70e0",
   "metadata": {},
   "source": [
    "As you can see, this is pretty clunky, and iteration makes this way slower than necessary. Fortunately, Pandas provides ways of broadcasting functions over rows and/or columns simultaneously, without iteration. So, let's get into Pandas functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2f57e-282b-4436-abcf-114400b18fd5",
   "metadata": {},
   "source": [
    "## Operations\n",
    "Pandas DataFrames have method counterparts to most NumPy statistical functions, string methods, plus **many** other convenience functions. We've already covered functions that report basic descriptive data about the DataFrame and functions that handle NaNs. Now we'll focus on the functions that move, change, or do something else with valid data or the Series/DataFrame as a whole. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4b3e8-3836-41e1-97ae-9ee939673761",
   "metadata": {},
   "source": [
    "#### Comparisons\n",
    "You can compare a Series or a DataFrame to a single scalar value with normal operators (`>=`, `!=`, etc).\n",
    "To compare 1 Series or DataFrame element-wise to another of the same shape (as the arg), use `.gt()`, `.lt()`, `.ge()`, `.le()`, `.eq()`, or `.ne()`. For any of the aforementioned comparison methods (to scalars or other pandas data structures), you can add `.any()` or `.all()` once to collapse the column axis, or twice to get 1 value. This is called Boolean Reduction.\n",
    "\n",
    "To find & print differences between 2 identically indexed Series or DataFrames (let's call them `df1` and `df2`; both objects must have the same row & column labels in the same order), you can use `df1.compare(df2)`. The `.compare()` method will show differences as subtle as extraneous whitespaces or differences of capitalization. However, it will **not** show data type differences if the values are numerically equal; for that, you''l need to use `pd.testing.assert_frame_equal(df1, df2)` or `pd.testing.assert_series_equal(df1, df2)` to see if it raises `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da227c11-1fbc-4698-8f3e-6216bf8675c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dummy = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9]], columns = ['a','b','c'], index = [0,1,2])\n",
    "dummy2 = dummy.astype(float)\n",
    "print(dummy.compare(dummy2))\n",
    "print(pd.testing.assert_frame_equal(dummy,dummy2,check_dtype=False)) #assertion error if True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa96ca-7a8b-4fdc-a3ff-3b2078ff57fd",
   "metadata": {},
   "source": [
    "#### Statistics\n",
    "Stats methods (`.mean()`, `.std()`, `.median()`, `.min()`/`.max()`, `.cumsum()`/`.comprod()`, `cov()`, `corr()`, etc.) mostly work like you'd expect based on their NumPy counterparts, and also ignore NaNs by default. If you input a Series or one column of a DataFrame, the result is typically a single value or a Series of the same length. If you input a DataFrame, you will have to choose an axis to evaluate the function over, and the result will be either a Series or, as in the case of the `.corr()` function, a DataFrame in the form of a square matrix with identical row and column labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "bad4125d-9a90-4934-92de-340b2af4b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average detected exoplanet mass is 1.447 Jupiter masses.\n"
     ]
    }
   ],
   "source": [
    "print('The average detected exoplanet mass is {:.3f} \\\n",
    "Jupiter masses.'.format(df['mass_ME'].mean()/317.907))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2ad66639-5f53-4970-b223-9a05198ef958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most distant planet detected is 27727 light years away.\n"
     ]
    }
   ],
   "source": [
    "print('The most distant planet detected is \\\n",
    "{:.0f} light years away.'.format(df['distance'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "4ea4d017-89d6-4211-8e98-82acc91782f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard deviation and standard error of the mean for orbital radii are 139 and 2 AU, respectively. \n"
     ]
    }
   ],
   "source": [
    "print(\"The standard deviation and standard error of the mean for\\\n",
    " orbital radii are {:.0f} and {:.0f} AU, respectively.\\\n",
    " \".format(df['orbital_radius_AU'].std(), df['orbital_radius_AU'].sem()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "6f984902-a534-412b-9416-b72c4a45bbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Neptune-like\n",
      "Name: planet_type, dtype: object\n",
      "The most common type of exoplanet detected is Neptune-like.\n"
     ]
    }
   ],
   "source": [
    "print(df['planet_type'].mode()) #output is a Series\n",
    "print(\"The most common type of exoplanet detected is \\\n",
    "{}.\".format(df['planet_type'].mode()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59702d46-1c4d-44e2-b18f-ffad6d9885f0",
   "metadata": {},
   "source": [
    "\n",
    "There are also methods that you typically need SciPy for, like `.skew()` and `.kurtosis()`, and functions for assigning ranks (`.rank()`) and quantiles (`.quantile()`).\n",
    "\n",
    "All of the above can be applied to the whole DataFrame column-wise or row-wise, and with the `.rolling` attribute between the DataFrame and the function, can also be evaluated over rolling windows of $n$ entries instead of the whole axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5254e-924c-4fa5-abd7-b94c85ebdff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd1c867-3a3e-4487-bd88-ec048ef4f378",
   "metadata": {},
   "source": [
    "#### Binary Operations\n",
    "\n",
    "The more exciting features of Pandas are that you can broadcast operations across rows, columns, whole DataFrames, and even do arithmetic on multiple DataFrames. For these purposes, DataFrames have the methods `add()`, `sub()`, `mul()`, `div()`, `pow()`, `mod()`, and `divmod()`, and reversed versions of each (`radd()`, `rsub()`, ...) for if the operations do not commute. To take `div()` for example, the forward version `dfA.div(other)` divides `dfA` by `other`, where `other` could be a scalar, a Series or vector of the same length as one of the dimensions of `dfA`, or a DataFrame of the same shape as `dfA`. In the reverse version, `dfA.rdiv(other)` divides `other` by `dfA`.\n",
    "\n",
    "Remember that to divide a DataFrame by a vector or Series, you must specify an axis. The axis determines which dimension the operation is to be broadcast along, NOT the orientation of the vector. If you set `axis='index'` or 0, each entry in the vector will be paired with a row, and if you set `axis='columns'`, each entry in the vector will be matched to a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a0bd2e9-959e-4429-b5e9-d92118a1c789",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a   b   c\n",
      "0  0   1   2\n",
      "1  3   4   5\n",
      "2  6   7   8\n",
      "3  9  10  11 \n",
      "\n",
      "      a          b          c\n",
      "0  0.0   0.250000   0.500000\n",
      "1  1.0   1.333333   1.666667\n",
      "2  3.0   3.500000   4.000000\n",
      "3  9.0  10.000000  11.000000 \n",
      "\n",
      "           a         b         c\n",
      "0       inf  4.000000  2.000000\n",
      "1  1.000000  0.750000  0.600000\n",
      "2  0.333333  0.285714  0.250000\n",
      "3  0.111111  0.100000  0.090909\n"
     ]
    }
   ],
   "source": [
    "dfA = pd.DataFrame(np.arange(12).reshape([4,3]),\n",
    "                   columns = ['a','b','c'])\n",
    "print(dfA,'\\n\\n',dfA.div([4.,3.,2., 1.], axis='index'),\n",
    "      '\\n\\n', dfA.rdiv([4.,3.,2., 1.], axis='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482bb983-b557-46a5-b866-9fd016fe9e08",
   "metadata": {},
   "source": [
    "The method `.divmod()` is a bit special in that a) it can only be called on dividends of Series or Index type, b) and it returns 2 Series instead of just 1. The first Series is the results of floor division, and the second Series is the list of remainders. The divisor (the argument in parentheses) may be either a scalar or a Series of the same length as the dividend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27db4e84-9fff-448a-be8e-598282834767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nums  rems  mods\n",
      "0    0.0   0.0   0.0\n",
      "1    0.5   0.0   0.5\n",
      "2    1.0   0.0   1.0\n",
      "3    1.5   0.0   1.5\n",
      "4    2.0   0.0   2.0\n",
      "5    2.5   0.0   2.5\n",
      "6    3.0   1.0   0.0\n",
      "7    3.5   1.0   0.5\n",
      "8    4.0   1.0   1.0\n",
      "9    4.5   1.0   1.5\n",
      "10   5.0   1.0   2.0\n",
      "11   5.5   1.0   2.5\n",
      "12   6.0   2.0   0.0\n",
      "13   6.5   2.0   0.5\n",
      "14   7.0   2.0   1.0\n",
      "15   7.5   2.0   1.5\n",
      "16   8.0   2.0   2.0\n",
      "17   8.5   2.0   2.5\n",
      "18   9.0   3.0   0.0\n"
     ]
    }
   ],
   "source": [
    "ser3 = pd.Series(np.linspace(0,9,19))\n",
    "rems,mods=ser3.divmod(3)\n",
    "print(pd.concat((ser3.rename('nums'),\n",
    "                 rems.rename('rems'),\n",
    "                 mods.rename('mods')), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb370ed-c3e7-4267-b3b4-6c5e2c63d13b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Vectorized String Methods\n",
    "Most familiar string methods like `.replace()`, `.lower()`/`.upper()`, `.split()`/`.rsplit()`, and `.strip()` have counterparts in Pandas that can be broadcast to every element with the syntax `.str.<method>`. The available args and kwargs for each of the functions are also mostly unchanged, except in the case of `.str.replace()`: the standard two positional arguments are accepted, as are regular expressions, but it will **not** accept a dictionary where the keys are the existing substrings and the values are the replacement characters. (Note: dictionary replacement also fails for the built-in string `replace()` method if any of the old or new strings contain LaTeX expressions because `replace()` implicitly uses string insertion via `'{}'.format()`, which collides with LaTeX's use of curly braces as delimiters for function arguments.)\n",
    "\n",
    "The function `.str.split(\" \", expand=False, n=None)` and its `.str.rsplit()` counterpart return either a Series in which the items are lists or, with the kwarg `expand=True`, a DataFrame with as many columns as there would have been items in the longest list of substrings (shorter lists will leave Nones in the columns they weren't long enought to fill). If you want to limit the number of length of each list or row, you can set `n` equal to the maximum number of occurences of the character or substring to split on. Unfortunately, if you set `expand=True`, there is not currently a kwarg to set the column names of the resulting DataFrame in the same step, so at least initially both rows and columns will be labeled with 0-based indexes. With `expand=False` (default), you can also use either `.str.get(i)` or just `.str[i]`, where `i` is the list index(es) or slice, to return a Series of subsets of each list of split strings. I'll demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1538ca32-d29a-4610-b743-8286f097065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " 0    Pandas are cute!\n",
      "1      I like trains.\n",
      "2       Hello, world?\n",
      "3               Hams.\n",
      "dtype: object \n",
      "\n",
      "Split and get:\n",
      " 0       [Pandas, are]\n",
      "1           [I, like]\n",
      "2    [Hello,, world?]\n",
      "3             [Hams.]\n",
      "dtype: object \n",
      "\n",
      "Expand=True:\n",
      "         0       1        2\n",
      "0  Pandas     are    cute!\n",
      "1       I    like  trains.\n",
      "2  Hello,  world?     None\n",
      "3   Hams.    None     None\n"
     ]
    }
   ],
   "source": [
    "dummy4 = pd.Series(['Pandas are cute!', 'I like trains.','Hello, world?', 'Hams.'])\n",
    "print('Original:\\n', dummy4, '\\n')\n",
    "dummy5 = dummy4.str.split()\n",
    "print('Split and get:\\n',dummy5.str[:2], '\\n')\n",
    "dummy6 = dummy4.str.split(expand=True)\n",
    "print('Expand=True:\\n', dummy6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0089525-ba15-4637-a166-de67f481efff",
   "metadata": {},
   "source": [
    "#### Time Series Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88560d2-be4e-432a-a3c8-53b2e108f910",
   "metadata": {},
   "source": [
    "### User-Defined Methods\n",
    "If you find that you need to broadcast more complicated functions to your data, there are 4 methods you can use depending on whether the function is a combination of named NumPy functions or totally user-defined, what shape the function broadcasts or reduces (aggregates) results to, and /or whether the function is to be applied row-wise, column-wise, or element-wise.\n",
    "\n",
    "#### Aggregating (aka reducing) with `.agg()`\n",
    "The `.agg()` method only accepts functions that take all the values along a specified axis (row or column in this case) as input and then output a single value, like `sum()` or `std()`. You can supply more than one of these functions at a time as a list of function names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c44348-f0f5-48c4-bc54-53ae7f732371",
   "metadata": {},
   "source": [
    "#### Broadcasting with `.transform()`\n",
    "The `.transform()` method requires the output to have the same shape as the input, but you can apply more than one function at a time as long as the functions are passed by name. You could even set up a function list to apply a different function to every column of a DataFrame. The powerful feature of `.transform()` is that it can be applied group-wise to GroupBy objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5843a-9785-4bcf-a1d7-d62a005d9785",
   "metadata": {},
   "source": [
    "#### Elementwise functions with `.map()`\n",
    "The `.map()` method is like a more restricted version of transform, and is typically used for element-wise replacement of values by function. It can only take 1 function at a time, but it is typically cleaner and more intuitive if that's all you need.\n",
    "\n",
    "That said, if you can recast your desired operations as one or more vectorized functions, like `df**0.5` instead of `df.map(np.sqrt)`, the vectorized version will generally be faster.\n",
    "\n",
    "One simple use case is if your function is piecewise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d235a64f-df16-4f49-9a41-8445c82a7e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C\n",
      "0  231  426  572\n",
      "1  497  628  410\n",
      "2  375  600  577\n",
      "3  408  206  616 \n",
      "\n",
      "          A         B         C\n",
      "0  0.211211  0.211957  0.306578\n",
      "1  0.260593  0.337479  0.200328\n",
      "2  0.174117  0.322379  0.309452\n",
      "3  0.198858  0.144310  0.331091\n"
     ]
    }
   ],
   "source": [
    "def my_func(T):\n",
    "    if T<=0 or np.isnan(T) is True:\n",
    "        pass\n",
    "    elif T<300:\n",
    "        return 0.2*(T**0.5)*np.exp(-616/T)\n",
    "    elif T>=300:\n",
    "        return 0.9*np.exp(-616/T)\n",
    "    \n",
    "junk = pd.DataFrame(np.random.randint(173,high=675,size=(4,3)),\n",
    "                    columns = ['A', 'B', 'C'])\n",
    "print(junk,'\\n')\n",
    "print(junk.map(my_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8303a14-8779-4f49-90b4-30b2193a7c57",
   "metadata": {},
   "source": [
    "#### For most other operations, there's `.apply()`\n",
    "\n",
    "Remember waaaaay back to when we were trying to combine 2 DataFrames and take the higher value wherever both DataFrames had data? We can do that faster and with less code using `.apply()`. I'll repost the dummy DataFrames and the clunky function that let us combine them as desired, and then show the improved way of doing it with `.apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5efc7aaf-d85d-4047-94d0-24d18f9e342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B   C\n",
      "e  0   1   2\n",
      "f  3   4   5\n",
      "g  6   7   8\n",
      "h  9  10  11 \n",
      "\n",
      "   A   B   C\n",
      "e  0   1   2\n",
      "f  3   4   5\n",
      "g  6   7   8\n",
      "h  9  10  11 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy2 = pd.DataFrame(np.arange(0,12).reshape(4,3),\n",
    "                        columns = ['A','B','C'],\n",
    "                        index = ['e','f','g','h'])\n",
    "dummy3 = pd.DataFrame(np.arange(-5,11).reshape(4,4),\n",
    "                        columns = ['B','C','D', 'E'],\n",
    "                        index = ['f','g','h','i'])\n",
    "dummy3.mask(dummy3<=1, inplace=True) #create more missing values to replace\n",
    "print(dummy2,'\\n')\n",
    "print(dummy2,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2301d2c1-427f-4a39-a91f-138402d5c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B     C    D     E\n",
      "e  0.0   1.0   2.0  NaN   NaN\n",
      "f  3.0   4.0   5.0  NaN   NaN\n",
      "g  6.0   7.0   8.0  NaN   2.0\n",
      "h  9.0  10.0  11.0  5.0   6.0\n",
      "i  NaN   7.0   8.0  9.0  10.0\n",
      "time taken in s: 0.010122537612915039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10410/3482005737.py:9: RuntimeWarning: All-NaN axis encountered\n",
      "  ser.loc[i] = np.nanmax([x.loc[i],y.loc[i]])\n",
      "/tmp/ipykernel_10410/3482005737.py:9: RuntimeWarning: All-NaN axis encountered\n",
      "  ser.loc[i] = np.nanmax([x.loc[i],y.loc[i]])\n",
      "/tmp/ipykernel_10410/3482005737.py:9: RuntimeWarning: All-NaN axis encountered\n",
      "  ser.loc[i] = np.nanmax([x.loc[i],y.loc[i]])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "# Old clunky function:\n",
    "def elem_wise_nanmax(x,y):\n",
    "    rns = set(list(x.index)+list(y.index))\n",
    "    ser = pd.Series(index=rns)\n",
    "    for i in list(rns):\n",
    "        try:\n",
    "            ser.loc[i] = np.nanmax([x.loc[i],y.loc[i]])\n",
    "        except IndexError:\n",
    "            if i in x.index:\n",
    "                ser.loc[i]=x.loc[i]\n",
    "            elif i in y.index:\n",
    "                ser.loc[i]=y.loc[i]\n",
    "    return ser\n",
    "\n",
    "print(dummy3.combine(dummy2,elem_wise_nanmax))\n",
    "print(\"time taken in s:\", time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "aafe738d-1440-49ce-89e3-33619c5375d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B     C    D     E\n",
      "e  0.0   1.0   2.0  NaN   NaN\n",
      "f  3.0   4.0   5.0  NaN   NaN\n",
      "g  6.0   7.0   8.0  NaN   2.0\n",
      "h  9.0  10.0  11.0  5.0   6.0\n",
      "i  NaN   7.0   8.0  9.0  10.0\n",
      "time taken in s: 0.006251811981201172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10410/2096513521.py:5: RuntimeWarning: All-NaN axis encountered\n",
      "  return row.combine(row2, lambda x, y: np.nanmax([x, y]))\n",
      "/tmp/ipykernel_10410/2096513521.py:5: RuntimeWarning: All-NaN axis encountered\n",
      "  return row.combine(row2, lambda x, y: np.nanmax([x, y]))\n"
     ]
    }
   ],
   "source": [
    "# new hotness (less intuitive & still clunky, but faster)\n",
    "t0 = time.time()\n",
    "def elem_wise_nanmax(row,dfr): #row is from df_left, dfr is df_right\n",
    "    row2 = dfr.loc[row.name] if row.name in dfr.index else pd.Series(index=dfr.columns)\n",
    "    return row.combine(row2, lambda x, y: np.nanmax([x, y]))        \n",
    "#.name attribute returns the name of a column if there is one, otherwise it returns a row name\n",
    "print(dummy3.combine_first(dummy2).apply(lambda z: elem_wise_nanmax(z,dummy2), axis=1))\n",
    "print(\"time taken in s:\", time.time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e1fef-7be2-4ee4-9e09-3732cc5c7f4d",
   "metadata": {},
   "source": [
    "## Preparing input for Machine Learning (ML)\n",
    "ML programs like TensorFlow and PyTorch are highly compatible with Series/DataFrame inputs, but there are still limitations to the data types that will produce useful results. For example (and this doesn't make much sense given what we actually know about exoplanets, but) let's say you wanted to build a neural network model that predicts the planet type based on a subset of the physical or orbital parameters. The physical and orbital parameters of the existing data are numerical, but Planet Type is a categorical variable with 5 unique values. What we would want to do, then, is turn each of those categories into columns where the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028449d4-1b76-4cfa-83b6-7a899ad6db33",
   "metadata": {},
   "source": [
    "## GroupBy Objects\n",
    "\n",
    "GroupBy objects (more properly DataFrameGroupBy) are the data returned by the function `df.groupby(by, axis=0)`, where the `by` argument can be a column label, a list of column names, row index(es) if you set the axis kwarg equal to 1, or even a Series. There is also a level kwarg if you choose to call `groupby()` on a hierarchical DataFrame... \n",
    "\n",
    "[need to add a bunch of stuff here. refer to](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html)\n",
    "\n",
    "When inspecting a GroupBy object, many DataFrame inspection methods still work, and there is an added `.nth()` method that lets you grab the *n*th row of every group, where *n* is a 0-based index that can also be negative if you want to start from the bottom. For instance, if you wanted just the last row of every group in a DataFrame `df` grouped by some column `col_B`, `df.groupby('col_B').nth(-1)` would return the same rows as `df.groupby('col_B').tail(1)`.\n",
    "\n",
    "...\n",
    "\n",
    "GroupBy objects share some built-in functions with DataFrames, but that does not include binary math functions. You should also be wary of using `.apply()` on a GroupBy object because `apply()` has to infer the level at which it's supposed to start applying the function, and that doesn't always work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fc9ed5d-6f4d-4894-ae27-cb84b9c2f3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance</th>\n",
       "      <th>star_mag</th>\n",
       "      <th>planet_type</th>\n",
       "      <th>discovery_yr</th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>detection_method</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LkCa 15 c</th>\n",
       "      <td>516.0</td>\n",
       "      <td>12.025</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.60000</td>\n",
       "      <td>0.999316</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Direct Imaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wolf 503 b</th>\n",
       "      <td>145.0</td>\n",
       "      <td>10.270</td>\n",
       "      <td>Neptune-like</td>\n",
       "      <td>2018</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2.043</td>\n",
       "      <td>0.05706</td>\n",
       "      <td>0.016427</td>\n",
       "      <td>0.41</td>\n",
       "      <td>Transit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YSES 2 b</th>\n",
       "      <td>357.0</td>\n",
       "      <td>10.885</td>\n",
       "      <td>Gas Giant</td>\n",
       "      <td>2021</td>\n",
       "      <td>2003.40</td>\n",
       "      <td>12.768</td>\n",
       "      <td>115.00000</td>\n",
       "      <td>1176.500000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Direct Imaging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YZ Ceti b</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.074</td>\n",
       "      <td>Terrestrial</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.01634</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.06</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YZ Ceti d</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.074</td>\n",
       "      <td>Super Earth</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.02851</td>\n",
       "      <td>0.012868</td>\n",
       "      <td>0.07</td>\n",
       "      <td>Radial Velocity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            distance  star_mag   planet_type  discovery_yr  mass_ME  \\\n",
       "#name                                                                 \n",
       "LkCa 15 c      516.0    12.025       Unknown          2015      NaN   \n",
       "Wolf 503 b     145.0    10.270  Neptune-like          2018     6.26   \n",
       "YSES 2 b       357.0    10.885     Gas Giant          2021  2003.40   \n",
       "YZ Ceti b       12.0    12.074   Terrestrial          2017     0.70   \n",
       "YZ Ceti d       12.0    12.074   Super Earth          2017     1.09   \n",
       "\n",
       "            radius_RE  orbital_radius_AU  orbital_period_yr  eccentricity  \\\n",
       "#name                                                                       \n",
       "LkCa 15 c         NaN           18.60000           0.999316          0.00   \n",
       "Wolf 503 b      2.043            0.05706           0.016427          0.41   \n",
       "YSES 2 b       12.768          115.00000        1176.500000          0.00   \n",
       "YZ Ceti b       0.913            0.01634           0.005476          0.06   \n",
       "YZ Ceti d       1.030            0.02851           0.012868          0.07   \n",
       "\n",
       "           detection_method  \n",
       "#name                        \n",
       "LkCa 15 c    Direct Imaging  \n",
       "Wolf 503 b          Transit  \n",
       "YSES 2 b     Direct Imaging  \n",
       "YZ Ceti b   Radial Velocity  \n",
       "YZ Ceti d   Radial Velocity  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped1=df.groupby(['planet_type'])\n",
    "grouped1.nth(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e38d65b8-58d1-416b-bc8b-a17f4b3894ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "planet_type\n",
       "Gas Giant       21.515449\n",
       "Neptune-like     0.224902\n",
       "Super Earth      0.109952\n",
       "Terrestrial      0.062381\n",
       "Unknown         16.650000\n",
       "Name: orbital_radius_AU, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped1['orbital_radius_AU'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67471159-daad-4967-86af-c8405427e2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KIC 10001893 b</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KIC 10001893 c</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KIC 10001893 d</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LkCa 15 b</th>\n",
       "      <td>14.7</td>\n",
       "      <td>0.999316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LkCa 15 c</th>\n",
       "      <td>18.6</td>\n",
       "      <td>0.999316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                orbital_radius_AU  orbital_period_yr\n",
       "#name                                               \n",
       "KIC 10001893 b                NaN           0.000548\n",
       "KIC 10001893 c                NaN           0.000821\n",
       "KIC 10001893 d                NaN           0.002190\n",
       "LkCa 15 b                    14.7           0.999316\n",
       "LkCa 15 c                    18.6           0.999316"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped1.get_group('Unknown').iloc[:,6:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "546a2649-8391-4635-bce1-72cbcac21382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass_ME</th>\n",
       "      <th>radius_RE</th>\n",
       "      <th>orbital_radius_AU</th>\n",
       "      <th>orbital_period_yr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detection_method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Astrometry</th>\n",
       "      <td>4890.840000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.499825</td>\n",
       "      <td>0.726626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Direct Imaging</th>\n",
       "      <td>7929.949333</td>\n",
       "      <td>15.835680</td>\n",
       "      <td>514.123769</td>\n",
       "      <td>40445.285440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disk Kinematics</th>\n",
       "      <td>795.000000</td>\n",
       "      <td>13.216000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>957.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eclipse Timing Variations</th>\n",
       "      <td>2154.773529</td>\n",
       "      <td>12.880000</td>\n",
       "      <td>3.962357</td>\n",
       "      <td>9.628240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gravitational Microlensing</th>\n",
       "      <td>746.775584</td>\n",
       "      <td>10.241521</td>\n",
       "      <td>2.541477</td>\n",
       "      <td>7.065273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orbital Brightness Modulation</th>\n",
       "      <td>350.513333</td>\n",
       "      <td>9.623000</td>\n",
       "      <td>0.013667</td>\n",
       "      <td>0.003164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulsar Timing</th>\n",
       "      <td>205.652857</td>\n",
       "      <td>5.395333</td>\n",
       "      <td>4.897800</td>\n",
       "      <td>17.617327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pulsation Timing Variations</th>\n",
       "      <td>2385.000000</td>\n",
       "      <td>12.712000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>2.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radial Velocity</th>\n",
       "      <td>1041.315930</td>\n",
       "      <td>10.031391</td>\n",
       "      <td>2.112706</td>\n",
       "      <td>5.167191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transit</th>\n",
       "      <td>172.593293</td>\n",
       "      <td>4.111279</td>\n",
       "      <td>0.128524</td>\n",
       "      <td>0.069854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transit Timing Variations</th>\n",
       "      <td>461.589167</td>\n",
       "      <td>5.698096</td>\n",
       "      <td>0.501715</td>\n",
       "      <td>0.532655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   mass_ME  radius_RE  orbital_radius_AU  \\\n",
       "detection_method                                                           \n",
       "Astrometry                     4890.840000  12.600000           0.499825   \n",
       "Direct Imaging                 7929.949333  15.835680         514.123769   \n",
       "Disk Kinematics                 795.000000  13.216000         130.000000   \n",
       "Eclipse Timing Variations      2154.773529  12.880000           3.962357   \n",
       "Gravitational Microlensing      746.775584  10.241521           2.541477   \n",
       "Orbital Brightness Modulation   350.513333   9.623000           0.013667   \n",
       "Pulsar Timing                   205.652857   5.395333           4.897800   \n",
       "Pulsation Timing Variations    2385.000000  12.712000           1.700000   \n",
       "Radial Velocity                1041.315930  10.031391           2.112706   \n",
       "Transit                         172.593293   4.111279           0.128524   \n",
       "Transit Timing Variations       461.589167   5.698096           0.501715   \n",
       "\n",
       "                               orbital_period_yr  \n",
       "detection_method                                  \n",
       "Astrometry                              0.726626  \n",
       "Direct Imaging                      40445.285440  \n",
       "Disk Kinematics                       957.300000  \n",
       "Eclipse Timing Variations               9.628240  \n",
       "Gravitational Microlensing              7.065273  \n",
       "Orbital Brightness Modulation           0.003164  \n",
       "Pulsar Timing                          17.617327  \n",
       "Pulsation Timing Variations             2.750000  \n",
       "Radial Velocity                         5.167191  \n",
       "Transit                                 0.069854  \n",
       "Transit Timing Variations               0.532655  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped2=df.groupby(['detection_method'])\n",
    "grouped2[['mass_ME','radius_RE', 'orbital_radius_AU', 'orbital_period_yr']].agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ffa70-c2c6-4358-b835-a6e93ebb6060",
   "metadata": {},
   "source": [
    "### Advanced I/O Example\n",
    "\n",
    "To show you an output example in the HDF5 format, I'm going to add another column with the planet masses in the other typical mass unit, Jupiter masses, and then pack the whole table and a meta-data dictionary with the above definitions into an HDF5 file.\n",
    "\n",
    "Jupiter's mass is 317.9 times that of Earth, so all I have to do is multiply the existing mass column by that number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df652548-0664-4e4f-98f2-7c6b0f082964",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f5182-6c75-46b2-bfcc-cc6e2870ea05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f268b-8973-4ff9-abc3-e0b257db4372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
